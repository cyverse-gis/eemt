#!/bin/bash

# Script for submitting workers to UA HPC

# The password is the argument
PASSWORD=$1
if [ "x$PASSWORD" = "x" ]; then
    echo "Please specify a password"
    exit 1
fi

#module load cctools
export PATH=${HOME}/cctools/bin:$PATH

for PROJECT in EEMT_UAHPC; do

    WAITING=0
    for VALUE in `work_queue_status | grep $PROJECT | awk '{print $4;}'`; do
        WAITING=$(($WAITING + $VALUE))
    done
    
    RUNNING=0
    for VALUE in `work_queue_status | grep $PROJECT | awk '{print $5;}'`; do
        RUNNING=$(($RUNNING + $VALUE))
    done
    
    WORKERS=0
    for VALUE in `work_queue_status | grep $PROJECT | awk '{print $7;}'`; do
        WORKERS=$(($WORKERS + $VALUE))
    done

    echo "Waiting=$WAITING, Running=$RUNNING, Workers=$WORKERS"
    
    # any waiting tasks?
    if [ $WAITING = 0 ]; then
        echo "No tasks waiting"
        continue
    fi
    
    if [ $WORKERS -gt 10 ]; then
        echo "Too many workers. "
        continue
    fi
    
    # start slow to make sure the system works
    NEW_WORKER_NODES=1
    
    if [ $WAITING -gt 0 -a $RUNNING -gt 0 ]; then
        NEW_WORKER_NODES=$WAITING
    fi
    
    # limit the number of new workers to 3 per iteration
    if [ $NEW_WORKER_NODES -gt 1 ]; then
        NEW_WORKER_NODES=3
    fi

    # make sure we don't have pending workers
    PENDING_WORKERS=`qstat -i -u $USER | wc -l`
    if [ $PENDING_WORKERS -gt 0 ]; then
        continue
    fi
    
    echo "Submitting $NEW_WORKERS new workers..."
    
    # keep logs on the scratch filesystem
    WORK_DIR=$HOME/workers/`/bin/date +'%F_%H%M%S'`
    mkdir -p $WORK_DIR
    cd $WORK_DIR

    # we need a wrapper to load modules
    cat >sol-worker.submit <<EOF
#!/bin/bash

### Set the Job Name
#PBS -N eemt_singularity

### Specify the PI group for this job
### List of PI groups available to each user can be found with the "va" command 
#PBS -W group_list=tswetnam

### Set the queue for this job as windfall or standard (adjust### and #)
### example: PBS -q windfall
#PBS -q standard

### Output/error streams
#PBS -j oe

### Set the number of nodes, cores (28 cores / node) and memory (168gb / node)
#PBS -l select=1:ncpus=28:mem=168gb

### Specify "wallclock time" required for this job, hhh:mm:ss
#PBS -l walltime=01:00:00

### Specify total cpu time required for this job, hhh:mm:ss
### total cputime = walltime * ncpus
#PBS -l cput=84:00:00

module load singularity

SCRATCH_DIR=`mktemp -d --tmpdir=/tmp eemt.XXXXXXXXX`

cd \$SCRATCH_DIR
    
# password file from argument
echo "$PASSWORD" >password.txt    

wget -nv http://xd-login.opensciencegrid.org/scratch/eemt/singularity/eemt-current.img

export PATH=/opt/eemt/bin:/opt/eemt/grass-7.2.0/bin:\$PATH
export LD_LIBRARY_PATH=/opt/eemt:/opt/eemt/grass-7.2.0/lib
singularity exec --home \$PWD:/srv --pwd /srv --scratch /var/tmp --scratch /tmp --containall eemt-current.img work_queue_worker -P password.txt -M $PROJECT -s /srv -t 600 --cores=0

rm -f eemt-*.img

EOF
    chmod 755 sol-worker.submit
    qsub sol-worker.submit

done

