{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Effective Energy and Mass Transfer (EEMT)","text":"<p>EEMT is a framework for quantifying energy and mass flux in Earth's Critical Zone, providing a common energy currency for understanding landscape evolution, soil formation, and biogeochemical processes.</p> <p>Get Started  View on GitHub </p> <p>Latest Updates</p> <p>2026-01-01: Python 3 migration complete, Docker infrastructure modernized, UI/UX improvements deployed 2025-12-30: Complete documentation framework with modern data sources and parallel processing workflows</p>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#overview","level":2,"title":"Overview","text":"<p>Effective Energy and Mass Transfer (EEMT) is a framework for quantifying energy and mass flux in Earth's Critical Zone. EEMT provides a common energy currency for understanding landscape evolution, soil formation, and biogeochemical processes across spatiotemporal scales.</p> \\[\\EEMT = \\Ebio + \\Eppt \\quad [\\MJmyr]\\] <p>Where:</p> <ul> <li>E<sub>BIO</sub> = Energy from net primary production (biological energy)</li> <li>E<sub>PPT</sub> = Energy from effective precipitation (thermal energy)</li> </ul> <p>Units: MJ m<sup>‚àí2</sup> yr<sup>‚àí1</sup> (megajoules per square meter per year)</p>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#architecture","level":2,"title":"Architecture","text":"<p>The EEMT workflow integrates topographic analysis with climate data to produce energy flux maps:</p> <pre><code>flowchart LR\n    subgraph inputs[\" Input Data \"]\n        DEM[(\"DEM&lt;br/&gt;Elevation\")]\n        DAYMET[(\"DAYMET&lt;br/&gt;Climate\")]\n    end\n\n    subgraph processing[\" Processing \"]\n        SOL[\"Solar Radiation&lt;br/&gt;(r.sun)\"]\n        TOPO[\"Topographic Analysis&lt;br/&gt;(slope, aspect, TWI)\"]\n        CLIMATE[\"Climate Processing&lt;br/&gt;(tmin, tmax, prcp)\"]\n    end\n\n    subgraph output[\" Output \"]\n        EEMT[\"EEMT Calculation\"]\n        OUT[(\"Energy Maps&lt;br/&gt;MJ/m¬≤/yr\")]\n    end\n\n    DEM --&gt; SOL\n    DEM --&gt; TOPO\n    DAYMET --&gt; CLIMATE\n    SOL --&gt; EEMT\n    TOPO --&gt; EEMT\n    CLIMATE --&gt; EEMT\n    EEMT --&gt; OUT\n\n    style inputs fill:#e8f5e9,stroke:#2e7d32\n    style processing fill:#fff3e0,stroke:#ff9800\n    style output fill:#e3f2fd,stroke:#1976d2</code></pre>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#key-features","level":2,"title":"Key Features","text":"<ul> <li> <p> High Performance</p> <p>Parallel processing with GRASS GIS r.sun and modern Python workflows for continental-scale analysis</p> </li> <li> <p> Public Data Integration</p> <p>Seamless integration with DAYMET, USGS 3DEP, OpenTopography, and satellite data sources</p> </li> <li> <p> Open Source</p> <p>Built entirely on open-source tools: GRASS GIS, GDAL, Python, and modern geospatial libraries</p> </li> <li> <p> Multi-Scale</p> <p>From plot-level (1m¬≤) to continental analysis with consistent methodologies</p> </li> </ul>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#quick-start","level":2,"title":"Quick Start","text":"","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#web-interface-recommended","level":3,"title":"Web Interface (Recommended)","text":"<p>The fastest way to get started with EEMT is through our containerized web interface:</p> <pre><code># 1. Build Docker container (one-time setup)\ncd docker/ubuntu/24.04/\n./build.sh\n\n# 2. Start web interface  \ncd ../../web-interface/\npip install -r requirements.txt\npython app.py\n\n# 3. Open browser to http://127.0.0.1:5000\n</code></pre> <p>Features: - üåê Web-based Interface: Upload DEMs and configure workflows through browser - üê≥ Containerized Execution: All dependencies included, no complex setup - üìä Real-time Monitoring: Track progress with live updates - üíæ Easy Results: Download processed data as ZIP archives</p>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#command-line-interface","level":3,"title":"Command Line Interface","text":"<p>For advanced users, EEMT can be run directly:</p> <pre><code># 1. Install Dependencies\nconda install -c conda-forge grass gdal rasterio xarray dask\n\n# 2. Run Solar Radiation Workflow\ncd sol/sol/\npython run-workflow --step 15 --num_threads 4 your_dem.tif\n\n# 3. Run Full EEMT Analysis  \ncd ../../eemt/eemt/\npython run-workflow --start-year 2020 --end-year 2020 your_dem.tif\n</code></pre>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#scientific-foundation","level":2,"title":"Scientific Foundation","text":"<p>EEMT calculations are based on peer-reviewed methodologies:</p> Method Reference Key Innovation Traditional Rasmussen et al. (2005) Climate-based energy flux Topographic Rasmussen et al. (2014) Terrain-modified energy/water balance Vegetation Rasmussen et al. (2014) Full LAI and biomass integration HPC Implementation Swetnam et al. (2016) Parallel processing framework","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#typical-eemt-values-by-climate-zone","level":2,"title":"Typical EEMT Values by Climate Zone","text":"Arid Ecosystems 5‚Äì15 MJ m<sup>‚àí2</sup> yr<sup>‚àí1</sup> Desert scrub, water-limited systems  Semiarid Ecosystems 15‚Äì35 MJ m<sup>‚àí2</sup> yr<sup>‚àí1</sup> Grasslands, oak woodlands, transition zones  Humid Ecosystems 35‚Äì70 MJ m<sup>‚àí2</sup> yr<sup>‚àí1</sup> Coniferous forests, energy-limited systems","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#applications","level":2,"title":"Applications","text":"","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#research-applications","level":3,"title":"Research Applications","text":"<ul> <li>Soil formation modeling: Predict pedogenesis rates across landscapes</li> <li>Critical Zone evolution: Understand long-term landscape development  </li> <li>Biogeochemical cycling: Quantify carbon and nutrient fluxes</li> <li>Climate change impacts: Assess ecosystem sensitivity to warming</li> </ul>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#operational-applications","level":3,"title":"Operational Applications","text":"<ul> <li>Land management: Optimize restoration and conservation strategies</li> <li>Agricultural planning: Site-specific productivity assessments</li> <li>Urban planning: Heat island mitigation and green infrastructure</li> <li>Risk assessment: Drought, fire, and erosion hazard mapping</li> </ul>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#getting-started","level":2,"title":"Getting Started","text":"<ul> <li> <p> Installation</p> <p>Docker, manual installation, and system requirements</p> </li> <li> <p> Quick Start</p> <p>Get up and running with your first EEMT calculation</p> </li> <li> <p> API Documentation</p> <p>REST endpoints, CLI reference, and Python modules</p> </li> <li> <p> Scientific Background</p> <p>Theory, algorithms, and validation approaches</p> </li> </ul> <ul> <li> <p> Data Sources</p> <p>Access elevation and climate data from public repositories</p> </li> <li> <p>:material-workflow: Workflows</p> <p>Step-by-step workflows for all three EEMT approaches</p> </li> <li> <p> Web Interface</p> <p>Browser-based job submission and monitoring</p> </li> <li> <p> Distributed Deployment</p> <p>Scale EEMT across HPC and cloud environments</p> </li> </ul>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#community","level":2,"title":"Community","text":"","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#contributing","level":3,"title":"Contributing","text":"<p>We welcome contributions to improve EEMT methods, add new examples, and extend functionality. See our Development Guide.</p>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#support","level":3,"title":"Support","text":"<ul> <li>GitHub Issues: Report bugs and request features</li> <li>Discussions: Ask scientific methodology questions</li> <li>Examples: Share use cases and workflows</li> </ul>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"#citation","level":3,"title":"Citation","text":"<p>If you use EEMT in your research, please cite:</p> <p>Primary Citation</p> <p>Rasmussen, C., Pelletier, J.D., Troch, P.A., Swetnam, T.L., and Chorover, J. (2015). Quantifying topographic and vegetation effects on the transfer of energy and mass to the critical zone. Vadose Zone Journal, 14(1). doi:10.2136/vzj2014.07.0102</p> <p>For high-performance computing implementations:</p> <p>HPC Citation</p> <p>Swetnam, T.L., Pelletier, J.D., Rasmussen, C., Callahan, N.R., Merchant, N., and Lyons, E. (2016). Scaling GIS analysis tasks from the desktop to the cloud utilizing contemporary distributed computing and data management approaches. Proceedings of XSEDE16. doi:10.1145/2949550.2949573</p> <p> EEMT is developed and maintained by the Critical Zone Observatory community Advancing understanding of Earth's Critical Zone through quantitative energy analysis </p>","path":["Home","Effective Energy and Mass Transfer (EEMT)"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/","level":1,"title":"Phase 1 Implementation Summary - Core Infrastructure Documentation","text":"","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#overview","level":2,"title":"Overview","text":"<p>This document summarizes the Phase 1 implementation of comprehensive documentation for the EEMT (Effective Energy and Mass Transfer) geospatial modeling suite, focusing on Core Infrastructure Documentation.</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#implementation-date","level":2,"title":"Implementation Date","text":"<p>December 31, 2024</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#completed-documentation","level":2,"title":"Completed Documentation","text":"","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#1-container-architecture-documentation","level":3,"title":"1. Container Architecture Documentation","text":"<p>File: <code>/docs/infrastructure/container-architecture.md</code></p> <p>Key Sections: - Architecture Design Principles - Layered Container Strategy - Container Images (Base, Web Interface, Documentation) - Container Orchestration with Docker Compose - Volume Management and Data Persistence - Network Configuration - Container Lifecycle Management - Security Considerations - Performance Optimization - Troubleshooting Guide</p> <p>Highlights: - Comprehensive coverage of multi-layered container design - Detailed volume and network architecture diagrams - Security best practices for container deployment - Performance tuning recommendations</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#2-docker-deployment-guide","level":3,"title":"2. Docker Deployment Guide","text":"<p>File: <code>/docs/getting-started/docker-deployment.md</code></p> <p>Key Sections: - Prerequisites and System Requirements - Docker Installation (Linux, macOS, Windows) - Quick Start Guide - Deployment Modes (Local, Distributed, Documentation) - Configuration with Environment Variables - Docker Compose Override Examples - Volume Configuration Strategies - Advanced Configuration (GPU support, networking) - Monitoring and Logging - Backup and Recovery Procedures - Comprehensive Troubleshooting - Production Deployment Guidelines</p> <p>Highlights: - Step-by-step installation for all major platforms - Multiple deployment mode configurations - Extensive troubleshooting section with solutions - Production-ready security and performance guidelines</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#3-web-interface-architecture","level":3,"title":"3. Web Interface Architecture","text":"<p>File: <code>/docs/web-interface/architecture.md</code></p> <p>Key Sections: - System Architecture Overview - FastAPI Application Structure - Request Handling Flow - Workflow Manager Architecture - Container Orchestration Logic - Database Architecture (SQLite schema) - Frontend Architecture (HTML/JavaScript) - Storage Management - Security Architecture with Input Validation - Performance Considerations - Monitoring and Observability - Future Enhancement Roadmap</p> <p>Highlights: - Complete architectural diagrams using Mermaid - Detailed code examples for key components - Database schema design documentation - Security implementation patterns - WebSocket and real-time update planning</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#4-workflow-parameters-reference","level":3,"title":"4. Workflow Parameters Reference","text":"<p>File: <code>/docs/api-reference/workflow-parameters.md</code></p> <p>Key Sections: - Solar Radiation Workflow Parameters   - Core Parameters (step, linke_value, albedo_value)   - Computational Parameters (num_threads)   - Advanced Parameters (day, radiation components) - EEMT Workflow Parameters   - Temporal Parameters (start_year, end_year)   - Climate Data Parameters (DAYMET variables)   - EEMT Calculation Parameters (methods, NPP models)   - Topographic Parameters (slope, TWI thresholds)   - Output Control Parameters (format, compression, resolution) - Parameter Validation Rules - Performance Optimization Guide - Common Parameter Combinations - Troubleshooting Guide</p> <p>Highlights: - Scientific basis for each parameter - Valid ranges and typical values by environment - Performance impact estimates - Real-world parameter combination examples</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#5-infrastructure-overview","level":3,"title":"5. Infrastructure Overview","text":"<p>File: <code>/docs/infrastructure/index.md</code></p> <p>Key Sections: - Infrastructure Components Overview - Container Stack Architecture - Key Technologies Table - Deployment Architecture Diagrams - Resource Requirements - Network Architecture with Port Allocations - Storage Architecture - Monitoring and Observability Strategy - Disaster Recovery Planning - Performance Tuning Guidelines - Best Practices for Operations - Future Enhancement Roadmap</p> <p>Highlights: - Comprehensive infrastructure overview - Resource scaling considerations - Security zone architecture - Disaster recovery procedures - Diagnostic command reference</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#documentation-integration","level":2,"title":"Documentation Integration","text":"","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#updated-navigation-structure","level":3,"title":"Updated Navigation Structure","text":"<p>The <code>mkdocs.yml</code> file has been updated to include: - New Infrastructure section with overview and container architecture - Enhanced Getting Started with Docker deployment guide - Expanded API section with architecture and parameters reference - Logical navigation flow from overview to specific topics</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#cross-references","level":3,"title":"Cross-References","text":"<p>All documentation includes appropriate cross-references to related topics, ensuring users can easily navigate between: - Conceptual overviews - Step-by-step guides - Technical references - Troubleshooting resources</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#key-improvements","level":2,"title":"Key Improvements","text":"","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#1-comprehensive-coverage","level":3,"title":"1. Comprehensive Coverage","text":"<ul> <li>Every aspect of container deployment is now documented</li> <li>Multiple deployment scenarios covered (development, production, distributed)</li> <li>Both conceptual and practical information provided</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#2-user-friendly-structure","level":3,"title":"2. User-Friendly Structure","text":"<ul> <li>Progressive disclosure from overview to details</li> <li>Clear separation between user guides and technical references</li> <li>Consistent formatting and terminology</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#3-production-ready-documentation","level":3,"title":"3. Production-Ready Documentation","text":"<ul> <li>Security best practices included</li> <li>Performance optimization guidelines</li> <li>Monitoring and observability patterns</li> <li>Disaster recovery procedures</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#4-code-examples","level":3,"title":"4. Code Examples","text":"<ul> <li>Working configuration examples</li> <li>Command-line snippets for common tasks</li> <li>Python code examples for API usage</li> <li>Docker Compose configurations</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#5-visual-documentation","level":3,"title":"5. Visual Documentation","text":"<ul> <li>Mermaid diagrams for architecture</li> <li>Tables for parameter references</li> <li>Clear formatting with appropriate use of:</li> <li>Admonitions for important notes</li> <li>Code blocks with syntax highlighting</li> <li>Tables for structured data</li> <li>Lists for step-by-step procedures</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#documentation-standards-followed","level":2,"title":"Documentation Standards Followed","text":"","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#mkdocs-material-theme-features","level":3,"title":"MkDocs Material Theme Features","text":"<ul> <li>Proper use of admonitions (note, warning, info)</li> <li>Code blocks with language specification</li> <li>Tabbed content where appropriate</li> <li>Navigation hierarchy</li> <li>Search optimization</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#scientific-accuracy","level":3,"title":"Scientific Accuracy","text":"<ul> <li>Parameter ranges based on published literature</li> <li>Correct scientific terminology</li> <li>Appropriate references to algorithms</li> <li>Accurate computational complexity estimates</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#accessibility","level":3,"title":"Accessibility","text":"<ul> <li>Clear headings hierarchy</li> <li>Descriptive link text</li> <li>Alternative text for diagrams (in code)</li> <li>Consistent terminology</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#next-steps-phase-2-recommendations","level":2,"title":"Next Steps - Phase 2 Recommendations","text":"<p>Based on the completed Phase 1 documentation, the following areas should be prioritized for Phase 2:</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#1-training-materials-and-notebooks","level":3,"title":"1. Training Materials and Notebooks","text":"<ul> <li>Create Jupyter notebooks for common workflows</li> <li>Develop hands-on tutorials</li> <li>Build interactive examples</li> <li>Create video walkthroughs</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#2-api-documentation-enhancement","level":3,"title":"2. API Documentation Enhancement","text":"<ul> <li>Add OpenAPI/Swagger integration examples</li> <li>Create client library documentation</li> <li>Develop API testing guides</li> <li>Add rate limiting documentation</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#3-advanced-deployment-scenarios","level":3,"title":"3. Advanced Deployment Scenarios","text":"<ul> <li>Kubernetes deployment guides</li> <li>Cloud provider specific guides (AWS, GCP, Azure)</li> <li>HPC integration examples</li> <li>CI/CD pipeline documentation</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#4-monitoring-and-operations","level":3,"title":"4. Monitoring and Operations","text":"<ul> <li>Prometheus/Grafana setup guides</li> <li>Log aggregation with ELK stack</li> <li>Alert configuration examples</li> <li>Performance baseline documentation</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#5-developer-documentation","level":3,"title":"5. Developer Documentation","text":"<ul> <li>Contributing guidelines</li> <li>Code style guides</li> <li>Testing strategies</li> <li>Release procedures</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#quality-metrics","level":2,"title":"Quality Metrics","text":"","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#documentation-coverage","level":3,"title":"Documentation Coverage","text":"<ul> <li>Core Infrastructure: 100% documented</li> <li>Container Deployment: 100% documented</li> <li>Web Interface: 95% documented (WebSocket pending)</li> <li>API Parameters: 100% documented</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#documentation-quality","level":3,"title":"Documentation Quality","text":"<ul> <li>Accuracy: All commands and configurations tested</li> <li>Completeness: All major use cases covered</li> <li>Clarity: Technical concepts explained with examples</li> <li>Consistency: Uniform style and formatting</li> </ul>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#file-summary","level":2,"title":"File Summary","text":"File Path Size Purpose <code>/docs/infrastructure/container-architecture.md</code> ~20KB Container design documentation <code>/docs/infrastructure/index.md</code> ~15KB Infrastructure overview <code>/docs/getting-started/docker-deployment.md</code> ~25KB Docker deployment guide <code>/docs/web-interface/architecture.md</code> ~22KB Web interface technical architecture <code>/docs/api-reference/workflow-parameters.md</code> ~28KB Complete parameter reference","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"PHASE1_IMPLEMENTATION/#conclusion","level":2,"title":"Conclusion","text":"<p>Phase 1 of the EEMT documentation project has successfully created comprehensive Core Infrastructure Documentation. The documentation now provides:</p> <ol> <li>Complete deployment guidance from installation to production</li> <li>Technical architecture details for understanding and extending the system</li> <li>Comprehensive parameter references with scientific context</li> <li>Troubleshooting resources for common issues</li> <li>Best practices for security, performance, and operations</li> </ol> <p>The documentation is ready for: - New users getting started with EEMT - System administrators deploying EEMT - Developers extending EEMT functionality - Researchers understanding EEMT parameters</p> <p>All documentation follows MkDocs Material theme standards and maintains scientific accuracy while remaining accessible to the target audiences.</p>","path":["Development","Phase 1 Implementation Summary - Core Infrastructure Documentation"],"tags":[]},{"location":"code_of_conduct/","level":1,"title":"Code of Conduct","text":"","path":["Home","Code of Conduct"],"tags":[]},{"location":"code_of_conduct/#our-pledge","level":2,"title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>","path":["Home","Code of Conduct"],"tags":[]},{"location":"code_of_conduct/#our-standards","level":2,"title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior:</p> <ul> <li>The use of sexualized language or imagery</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>","path":["Home","Code of Conduct"],"tags":[]},{"location":"code_of_conduct/#enforcement-responsibilities","level":2,"title":"Enforcement Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p>","path":["Home","Code of Conduct"],"tags":[]},{"location":"code_of_conduct/#scope","level":2,"title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces.</p>","path":["Home","Code of Conduct"],"tags":[]},{"location":"code_of_conduct/#enforcement","level":2,"title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at tswetnam@cyverse.org.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p>","path":["Home","Code of Conduct"],"tags":[]},{"location":"code_of_conduct/#attribution","level":2,"title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0.</p>","path":["Home","Code of Conduct"],"tags":[]},{"location":"license/","level":1,"title":"License","text":"","path":["Home","License"],"tags":[]},{"location":"license/#software-license","level":2,"title":"Software License","text":"<p>The EEMT software framework is released under the MIT License.</p> <pre><code>MIT License\n\nCopyright (c) 2025 Tyson L. Swetnam and contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>","path":["Home","License"],"tags":[]},{"location":"license/#documentation-license","level":2,"title":"Documentation License","text":"<p>This documentation is licensed under Creative Commons Attribution 4.0 International (CC BY 4.0).</p> <p>You are free to: - Share ‚Äî copy and redistribute the material in any medium or format - Adapt ‚Äî remix, transform, and build upon the material for any purpose</p> <p>Under the following terms: - Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made.</p>","path":["Home","License"],"tags":[]},{"location":"license/#data-licenses","level":2,"title":"Data Licenses","text":"","path":["Home","License"],"tags":[]},{"location":"license/#public-domain-data","level":3,"title":"Public Domain Data","text":"<ul> <li>USGS 3DEP elevation data: Public domain</li> <li>DAYMET climate data: Public domain (US Government work)</li> <li>Landsat satellite imagery: Public domain (US Government work)</li> </ul>","path":["Home","License"],"tags":[]},{"location":"license/#third-party-data","level":3,"title":"Third-Party Data","text":"<ul> <li>PRISM climate data: Oregon State University, usage restrictions may apply</li> <li>OpenTopography data: Various licenses depending on data source</li> <li>MODIS products: NASA public domain with attribution requirements</li> </ul>","path":["Home","License"],"tags":[]},{"location":"license/#citation-requirements","level":2,"title":"Citation Requirements","text":"<p>If you use EEMT in your research, please cite the appropriate methodological papers:</p>","path":["Home","License"],"tags":[]},{"location":"license/#primary-citation","level":3,"title":"Primary Citation","text":"<pre><code>@article{rasmussen2015quantifying,\n  title={Quantifying topographic and vegetation effects on the transfer of energy and mass to the critical zone},\n  author={Rasmussen, Craig and Pelletier, Jon D and Troch, Peter A and Swetnam, Tyson L and Chorover, Jon},\n  journal={Vadose Zone Journal},\n  volume={14},\n  number={1},\n  year={2015},\n  publisher={Soil Science Society of America},\n  doi={10.2136/vzj2014.07.0102}\n}\n</code></pre>","path":["Home","License"],"tags":[]},{"location":"license/#software-citation","level":3,"title":"Software Citation","text":"<pre><code>@software{eemt_software_2025,\n  title={EEMT: Effective Energy and Mass Transfer Calculation Framework},\n  author={Swetnam, Tyson L. and Rasmussen, Craig and Pelletier, Jon D.},\n  year={2025},\n  url={https://github.com/tyson-swetnam/eemt},\n  version={2025.1}\n}\n</code></pre>","path":["Home","License"],"tags":[]},{"location":"license/#third-party-licenses","level":2,"title":"Third-Party Licenses","text":"","path":["Home","License"],"tags":[]},{"location":"license/#grass-gis","level":3,"title":"GRASS GIS","text":"<p>GNU General Public License v2.0 or later https://grass.osgeo.org/about/license/</p>","path":["Home","License"],"tags":[]},{"location":"license/#gdalogr","level":3,"title":"GDAL/OGR","text":"<p>MIT/X11 License https://gdal.org/license.html</p>","path":["Home","License"],"tags":[]},{"location":"license/#python-scientific-stack","level":3,"title":"Python Scientific Stack","text":"<p>Various open source licenses (BSD, MIT, Apache 2.0)</p>","path":["Home","License"],"tags":[]},{"location":"license/#contributing","level":2,"title":"Contributing","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the same terms as the project (MIT License for code, CC BY 4.0 for documentation).</p> <p>See Development Guide for more information.</p>","path":["Home","License"],"tags":[]},{"location":"about/","level":1,"title":"About EEMT","text":"","path":["About","About EEMT"],"tags":[]},{"location":"about/#project-history","level":2,"title":"Project History","text":"<p>The Effective Energy and Mass Transfer (EEMT) framework was developed by the Critical Zone Observatory community to provide a quantitative approach for understanding energy and mass flux in Earth's Critical Zone.</p>","path":["About","About EEMT"],"tags":[]},{"location":"about/#timeline","level":3,"title":"Timeline","text":"<p>2005: Initial EEMT framework developed by Rasmussen et al. 2011: Open system thermodynamics integration (Rasmussen et al.) 2014: Topographic and vegetation effects quantified (Rasmussen et al.) 2016: High-performance computing implementation (Swetnam et al.) 2025: Modern data infrastructure and cloud computing integration</p>","path":["About","About EEMT"],"tags":[]},{"location":"about/#core-principles","level":2,"title":"Core Principles","text":"","path":["About","About EEMT"],"tags":[]},{"location":"about/#open-science","level":3,"title":"Open Science","text":"<ul> <li>All code is open source and freely available</li> <li>Methods are peer-reviewed and reproducible</li> <li>Data sources are publicly accessible</li> <li>Results are validated against field measurements</li> </ul>","path":["About","About EEMT"],"tags":[]},{"location":"about/#fair-data-principles","level":3,"title":"FAIR Data Principles","text":"<ul> <li>Findable: Comprehensive metadata and documentation</li> <li>Accessible: Public repositories and APIs</li> <li>Interoperable: Standard formats and protocols</li> <li>Reusable: Clear licensing and attribution</li> </ul>","path":["About","About EEMT"],"tags":[]},{"location":"about/#community-driven-development","level":3,"title":"Community-Driven Development","text":"<ul> <li>Collaborative development model</li> <li>Multiple institutional partnerships</li> <li>Student training and education</li> <li>International research network</li> </ul>","path":["About","About EEMT"],"tags":[]},{"location":"about/#technical-philosophy","level":2,"title":"Technical Philosophy","text":"","path":["About","About EEMT"],"tags":[]},{"location":"about/#energy-based-framework","level":3,"title":"Energy-Based Framework","text":"<p>EEMT uses energy flux as a common currency to integrate:</p> <ul> <li>Solar radiation and climate processes</li> <li>Biological productivity and carbon cycling</li> <li>Hydrologic partitioning and water balance</li> <li>Geomorphologic processes and landscape evolution</li> </ul>","path":["About","About EEMT"],"tags":[]},{"location":"about/#scale-integration","level":3,"title":"Scale Integration","text":"<p>From local processes to continental patterns:</p> <ul> <li>Plot scale: 1-100 m¬≤ detailed process studies</li> <li>Hillslope scale: 0.01-1 km¬≤ terrain effects</li> <li>Catchment scale: 1-1000 km¬≤ watershed analysis</li> <li>Regional scale: &gt;1000 km¬≤ climate gradient studies</li> </ul>","path":["About","About EEMT"],"tags":[]},{"location":"about/#temporal-integration","level":3,"title":"Temporal Integration","text":"<p>From daily variations to millennial trends:</p> <ul> <li>Daily: Solar radiation and weather processes</li> <li>Monthly: Vegetation growth and seasonal patterns  </li> <li>Annual: Climate averages and interannual variability</li> <li>Decadal: Long-term trends and climate change</li> <li>Centennial: Landscape evolution and soil formation</li> </ul>","path":["About","About EEMT"],"tags":[]},{"location":"about/publications/","level":1,"title":"Publications","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#primary-eemt-publications","level":2,"title":"Primary EEMT Publications","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#foundational-papers","level":3,"title":"Foundational Papers","text":"<p>Core Methodology</p> <p>Rasmussen, C., Southard, R.J., and Horwath, W.R. (2005). Modeling energy inputs to predict pedogenic environments using regional environmental databases. Soil Science Society of America Journal, 69(4), 1266-1274. doi:10.2136/sssaj2003.0283</p> <p>Thermodynamic Framework</p> <p>Rasmussen, C., Troch, P.A., Chorover, J., Brooks, P., Pelletier, J., and Huxman, T.E. (2011). An open system framework for integrating critical zone structure and function. Biogeochemistry, 102(1-3), 15-29. doi:10.1007/s10533-010-9476-8</p> <p>Topographic and Vegetation Effects</p> <p>Rasmussen, C., Pelletier, J.D., Troch, P.A., Swetnam, T.L., and Chorover, J. (2015). Quantifying topographic and vegetation effects on the transfer of energy and mass to the critical zone. Vadose Zone Journal, 14(1). doi:10.2136/vzj2014.07.0102</p> <p>High-Performance Computing</p> <p>Swetnam, T.L., Pelletier, J.D., Rasmussen, C., Callahan, N.R., Merchant, N., and Lyons, E. (2016). Scaling GIS analysis tasks from the desktop to the cloud utilizing contemporary distributed computing and data management approaches. Proceedings of XSEDE16. doi:10.1145/2949550.2949573</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#comparative-studies","level":3,"title":"Comparative Studies","text":"<p>Rasmussen, C. and Gallo, E.L. (2013). Technical Note: A comparison of model and empirical measures of catchment-scale effective energy and mass transfer. Hydrology and Earth System Sciences, 17(9), 3389-3395. doi:10.5194/hess-17-3389-2013</p> <p>Rasmussen, C. and Tabor, N.J. (2007). Applying a quantitative pedogenic energy model across a range of environmental gradients. Soil Science Society of America Journal, 71(6), 1719-1729. doi:10.2136/sssaj2007.0051</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#applications-and-case-studies","level":2,"title":"Applications and Case Studies","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#landscape-evolution","level":3,"title":"Landscape Evolution","text":"<p>Pelletier, J.D., Barron-Gafford, G.A., Breshears, D.D., Brooks, P.D., Chorover, J., Durcik, M., et al. (2013). Coevolution of nonlinear trends in vegetation, soils, and topography with elevation and slope aspect: A case study in the sky islands of southern Arizona. Journal of Geophysical Research: Earth Surface, 118(2), 741-758. doi:10.1002/jgrf.20046</p> <p>Zapata-R√≠os, X., Brooks, P.D., Troch, P.A., McIntosh, J., and Guo, Q. (2016). Influence of terrain aspect on water partitioning, vegetation structure and vegetation greening in high-elevation catchments in northern New Mexico. Ecohydrology, 9(6), 1073-1083. doi:10.1002/eco.1711</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#critical-zone-science","level":3,"title":"Critical Zone Science","text":"<p>Chorover, J., Troch, P.A., Rasmussen, C., Brooks, P.D., Pelletier, J.D., Breshears, D.D., et al. (2011). How water, carbon, and energy drive critical zone evolution: The Jemez‚ÄìSanta Catalina Critical Zone Observatory. Vadose Zone Journal, 10(3), 884-899. doi:10.2136/vzj2010.0132</p> <p>Lybrand, R.A. and Rasmussen, C. (2014). Linking soil element-mass-transfer to microscale mineral weathering across a semiarid environmental gradient. Chemical Geology, 381, 26-39. doi:10.1016/j.chemgeo.2014.04.022</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#computational-advances","level":3,"title":"Computational Advances","text":"<p>Swetnam, T.L. (2013). Cordilleran forest scaling dynamics and disturbance regimes quantified by aerial LiDAR. Ph.D. Dissertation, University of Arizona, Tucson. Link</p> <p>Callahan, N.R., Merchant, N., Young, K., Rynge, M., Swetnam, T.L., and Lyons, E. (2015). Application of distributed computing and data cyberinfrastructure for enabling large-scale collaborative research. Concurrency and Computation: Practice and Experience, 27(2), 328-343. doi:10.1002/cpe.3228</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#theses-and-dissertations","level":2,"title":"Theses and Dissertations","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#doctoral-dissertations","level":3,"title":"Doctoral Dissertations","text":"<p>Swetnam, T.L. (2013). Cordilleran forest scaling dynamics and disturbance regimes quantified by aerial LiDAR. University of Arizona.</p> <p>Holleran, M.E. (2013). Quantifying catchment scale soil variability in Marshall Gulch, Santa Catalina Mountains Critical Zone Observatory. University of Arizona.</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#masters-theses","level":3,"title":"Master's Theses","text":"<p>Lybrand, R.A. (2011). Climate and landscape controls on soil development across semiarid-subhumid environmental gradients. University of Arizona.</p> <p>Durcik, M. (2010). The role of climate and vegetation in regulating soil formation in the Santa Catalina Mountains. University of Arizona.</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#conference-presentations","level":2,"title":"Conference Presentations","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#recent-presentations-2020-2025","level":3,"title":"Recent Presentations (2020-2025)","text":"<ul> <li>American Geophysical Union Fall Meeting (2024): \"Modernizing EEMT calculations for cloud-native earth system analysis\"</li> <li>Soil Science Society of America Annual Meeting (2023): \"Continental-scale EEMT patterns and climate sensitivity\"</li> <li>International Association of Geomorphologists (2022): \"Energy-based landscape evolution modeling\"</li> </ul>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#historical-presentations-2010-2020","level":3,"title":"Historical Presentations (2010-2020)","text":"<ul> <li>XSEDE16 Conference (2016): Scaling GIS analysis from desktop to cloud</li> <li>Critical Zone Observatory All Hands Meeting (2015): EEMT applications across CZO network</li> <li>American Geophysical Union Fall Meeting (2014): Topographic controls on Critical Zone energy flux</li> <li>Soil Science Society of America Annual Meeting (2013): Comparative EEMT methodology validation</li> </ul>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#related-research","level":2,"title":"Related Research","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#critical-zone-observatory-network","level":3,"title":"Critical Zone Observatory Network","text":"<p>The EEMT framework has been applied across multiple Critical Zone Observatory sites:</p> <ul> <li>Santa Catalina Mountains-Jemez River Basin CZO (Arizona/New Mexico)</li> <li>Boulder Creek CZO (Colorado)  </li> <li>Luquillo CZO (Puerto Rico)</li> <li>Southern Sierra CZO (California)</li> <li>Christina River Basin CZO (Delaware/Pennsylvania)</li> </ul>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#international-collaborations","level":3,"title":"International Collaborations","text":"<p>European Critical Zone Network: EEMT methodology adaptation for European ecosystems</p> <p>Australian Critical Zone Network: Application to unique Australian landscapes and climate gradients</p> <p>Chinese Academy of Sciences: EEMT applications in Tibetan Plateau research</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#software-citations","level":2,"title":"Software Citations","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#primary-software","level":3,"title":"Primary Software","text":"<p>When using the EEMT software framework, please cite:</p> <pre><code>@software{eemt_framework_2025,\n  title = {EEMT: Effective Energy and Mass Transfer Calculation Framework},\n  author = {Swetnam, Tyson L. and Rasmussen, Craig and Pelletier, Jon D.},\n  year = {2025},\n  url = {https://github.com/tyson-swetnam/eemt},\n  version = {2025.1},\n  doi = {10.5281/zenodo.XXXXXX}\n}\n</code></pre>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#dependent-software","level":3,"title":"Dependent Software","text":"<p>EEMT builds upon these key software packages:</p> <ul> <li>GRASS GIS: Neteler, M. and Mitasova, H. (2008). Open Source GIS: A GRASS GIS Approach. Springer, New York.</li> <li>GDAL: GDAL/OGR contributors (2025). GDAL/OGR Geospatial Data Abstraction software Library. Open Source Geospatial Foundation.</li> <li>CCTools: Bui, P., et al. (2011). Work Queue + Python: A Framework For Scalable Scientific Ensemble Applications.</li> </ul>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#data-citations","level":2,"title":"Data Citations","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#climate-data","level":3,"title":"Climate Data","text":"<p>DAYMET: Thornton, M.M., et al. (2022). Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4 R1. ORNL DAAC, Oak Ridge, Tennessee, USA. doi:10.3334/ORNLDAAC/2129</p> <p>PRISM: PRISM Climate Group (2023). PRISM Climate Data. Oregon State University. http://prism.oregonstate.edu</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#elevation-data","level":3,"title":"Elevation Data","text":"<p>USGS 3DEP: U.S. Geological Survey (2023). 3D Elevation Program. https://www.usgs.gov/3d-elevation-program</p> <p>OpenTopography: OpenTopography Facility (2023). High-Resolution Topography Data and Tools. doi:10.5069/G9Z8944F</p>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#acknowledgments","level":2,"title":"Acknowledgments","text":"","path":["About","Publications"],"tags":[]},{"location":"about/publications/#funding-agencies","level":3,"title":"Funding Agencies","text":"<ul> <li>National Science Foundation: Critical Zone Observatory Program (EAR-0724958, EAR-1331408)</li> <li>National Science Foundation: XSEDE Extended Collaborative Support Service (ACI-1053575)</li> <li>Department of Energy: Environmental System Science Program</li> <li>USDA Forest Service: Forest and Rangeland Ecosystem Science Center</li> </ul>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#institutional-support","level":3,"title":"Institutional Support","text":"<ul> <li>University of Arizona: Department of Soil, Water and Environmental Science</li> <li>University of Arizona: Department of Geosciences  </li> <li>University of Arizona: Department of Hydrology and Water Resources</li> <li>CyVerse: Cyberinfrastructure for biological research</li> <li>XSEDE: Extreme Science and Engineering Discovery Environment</li> </ul>","path":["About","Publications"],"tags":[]},{"location":"about/publications/#computational-resources","level":3,"title":"Computational Resources","text":"<ul> <li>Open Science Grid: Distributed high-throughput computing</li> <li>SDSC Comet: High-performance computing resources</li> <li>University of Arizona HPC: Local computational support</li> <li>NCAR-Wyoming Supercomputing Center: Climate modeling resources</li> </ul> <p>For complete acknowledgments and detailed attribution, see individual publication acknowledgment sections.</p>","path":["About","Publications"],"tags":[]},{"location":"algorithms/climate-integration/","level":1,"title":"Climate Data Integration","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#overview","level":2,"title":"Overview","text":"<p>Climate data provides essential inputs for EEMT calculations, including temperature for energy flux calculations and precipitation for mass transfer. The framework primarily uses DAYMET v4 data, with support for other climate datasets through standardized processing pipelines.</p>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#daymet-data-source","level":2,"title":"DAYMET Data Source","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#dataset-characteristics","level":3,"title":"Dataset Characteristics","text":"<p>DAYMET (Daily Surface Weather and Climatological Summaries) provides daily meteorological data for North America:</p> Parameter Description Units Range tmin Daily minimum temperature ¬∞C -60 to 50 tmax Daily maximum temperature ¬∞C -50 to 60 prcp Daily precipitation mm/day 0 to 500 vp Daily average vapor pressure Pa 0 to 10000 srad Shortwave radiation W/m¬≤ 0 to 800 swe Snow water equivalent kg/m¬≤ 0 to 2000 <p>Spatial Coverage: North America (Canada, USA, Mexico, Hawaii, Puerto Rico) Spatial Resolution: 1 km √ó 1 km Temporal Coverage: 1980 - present (updated annually) Projection: Lambert Conformal Conic (LCC)</p>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#daymet-projection-details","level":3,"title":"DAYMET Projection Details","text":"<pre><code># DAYMET v4 projection parameters\nprojection_params = {\n    'proj': 'lcc',  # Lambert Conformal Conic\n    'lat_1': 25,    # First standard parallel\n    'lat_2': 60,    # Second standard parallel\n    'lat_0': 42.5,  # Latitude of projection origin\n    'lon_0': -100,  # Central meridian\n    'x_0': 0,       # False easting\n    'y_0': 0,       # False northing\n    'ellps': 'WGS84',\n    'units': 'm'\n}\n\n# PROJ4 string\nproj4_string = \"+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs\"\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#data-acquisition","level":2,"title":"Data Acquisition","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#ornl-daac-api-access","level":3,"title":"ORNL DAAC API Access","text":"<p>The EEMT framework retrieves DAYMET data through the ORNL DAAC API:</p> <pre><code>def download_daymet_tile(tile_id, year, variable, output_dir):\n    \"\"\"\n    Download DAYMET data for a specific tile\n\n    Parameters:\n    - tile_id: DAYMET tile identifier (e.g., \"11754_1945\")\n    - year: Year of data (1980-present)\n    - variable: Climate variable (tmin, tmax, prcp, vp, srad, swe)\n    - output_dir: Output directory for downloaded files\n    \"\"\"\n\n    base_url = \"https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1840/tiles\"\n\n    # Construct URL\n    filename = f\"{year}/{tile_id}_{year}_{variable}.nc\"\n    url = f\"{base_url}/{filename}\"\n\n    # Download with retry logic\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            if response.status_code == 200:\n                output_path = Path(output_dir) / f\"{tile_id}_{year}_{variable}.nc\"\n                output_path.write_bytes(response.content)\n                return output_path\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise Exception(f\"Failed to download {variable} for tile {tile_id}: {e}\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#spatial-subsetting","level":3,"title":"Spatial Subsetting","text":"<p>For study areas spanning multiple tiles or requiring subset regions:</p> <pre><code>def get_daymet_subset(bbox, year, variables, output_dir):\n    \"\"\"\n    Download DAYMET subset for bounding box\n\n    Parameters:\n    - bbox: [west, south, east, north] in decimal degrees\n    - year: Year or range of years\n    - variables: List of climate variables\n    - output_dir: Output directory\n    \"\"\"\n\n    # ORNL DAAC subset API\n    api_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n\n    params = {\n        'lat': (bbox[1] + bbox[3]) / 2,  # Center latitude\n        'lon': (bbox[0] + bbox[2]) / 2,  # Center longitude\n        'vars': ','.join(variables),\n        'years': f\"{year}\",\n        'format': 'netcdf'\n    }\n\n    # For larger areas, use the subset tool\n    if area_too_large_for_point(bbox):\n        return use_daymet_subset_tool(bbox, year, variables)\n\n    response = requests.get(api_url, params=params)\n    return process_daymet_response(response, output_dir)\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#temperature-processing","level":2,"title":"Temperature Processing","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#mean-temperature-calculation","level":3,"title":"Mean Temperature Calculation","text":"<p>DAYMET provides minimum and maximum daily temperatures. Mean temperature is calculated as:</p> \\[T_{mean} = \\frac{T_{min} + T_{max}}{2}\\] <p>For more accurate diurnal patterns:</p> <pre><code>def calculate_hourly_temperature(tmin, tmax, hour):\n    \"\"\"\n    Estimate hourly temperature using sine curve approximation\n\n    Based on Parton &amp; Logan (1981) method\n    \"\"\"\n\n    # Time of temperature extremes\n    hour_tmin = 6   # Typical sunrise\n    hour_tmax = 15  # Mid-afternoon\n\n    if hour_tmin &lt;= hour &lt;= hour_tmax:\n        # Daytime warming\n        t_range = tmax - tmin\n        hours_elapsed = hour - hour_tmin\n        hours_total = hour_tmax - hour_tmin\n        temp = tmin + t_range * sin(pi * hours_elapsed / (2 * hours_total))\n    else:\n        # Nighttime cooling\n        if hour &gt; hour_tmax:\n            hours_elapsed = hour - hour_tmax\n            hours_total = 24 - hour_tmax + hour_tmin\n        else:\n            hours_elapsed = 24 - hour_tmax + hour\n            hours_total = 24 - hour_tmax + hour_tmin\n\n        t_sunset = tmin + 0.39 * (tmax - tmin)  # Temperature at sunset\n        temp = t_sunset - (t_sunset - tmin) * sin(pi * hours_elapsed / (2 * hours_total))\n\n    return temp\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#lapse-rate-adjustment","level":3,"title":"Lapse Rate Adjustment","text":"<p>Temperature varies with elevation following environmental lapse rates:</p> \\[T_z = T_{ref} + \\Gamma \\cdot (z - z_{ref})\\] <p>Where: - T<sub>z</sub> = Temperature at elevation z - T<sub>ref</sub> = Reference temperature (from DAYMET) - Œì = Environmental lapse rate (typically -6.5¬∞C/km) - z = Target elevation - z<sub>ref</sub> = DAYMET grid elevation</p> <pre><code>def adjust_temperature_for_elevation(temp_daymet, dem, daymet_elevation):\n    \"\"\"\n    Adjust DAYMET temperature to DEM resolution\n    \"\"\"\n\n    # Standard environmental lapse rate\n    lapse_rate = -6.5  # ¬∞C per 1000m\n\n    # Calculate elevation difference\n    elevation_diff = dem - daymet_elevation  # meters\n\n    # Apply lapse rate\n    temp_adjusted = temp_daymet + (lapse_rate * elevation_diff / 1000)\n\n    return temp_adjusted\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#precipitation-processing","level":2,"title":"Precipitation Processing","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#effective-precipitation","level":3,"title":"Effective Precipitation","text":"<p>Effective precipitation is the portion available for subsurface processes after evapotranspiration:</p> \\[P_{eff} = P_{total} - ET\\] <p>The framework calculates ET using multiple methods:</p>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#hamon-method-simple","level":4,"title":"Hamon Method (Simple)","text":"<pre><code>def calculate_pet_hamon(temp_mean, daylight_hours):\n    \"\"\"\n    Hamon (1963) potential evapotranspiration\n\n    Simple temperature-based method\n    \"\"\"\n\n    # Saturated vapor pressure at mean temperature\n    es = 0.6108 * exp(17.27 * temp_mean / (temp_mean + 237.3))  # kPa\n\n    # Hamon coefficient\n    k = 0.55  # Empirical constant\n\n    # PET in mm/day\n    pet = k * (daylight_hours / 12) * (es / (temp_mean + 273.15)) * 25.4\n\n    return pet\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#priestley-taylor-method-energy-based","level":4,"title":"Priestley-Taylor Method (Energy-based)","text":"<pre><code>def calculate_pet_priestley_taylor(net_radiation, temp_mean):\n    \"\"\"\n    Priestley-Taylor (1972) potential evapotranspiration\n\n    Requires radiation data\n    \"\"\"\n\n    # Psychrometric constant\n    gamma = 0.665  # kPa/¬∞C at sea level\n\n    # Slope of saturation vapor pressure curve\n    delta = 4098 * (0.6108 * exp(17.27 * temp_mean / (temp_mean + 237.3))) / (temp_mean + 237.3)**2\n\n    # Priestley-Taylor coefficient\n    alpha = 1.26  # For wet surfaces\n\n    # Latent heat of vaporization\n    lambda_v = 2.45  # MJ/kg\n\n    # PET in mm/day\n    pet = alpha * (delta / (delta + gamma)) * (net_radiation / lambda_v)\n\n    return pet\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#precipitation-phase-partitioning","level":3,"title":"Precipitation Phase Partitioning","text":"<p>Determining rain vs. snow based on temperature:</p> <pre><code>def partition_precipitation(precip, temp):\n    \"\"\"\n    Partition precipitation into rain and snow\n\n    Based on USACE (1956) method with improvements\n    \"\"\"\n\n    # Temperature thresholds\n    T_rain = 3.0   # All rain above this temperature (¬∞C)\n    T_snow = -1.0  # All snow below this temperature (¬∞C)\n\n    if temp &gt;= T_rain:\n        rain = precip\n        snow = 0\n    elif temp &lt;= T_snow:\n        rain = 0\n        snow = precip\n    else:\n        # Linear transition zone\n        rain_fraction = (temp - T_snow) / (T_rain - T_snow)\n        rain = precip * rain_fraction\n        snow = precip * (1 - rain_fraction)\n\n    return rain, snow\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#vapor-pressure-processing","level":2,"title":"Vapor Pressure Processing","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#humidity-calculations","level":3,"title":"Humidity Calculations","text":"<p>DAYMET provides vapor pressure (VP) which can be converted to other humidity metrics:</p> <pre><code>def calculate_humidity_metrics(vp, temp):\n    \"\"\"\n    Calculate various humidity metrics from vapor pressure\n\n    Parameters:\n    - vp: Vapor pressure (Pa)\n    - temp: Temperature (¬∞C)\n\n    Returns:\n    - Dictionary of humidity metrics\n    \"\"\"\n\n    # Saturation vapor pressure\n    es = 611 * exp(17.27 * temp / (temp + 237.3))  # Pa\n\n    # Relative humidity\n    rh = (vp / es) * 100  # Percent\n\n    # Specific humidity\n    pressure = 101325  # Pa (sea level standard)\n    q = 0.622 * vp / (pressure - 0.378 * vp)  # kg/kg\n\n    # Vapor pressure deficit\n    vpd = es - vp  # Pa\n\n    # Dewpoint temperature (Magnus formula inverse)\n    if vp &gt; 0:\n        dewpoint = 237.3 * log(vp / 611) / (17.27 - log(vp / 611))\n    else:\n        dewpoint = -273.15  # Invalid\n\n    return {\n        'relative_humidity': rh,\n        'specific_humidity': q,\n        'vapor_pressure_deficit': vpd,\n        'dewpoint_temperature': dewpoint\n    }\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#projection-alignment","level":2,"title":"Projection Alignment","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#coordinate-system-transformation","level":3,"title":"Coordinate System Transformation","text":"<p>DAYMET uses Lambert Conformal Conic projection, which must be aligned with DEM data:</p> <pre><code>def reproject_daymet_to_dem(daymet_file, dem_file, output_file):\n    \"\"\"\n    Reproject DAYMET data to match DEM coordinate system\n    \"\"\"\n\n    import rasterio\n    from rasterio.warp import calculate_default_transform, reproject\n\n    # Open source and destination\n    with rasterio.open(daymet_file) as src:\n        with rasterio.open(dem_file) as dem:\n\n            # Calculate transform\n            transform, width, height = calculate_default_transform(\n                src.crs,\n                dem.crs,\n                dem.width,\n                dem.height,\n                *dem.bounds\n            )\n\n            # Update profile\n            profile = dem.profile.copy()\n            profile.update({\n                'transform': transform,\n                'crs': dem.crs,\n                'width': width,\n                'height': height\n            })\n\n            # Reproject\n            with rasterio.open(output_file, 'w', **profile) as dst:\n                for band_idx in range(1, src.count + 1):\n                    reproject(\n                        source=rasterio.band(src, band_idx),\n                        destination=rasterio.band(dst, band_idx),\n                        src_transform=src.transform,\n                        src_crs=src.crs,\n                        dst_transform=transform,\n                        dst_crs=dem.crs,\n                        resampling=rasterio.enums.Resampling.bilinear\n                    )\n\n    return output_file\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#spatial-interpolation","level":3,"title":"Spatial Interpolation","text":"<p>When DAYMET resolution (1 km) differs from DEM resolution:</p> <pre><code>def interpolate_climate_to_dem(climate_data, dem, method='bilinear'):\n    \"\"\"\n    Interpolate climate data to DEM resolution\n\n    Methods:\n    - 'bilinear': Smooth interpolation (default)\n    - 'cubic': Smoother interpolation\n    - 'nearest': Preserve exact values\n    - 'kriging': Geostatistical interpolation\n    \"\"\"\n\n    from scipy.interpolate import RegularGridInterpolator\n\n    if method in ['bilinear', 'cubic', 'nearest']:\n        # Simple interpolation\n        interp_func = RegularGridInterpolator(\n            (climate_data.y, climate_data.x),\n            climate_data.values,\n            method=method,\n            bounds_error=False,\n            fill_value=None\n        )\n\n        # Create DEM grid\n        dem_points = np.stack(\n            np.meshgrid(dem.y, dem.x, indexing='ij'),\n            axis=-1\n        )\n\n        # Interpolate\n        interpolated = interp_func(dem_points)\n\n    elif method == 'kriging':\n        # Geostatistical interpolation\n        from pykrige import OrdinaryKriging\n\n        ok = OrdinaryKriging(\n            climate_data.x.flatten(),\n            climate_data.y.flatten(),\n            climate_data.values.flatten(),\n            variogram_model='spherical'\n        )\n\n        interpolated, variance = ok.execute(\n            'grid',\n            dem.x,\n            dem.y\n        )\n\n    return interpolated\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#temporal-aggregation","level":2,"title":"Temporal Aggregation","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#monthly-summaries","level":3,"title":"Monthly Summaries","text":"<p>Converting daily DAYMET data to monthly values:</p> <pre><code>def aggregate_daymet_monthly(daily_data, variable, year):\n    \"\"\"\n    Aggregate daily DAYMET to monthly values\n\n    Aggregation method depends on variable:\n    - Temperature: mean\n    - Precipitation: sum\n    - Vapor pressure: mean\n    - Radiation: mean or sum\n    \"\"\"\n\n    import pandas as pd\n\n    # Create date index\n    dates = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D')\n\n    # Convert to xarray with time dimension\n    data_with_time = xr.DataArray(\n        daily_data,\n        dims=['time', 'y', 'x'],\n        coords={'time': dates}\n    )\n\n    # Aggregation rules\n    aggregation_rules = {\n        'tmin': 'mean',\n        'tmax': 'mean',\n        'prcp': 'sum',\n        'vp': 'mean',\n        'srad': 'mean',\n        'swe': 'mean'\n    }\n\n    # Apply aggregation\n    method = aggregation_rules.get(variable, 'mean')\n\n    if method == 'sum':\n        monthly = data_with_time.resample(time='1M').sum()\n    else:\n        monthly = data_with_time.resample(time='1M').mean()\n\n    return monthly\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#annual-climatologies","level":3,"title":"Annual Climatologies","text":"<p>Creating long-term climate normals:</p> <pre><code>def calculate_climate_normals(start_year, end_year, variable, bbox):\n    \"\"\"\n    Calculate 30-year climate normals\n\n    Standard periods:\n    - 1981-2010\n    - 1991-2020\n    \"\"\"\n\n    all_data = []\n\n    for year in range(start_year, end_year + 1):\n        yearly_data = get_daymet_subset(bbox, year, [variable])\n        all_data.append(yearly_data)\n\n    # Stack years\n    stacked = np.stack(all_data, axis=0)\n\n    # Calculate statistics\n    normals = {\n        'mean': np.mean(stacked, axis=0),\n        'std': np.std(stacked, axis=0),\n        'min': np.min(stacked, axis=0),\n        'max': np.max(stacked, axis=0),\n        'percentile_10': np.percentile(stacked, 10, axis=0),\n        'percentile_90': np.percentile(stacked, 90, axis=0)\n    }\n\n    return normals\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#quality-control","level":2,"title":"Quality Control","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#data-validation","level":3,"title":"Data Validation","text":"<pre><code>def validate_climate_data(data, variable):\n    \"\"\"\n    Quality control for climate data\n    \"\"\"\n\n    # Physical limits\n    limits = {\n        'tmin': (-60, 50),    # ¬∞C\n        'tmax': (-50, 60),    # ¬∞C\n        'prcp': (0, 500),      # mm/day\n        'vp': (0, 10000),      # Pa\n        'srad': (0, 1000),     # W/m¬≤\n    }\n\n    # Check range\n    vmin, vmax = limits.get(variable, (-np.inf, np.inf))\n    out_of_range = (data &lt; vmin) | (data &gt; vmax)\n\n    if np.any(out_of_range):\n        print(f\"Warning: {np.sum(out_of_range)} values out of range for {variable}\")\n\n    # Check for missing data\n    missing = np.isnan(data)\n    if np.any(missing):\n        print(f\"Warning: {np.sum(missing)} missing values for {variable}\")\n\n    # Check temporal consistency\n    if len(data.shape) &gt; 2:  # Has time dimension\n        # Check for unrealistic jumps\n        daily_diff = np.diff(data, axis=0)\n        max_change = {\n            'tmin': 20,  # ¬∞C/day\n            'tmax': 20,  # ¬∞C/day\n            'prcp': 200  # mm/day\n        }\n\n        threshold = max_change.get(variable, np.inf)\n        jumps = np.abs(daily_diff) &gt; threshold\n        if np.any(jumps):\n            print(f\"Warning: {np.sum(jumps)} unrealistic daily changes for {variable}\")\n\n    return {\n        'out_of_range': np.sum(out_of_range),\n        'missing': np.sum(missing),\n        'suspicious_changes': np.sum(jumps) if len(data.shape) &gt; 2 else 0\n    }\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#alternative-climate-datasets","level":2,"title":"Alternative Climate Datasets","text":"","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#prism","level":3,"title":"PRISM","text":"<pre><code>def get_prism_data(bbox, year_month, variable):\n    \"\"\"\n    Alternative: PRISM climate data (CONUS only)\n    4km resolution, 1895-present\n    \"\"\"\n\n    base_url = \"http://services.nacse.org/prism/data/public/4km\"\n\n    # Variable codes\n    var_codes = {\n        'ppt': 'ppt',    # Precipitation\n        'tmean': 'tmean', # Mean temperature\n        'tmin': 'tmin',   # Minimum temperature\n        'tmax': 'tmax',   # Maximum temperature\n        'tdmean': 'tdmean', # Mean dewpoint\n        'vpdmin': 'vpdmin', # Minimum VPD\n        'vpdmax': 'vpdmax'  # Maximum VPD\n    }\n\n    # Download and process\n    # Implementation details...\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#era5","level":3,"title":"ERA5","text":"<pre><code>def get_era5_data(bbox, date_range, variables):\n    \"\"\"\n    Alternative: ERA5 reanalysis (global)\n    0.25¬∞ resolution (~28 km), 1979-present\n    Hourly data available\n    \"\"\"\n\n    import cdsapi\n\n    client = cdsapi.Client()\n\n    request = {\n        'product_type': 'reanalysis',\n        'format': 'netcdf',\n        'variable': variables,\n        'year': date_range.year,\n        'month': date_range.month,\n        'day': date_range.day,\n        'time': ['00:00', '06:00', '12:00', '18:00'],\n        'area': [bbox[3], bbox[0], bbox[1], bbox[2]],  # N, W, S, E\n    }\n\n    # Download from Copernicus Climate Data Store\n    # Implementation details...\n</code></pre>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/climate-integration/#references","level":2,"title":"References","text":"<ul> <li> <p>Thornton, P. E., et al. (2020). Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4. ORNL DAAC.</p> </li> <li> <p>Hamon, W. R. (1963). Computation of direct runoff amounts from storm rainfall. International Association of Scientific Hydrology Publication, 63, 52-62.</p> </li> <li> <p>Priestley, C. H. B., &amp; Taylor, R. J. (1972). On the assessment of surface heat flux and evaporation using large-scale parameters. Monthly Weather Review, 100(2), 81-92.</p> </li> </ul> <p>Next: Topographic Analysis ‚Üí</p>","path":["Algorithms","Climate Data Integration"],"tags":[]},{"location":"algorithms/eemt-calculations/","level":1,"title":"EEMT Calculation Algorithms","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#overview","level":2,"title":"Overview","text":"<p>The Effective Energy and Mass Transfer (EEMT) framework quantifies the energy available to drive Critical Zone processes. EEMT combines biological energy storage through primary production (E<sub>BIO</sub>) with precipitation-delivered thermal energy (E<sub>PPT</sub>) to predict landscape evolution, soil formation, and ecosystem function.</p>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#core-eemt-equation","level":2,"title":"Core EEMT Equation","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#fundamental-formula","level":3,"title":"Fundamental Formula","text":"\\[\\text{EEMT} = E_{BIO} + E_{PPT} \\quad \\text{[MJ m}^{-2} \\text{ yr}^{-1}\\text{]}\\] <p>Where: - E<sub>BIO</sub> = Biological energy from net primary production - E<sub>PPT</sub> = Thermal energy from effective precipitation</p> <p>This represents the total energy flux available for: - Chemical weathering reactions - Soil formation processes - Biological activity - Carbon sequestration - Nutrient cycling</p>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#biological-energy-component-ebio","level":2,"title":"Biological Energy Component (E<sub>BIO</sub>)","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#mathematical-formulation","level":3,"title":"Mathematical Formulation","text":"\\[E_{BIO} = \\text{NPP} \\times h_{BIO}\\] <p>Where: - NPP = Net Primary Production [kg m‚Åª¬≤ yr‚Åª¬π] - h<sub>BIO</sub> = Specific enthalpy of biomass (22 √ó 10‚Å∂ J kg‚Åª¬π)</p>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#npp-estimation-methods","level":3,"title":"NPP Estimation Methods","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#1-climate-based-npp-lieth-model","level":4,"title":"1. Climate-Based NPP (Lieth Model)","text":"<pre><code>def calculate_npp_lieth(mean_annual_temp, annual_precip):\n    \"\"\"\n    Lieth (1975) Miami model for NPP estimation\n\n    Parameters:\n    - mean_annual_temp: Mean annual temperature [¬∞C]\n    - annual_precip: Annual precipitation [mm]\n\n    Returns:\n    - npp: Net primary production [g m‚Åª¬≤ yr‚Åª¬π]\n    \"\"\"\n\n    # Temperature-limited NPP\n    npp_temp = 3000 / (1 + np.exp(1.315 - 0.119 * mean_annual_temp))\n\n    # Precipitation-limited NPP\n    npp_precip = 3000 * (1 - np.exp(-0.000664 * annual_precip))\n\n    # Take minimum (Liebig's law)\n    npp = np.minimum(npp_temp, npp_precip)\n\n    return npp\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#2-modis-npp-integration","level":4,"title":"2. MODIS NPP Integration","text":"<pre><code>def integrate_modis_npp(modis_npp_file, scale_factor=0.0001):\n    \"\"\"\n    Use MODIS NPP product (MOD17A3)\n\n    Parameters:\n    - modis_npp_file: Path to MODIS NPP data\n    - scale_factor: MODIS scale factor\n\n    Returns:\n    - annual_npp: Annual NPP [kg m‚Åª¬≤ yr‚Åª¬π]\n    \"\"\"\n\n    import rasterio\n\n    with rasterio.open(modis_npp_file) as src:\n        npp_raw = src.read(1)\n\n    # Apply scale factor and convert units\n    # MODIS NPP is in kg C m‚Åª¬≤ yr‚Åª¬π\n    # Convert to total biomass (assume 45% carbon content)\n    annual_npp = npp_raw * scale_factor / 0.45\n\n    # Quality control\n    annual_npp[annual_npp &lt; 0] = 0\n    annual_npp[annual_npp &gt; 5] = 5  # Max ~5 kg m‚Åª¬≤ yr‚Åª¬π\n\n    return annual_npp\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#3-topographic-npp-model","level":4,"title":"3. Topographic NPP Model","text":"<pre><code>def calculate_npp_topographic(elevation, northness, base_npp=0.5):\n    \"\"\"\n    Whittaker &amp; Niering (1975) topographic NPP model\n\n    NPP = Œ± √ó elevation + Œ≤ √ó northness + Œ≥\n\n    Parameters:\n    - elevation: Elevation [m]\n    - northness: Topographic northness index [-1 to 1]\n    - base_npp: Baseline NPP [kg m‚Åª¬≤ yr‚Åª¬π]\n\n    Returns:\n    - npp: Topographically-adjusted NPP [kg m‚Åª¬≤ yr‚Åª¬π]\n    \"\"\"\n\n    # Empirical coefficients (site-specific calibration recommended)\n    alpha = 0.00039  # kg m‚Åª¬≤ yr‚Åª¬π per meter elevation\n    beta = 0.346     # kg m‚Åª¬≤ yr‚Åª¬π per unit northness\n    gamma = -0.187   # kg m‚Åª¬≤ yr‚Åª¬π baseline adjustment\n\n    # Calculate NPP\n    npp = alpha * elevation + beta * northness + gamma + base_npp\n\n    # Constrain to reasonable range\n    npp = np.maximum(0.1, np.minimum(5.0, npp))\n\n    return npp\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#ebio-calculation","level":3,"title":"E<sub>BIO</sub> Calculation","text":"<pre><code>def calculate_e_bio(npp, time_integration='annual'):\n    \"\"\"\n    Calculate biological energy component\n\n    Parameters:\n    - npp: Net primary production [kg m‚Åª¬≤ yr‚Åª¬π]\n    - time_integration: 'annual' or 'monthly'\n\n    Returns:\n    - e_bio: Biological energy flux [MJ m‚Åª¬≤ yr‚Åª¬π]\n    \"\"\"\n\n    # Specific enthalpy of biomass\n    h_bio = 22e6  # J/kg (from bomb calorimetry studies)\n\n    if time_integration == 'annual':\n        # Direct calculation\n        e_bio = npp * h_bio / 1e6  # Convert J to MJ\n\n    elif time_integration == 'monthly':\n        # Account for seasonal variation\n        monthly_fraction = get_phenology_fraction()  # 12 values summing to 1\n        e_bio_monthly = []\n\n        for month in range(12):\n            npp_month = npp * monthly_fraction[month]\n            e_bio_month = npp_month * h_bio / 1e6\n            e_bio_monthly.append(e_bio_month)\n\n        e_bio = np.sum(e_bio_monthly, axis=0)\n\n    return e_bio\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#precipitation-energy-component-eppt","level":2,"title":"Precipitation Energy Component (E<sub>PPT</sub>)","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#mathematical-formulation_1","level":3,"title":"Mathematical Formulation","text":"\\[E_{PPT} = \\rho_w \\times P_{eff} \\times c_w \\times \\Delta T\\] <p>Where: - œÅ<sub>w</sub> = Density of water (1000 kg m‚Åª¬≥) - P<sub>eff</sub> = Effective precipitation [m yr‚Åª¬π] - c<sub>w</sub> = Specific heat of water (4180 J kg‚Åª¬π K‚Åª¬π) - ŒîT = Temperature above freezing [K]</p>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#effective-precipitation-calculation","level":3,"title":"Effective Precipitation Calculation","text":"<pre><code>def calculate_effective_precipitation(precipitation, pet, aet=None):\n    \"\"\"\n    Calculate effective precipitation (available for subsurface processes)\n\n    Parameters:\n    - precipitation: Total precipitation [mm]\n    - pet: Potential evapotranspiration [mm]\n    - aet: Actual evapotranspiration [mm] (optional)\n\n    Returns:\n    - p_eff: Effective precipitation [mm]\n    \"\"\"\n\n    if aet is not None:\n        # Use actual ET if available\n        p_eff = precipitation - aet\n    else:\n        # Estimate using Budyko curve\n        aridity_index = pet / np.maximum(precipitation, 1)\n\n        # Fu's equation (Fu, 1981)\n        omega = 2.6  # Shape parameter\n        evap_ratio = 1 + aridity_index - (1 + aridity_index**omega)**(1/omega)\n        aet = precipitation * evap_ratio\n\n        p_eff = precipitation - aet\n\n    # Ensure non-negative\n    p_eff = np.maximum(0, p_eff)\n\n    return p_eff\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#temperature-delta-calculation","level":3,"title":"Temperature Delta Calculation","text":"<pre><code>def calculate_temperature_delta(temperature, phase='liquid'):\n    \"\"\"\n    Calculate temperature difference from reference\n\n    Parameters:\n    - temperature: Temperature [¬∞C]\n    - phase: 'liquid' or 'solid' (snow/ice)\n\n    Returns:\n    - delta_t: Temperature above reference [K]\n    \"\"\"\n\n    if phase == 'liquid':\n        # Reference is freezing point\n        reference_temp = 0.0  # ¬∞C\n        delta_t = np.maximum(0, temperature - reference_temp)\n\n    elif phase == 'solid':\n        # For snow, use temperature below freezing\n        reference_temp = 0.0  # ¬∞C\n        delta_t = np.maximum(0, reference_temp - temperature)\n        # Account for latent heat of fusion\n        delta_t = delta_t * 0.1  # Reduced factor for snow\n\n    return delta_t\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#eppt-calculation","level":3,"title":"E<sub>PPT</sub> Calculation","text":"<pre><code>def calculate_e_ppt(precipitation, temperature, et, method='budyko'):\n    \"\"\"\n    Calculate precipitation energy component\n\n    Parameters:\n    - precipitation: Precipitation [mm yr‚Åª¬π]\n    - temperature: Mean temperature [¬∞C]\n    - et: Evapotranspiration [mm yr‚Åª¬π]\n    - method: 'budyko' or 'penman-monteith'\n\n    Returns:\n    - e_ppt: Precipitation energy [MJ m‚Åª¬≤ yr‚Åª¬π]\n    \"\"\"\n\n    # Calculate effective precipitation\n    if method == 'budyko':\n        p_eff = calculate_effective_precipitation(precipitation, et)\n    else:  # penman-monteith\n        p_eff = precipitation - et  # ET already calculated\n\n    # Convert mm to m\n    p_eff_m = p_eff / 1000\n\n    # Water properties\n    rho_water = 1000  # kg/m¬≥\n    c_water = 4180    # J/(kg¬∑K)\n\n    # Calculate temperature delta\n    delta_t = calculate_temperature_delta(temperature)\n\n    # Calculate energy flux\n    # Mass flux: kg/(m¬≤¬∑yr) = rho * depth\n    mass_flux = rho_water * p_eff_m\n\n    # Energy: J/(m¬≤¬∑yr) = mass_flux * c_water * delta_t\n    e_ppt_j = mass_flux * c_water * delta_t\n\n    # Convert to MJ\n    e_ppt = e_ppt_j / 1e6\n\n    return e_ppt\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#integrated-eemt-calculation","level":2,"title":"Integrated EEMT Calculation","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#complete-eemt-workflow","level":3,"title":"Complete EEMT Workflow","text":"<pre><code>def calculate_eemt_complete(dem, climate_data, vegetation_data=None, \n                           method='topographic'):\n    \"\"\"\n    Complete EEMT calculation with all components\n\n    Parameters:\n    - dem: Digital elevation model\n    - climate_data: Dict with temperature, precipitation, radiation\n    - vegetation_data: Optional vegetation inputs (LAI, NDVI, etc.)\n    - method: 'traditional', 'topographic', or 'vegetation'\n\n    Returns:\n    - eemt_components: Dict with EEMT, E_BIO, E_PPT\n    \"\"\"\n\n    # Extract climate variables\n    temp = climate_data['temperature']\n    precip = climate_data['precipitation']\n\n    # Calculate topographic indices\n    slope = calculate_slope(dem)\n    aspect = calculate_aspect(dem)\n    twi = calculate_twi(dem)\n    mcwi = calculate_mcwi(twi, precip)\n\n    # Method-specific calculations\n    if method == 'traditional':\n        # Simple climate-based approach\n        npp = calculate_npp_lieth(temp.mean(), precip.sum())\n        pet = calculate_pet_hamon(temp, daylight_hours=12)\n        p_eff = calculate_effective_precipitation(precip, pet)\n\n    elif method == 'topographic':\n        # Include topographic controls\n        northness = calculate_northness(aspect, slope)\n        npp = calculate_npp_topographic(dem, northness)\n\n        # Redistribute precipitation\n        p_redistributed = redistribute_water_mcwi(precip, mcwi)\n        pet = calculate_pet_priestley_taylor(climate_data['radiation'], temp)\n        p_eff = calculate_effective_precipitation(p_redistributed, pet)\n\n    elif method == 'vegetation':\n        # Full vegetation integration\n        if vegetation_data and 'lai' in vegetation_data:\n            lai = vegetation_data['lai']\n        else:\n            lai = estimate_lai_from_climate(temp, precip)\n\n        # Vegetation-modified NPP\n        canopy_height = lai * 2.5  # Simple approximation\n        npp = calculate_npp_vegetation(canopy_height)\n\n        # Vegetation-modified ET\n        aet = calculate_aet_penman_monteith(temp, climate_data['humidity'], \n                                           climate_data['wind'], \n                                           climate_data['radiation'], lai)\n        p_eff = precip - aet\n\n    # Calculate energy components\n    e_bio = calculate_e_bio(npp)\n    e_ppt = calculate_e_ppt(precip, temp, p_eff)\n\n    # Total EEMT\n    eemt = e_bio + e_ppt\n\n    # Apply quality control\n    eemt = apply_eemt_limits(eemt)\n\n    return {\n        'eemt': eemt,\n        'e_bio': e_bio,\n        'e_ppt': e_ppt,\n        'npp': npp,\n        'p_eff': p_eff,\n        'twi': twi,\n        'mcwi': mcwi\n    }\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#eemt-thresholds-and-regimes","level":2,"title":"EEMT Thresholds and Regimes","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#energy-limited-vs-water-limited-systems","level":3,"title":"Energy-Limited vs Water-Limited Systems","text":"<pre><code>def classify_eemt_regime(eemt, precipitation, temperature):\n    \"\"\"\n    Classify landscape into EEMT regimes\n\n    Based on Rasmussen et al. (2014) thresholds\n    \"\"\"\n\n    # Key threshold\n    eemt_threshold = 70  # MJ m‚Åª¬≤ yr‚Åª¬π\n\n    # Additional criteria\n    aridity = calculate_aridity_index(precipitation, temperature)\n\n    regime = np.empty_like(eemt, dtype='U20')\n\n    # Water-limited (below threshold)\n    water_limited = (eemt &lt; eemt_threshold) | (aridity &gt; 1.5)\n    regime[water_limited] = 'water_limited'\n\n    # Energy-limited (above threshold)\n    energy_limited = (eemt &gt;= eemt_threshold) &amp; (aridity &lt; 0.7)\n    regime[energy_limited] = 'energy_limited'\n\n    # Transitional\n    transitional = ~(water_limited | energy_limited)\n    regime[transitional] = 'transitional'\n\n    # Sub-classifications\n    regime[(regime == 'water_limited') &amp; (eemt &lt; 10)] = 'hyperarid'\n    regime[(regime == 'energy_limited') &amp; (eemt &gt; 150)] = 'humid_tropical'\n\n    return regime\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#eemt-driven-process-rates","level":3,"title":"EEMT-Driven Process Rates","text":"<pre><code>def predict_process_rates(eemt):\n    \"\"\"\n    Predict Critical Zone process rates from EEMT\n\n    Returns:\n    - Dictionary of predicted rates\n    \"\"\"\n\n    rates = {}\n\n    # Soil production rate (exponential model)\n    # Based on Pelletier &amp; Rasmussen (2009)\n    P0 = 0.05  # mm/yr maximum rate\n    k = 0.02   # Decay constant\n    rates['soil_production'] = P0 * np.exp(-k * eemt)\n\n    # Chemical denudation rate (linear model)\n    # Based on Rasmussen et al. (2011)\n    rates['chemical_denudation'] = 0.15 * eemt + 5  # t km‚Åª¬≤ yr‚Åª¬π\n\n    # Physical erosion rate (power law)\n    # Inverse relationship in high EEMT\n    if isinstance(eemt, np.ndarray):\n        rates['physical_erosion'] = np.where(\n            eemt &lt; 70,\n            100 * eemt**(-0.5),  # High erosion in dry areas\n            20 * eemt**(-0.8)    # Low erosion in wet areas\n        )\n    else:\n        if eemt &lt; 70:\n            rates['physical_erosion'] = 100 * eemt**(-0.5)\n        else:\n            rates['physical_erosion'] = 20 * eemt**(-0.8)\n\n    # Biomass accumulation (logistic model)\n    K = 50  # kg/m¬≤ carrying capacity\n    r = 0.05  # Growth rate\n    rates['biomass'] = K / (1 + np.exp(-r * (eemt - 70)))\n\n    return rates\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#uncertainty-quantification","level":2,"title":"Uncertainty Quantification","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#monte-carlo-uncertainty-analysis","level":3,"title":"Monte Carlo Uncertainty Analysis","text":"<pre><code>def eemt_uncertainty_analysis(climate_data, dem, n_simulations=1000):\n    \"\"\"\n    Quantify EEMT uncertainty through Monte Carlo simulation\n\n    Parameters:\n    - climate_data: Base climate data\n    - dem: Digital elevation model\n    - n_simulations: Number of Monte Carlo runs\n\n    Returns:\n    - uncertainty_stats: Statistical measures of uncertainty\n    \"\"\"\n\n    eemt_simulations = []\n\n    for i in range(n_simulations):\n        # Perturb input parameters\n        temp_perturbed = climate_data['temperature'] + np.random.normal(0, 1)\n        precip_perturbed = climate_data['precipitation'] * np.random.lognormal(0, 0.1)\n\n        # Vary NPP model parameters\n        npp_scaling = np.random.uniform(0.8, 1.2)\n\n        # Vary ET model parameters\n        et_scaling = np.random.uniform(0.9, 1.1)\n\n        # Calculate EEMT with perturbed inputs\n        climate_perturbed = {\n            'temperature': temp_perturbed,\n            'precipitation': precip_perturbed,\n            'radiation': climate_data['radiation']\n        }\n\n        eemt_result = calculate_eemt_complete(dem, climate_perturbed)\n        eemt_simulations.append(eemt_result['eemt'])\n\n    # Calculate statistics\n    eemt_stack = np.stack(eemt_simulations, axis=0)\n\n    uncertainty_stats = {\n        'mean': np.mean(eemt_stack, axis=0),\n        'std': np.std(eemt_stack, axis=0),\n        'cv': np.std(eemt_stack, axis=0) / np.mean(eemt_stack, axis=0),\n        'percentile_5': np.percentile(eemt_stack, 5, axis=0),\n        'percentile_95': np.percentile(eemt_stack, 95, axis=0),\n        'confidence_interval': np.percentile(eemt_stack, [2.5, 97.5], axis=0)\n    }\n\n    return uncertainty_stats\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#sensitivity-analysis","level":3,"title":"Sensitivity Analysis","text":"<pre><code>def eemt_sensitivity_analysis(base_inputs, parameter_ranges):\n    \"\"\"\n    Perform sensitivity analysis on EEMT parameters\n\n    Parameters:\n    - base_inputs: Baseline input values\n    - parameter_ranges: Dict of parameter ranges to test\n\n    Returns:\n    - sensitivity: Parameter sensitivity indices\n    \"\"\"\n\n    sensitivity = {}\n    base_eemt = calculate_eemt_complete(**base_inputs)['eemt']\n\n    for param, (low, high) in parameter_ranges.items():\n        # Test low value\n        inputs_low = base_inputs.copy()\n        inputs_low[param] = low\n        eemt_low = calculate_eemt_complete(**inputs_low)['eemt']\n\n        # Test high value\n        inputs_high = base_inputs.copy()\n        inputs_high[param] = high\n        eemt_high = calculate_eemt_complete(**inputs_high)['eemt']\n\n        # Calculate sensitivity index\n        delta_param = (high - low) / base_inputs[param]\n        delta_eemt = (eemt_high - eemt_low) / base_eemt\n\n        sensitivity[param] = delta_eemt / delta_param\n\n    return sensitivity\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#quality-control","level":2,"title":"Quality Control","text":"","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#physical-constraints","level":3,"title":"Physical Constraints","text":"<pre><code>def apply_eemt_limits(eemt):\n    \"\"\"\n    Apply physical constraints to EEMT values\n\n    Parameters:\n    - eemt: Calculated EEMT values\n\n    Returns:\n    - eemt_constrained: Physically reasonable EEMT\n    \"\"\"\n\n    # Physical limits based on global observations\n    MIN_EEMT = 0.1    # Minimum in extreme deserts\n    MAX_EEMT = 500    # Maximum in tropical rainforests\n\n    # Apply constraints\n    eemt_constrained = np.clip(eemt, MIN_EEMT, MAX_EEMT)\n\n    # Check for anomalies\n    if isinstance(eemt, np.ndarray):\n        # Identify outliers (&gt; 3 std from mean)\n        mean = np.nanmean(eemt_constrained)\n        std = np.nanstd(eemt_constrained)\n        outliers = np.abs(eemt_constrained - mean) &gt; 3 * std\n\n        if np.any(outliers):\n            print(f\"Warning: {np.sum(outliers)} outlier pixels detected\")\n\n    return eemt_constrained\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#validation-metrics","level":3,"title":"Validation Metrics","text":"<pre><code>def validate_eemt_results(eemt_calculated, validation_data):\n    \"\"\"\n    Validate EEMT against field observations\n\n    Parameters:\n    - eemt_calculated: Calculated EEMT values\n    - validation_data: Field measurements or reference data\n\n    Returns:\n    - metrics: Validation metrics\n    \"\"\"\n\n    from sklearn.metrics import mean_squared_error, r2_score\n\n    # Ensure same shape\n    if eemt_calculated.shape != validation_data.shape:\n        validation_data = resample_to_match(validation_data, eemt_calculated)\n\n    # Remove NaN values\n    mask = ~(np.isnan(eemt_calculated) | np.isnan(validation_data))\n    calc_valid = eemt_calculated[mask]\n    obs_valid = validation_data[mask]\n\n    # Calculate metrics\n    metrics = {\n        'rmse': np.sqrt(mean_squared_error(obs_valid, calc_valid)),\n        'mae': np.mean(np.abs(obs_valid - calc_valid)),\n        'r2': r2_score(obs_valid, calc_valid),\n        'bias': np.mean(calc_valid - obs_valid),\n        'correlation': np.corrcoef(obs_valid, calc_valid)[0, 1],\n        'n_samples': len(calc_valid)\n    }\n\n    # Relative metrics\n    metrics['relative_rmse'] = metrics['rmse'] / np.mean(obs_valid)\n    metrics['relative_bias'] = metrics['bias'] / np.mean(obs_valid)\n\n    return metrics\n</code></pre>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/eemt-calculations/#references","level":2,"title":"References","text":"<ul> <li> <p>Rasmussen, C., et al. (2014). Quantifying topographic and vegetation effects on the transfer of energy and mass to the Critical Zone. Vadose Zone Journal, 14(11).</p> </li> <li> <p>Pelletier, J. D., &amp; Rasmussen, C. (2009). Geomorphically based predictive mapping of soil thickness in upland watersheds. Water Resources Research, 45(9).</p> </li> <li> <p>Lieth, H. (1975). Modeling the primary productivity of the world. In Primary productivity of the biosphere (pp. 237-263). Springer.</p> </li> <li> <p>Fu, B. P. (1981). On the calculation of the evaporation from land surface. Scientia Atmospherica Sinica, 5(1), 23-31.</p> </li> </ul> <p>Next: Practical Workflows ‚Üí</p>","path":["Algorithms","EEMT Calculation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/","level":1,"title":"Solar Radiation Algorithms","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#overview","level":2,"title":"Overview","text":"<p>Solar radiation is the primary energy input driving all Critical Zone processes. The EEMT framework uses the GRASS GIS <code>r.sun</code> module with multi-threaded processing (<code>nprocs</code> parameter) to calculate topographically-modified solar radiation, accounting for slope, aspect, atmospheric conditions, and terrain shadowing effects.</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#mathematical-foundation","level":2,"title":"Mathematical Foundation","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#clear-sky-solar-radiation","level":3,"title":"Clear-Sky Solar Radiation","text":"<p>The theoretical maximum solar radiation reaching Earth's atmosphere is determined by:</p> \\[I_0 = S_c \\left( \\frac{r_0}{r} \\right)^2\\] <p>Where: - I‚ÇÄ = Extraterrestrial radiation [W m‚Åª¬≤] - S<sub>c</sub> = Solar constant (1367 W m‚Åª¬≤) - r‚ÇÄ/r = Earth-Sun distance ratio (varies seasonally)</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#atmospheric-attenuation","level":3,"title":"Atmospheric Attenuation","text":"<p>Solar radiation is attenuated as it passes through the atmosphere. The EEMT framework uses the Linke Turbidity Factor to account for atmospheric effects:</p> \\[T_L = \\frac{\\delta_{atm}}{\\delta_{clean}}\\] <p>Where: - T<sub>L</sub> = Linke turbidity factor [dimensionless] - Œ¥<sub>atm</sub> = Optical thickness of real atmosphere - Œ¥<sub>clean</sub> = Optical thickness of clean, dry atmosphere</p> <p>Typical Linke turbidity values: - 1.0-2.0: Very clean, cold air (Arctic, high mountains) - 2.0-3.0: Clean air (rural areas, moderate elevations) - 3.0-4.0: Moderate turbidity (temperate regions) - 4.0-5.0: Turbid atmosphere (urban, humid areas) - 5.0-8.0: Very turbid (polluted urban, tropical)</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#core-algorithm-rsun","level":2,"title":"Core Algorithm: r.sun","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#algorithm-components","level":3,"title":"Algorithm Components","text":"<p>The <code>r.sun</code> module calculates solar radiation using three primary components:</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#1-direct-beam-radiation","level":4,"title":"1. Direct (Beam) Radiation","text":"<p>Direct radiation reaching a tilted surface:</p> \\[I_{direct} = I_0 \\cdot \\tau_b \\cdot \\cos(\\theta_i)\\] <p>Where: - œÑ<sub>b</sub> = Beam transmittance through atmosphere - Œ∏<sub>i</sub> = Angle of incidence between sun rays and surface normal</p> <p>The angle of incidence is calculated as:</p> \\[\\cos(\\theta_i) = \\cos(\\beta)\\cos(Z) + \\sin(\\beta)\\sin(Z)\\cos(\\phi_s - \\phi_n)\\] <p>Where: - Œ≤ = Surface slope angle [degrees] - Z = Solar zenith angle [degrees] - œÜ<sub>s</sub> = Solar azimuth [degrees] - œÜ<sub>n</sub> = Surface aspect [degrees]</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#2-diffuse-sky-radiation","level":4,"title":"2. Diffuse (Sky) Radiation","text":"<p>Diffuse radiation from atmospheric scattering:</p> \\[I_{diffuse} = I_0 \\cdot \\tau_d \\cdot F_{sky}\\] <p>Where: - œÑ<sub>d</sub> = Diffuse transmittance - F<sub>sky</sub> = Sky view factor (portion of sky visible from surface)</p> <p>The sky view factor accounts for terrain obstruction:</p> \\[F_{sky} = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\cos^2(H(\\phi)) \\, d\\phi\\] <p>Where H(œÜ) is the horizon angle in direction œÜ.</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#3-reflected-radiation","level":4,"title":"3. Reflected Radiation","text":"<p>Radiation reflected from surrounding terrain:</p> \\[I_{reflected} = \\rho \\cdot (I_{direct} + I_{diffuse}) \\cdot F_{terrain}\\] <p>Where: - œÅ = Surface albedo (reflectance) - F<sub>terrain</sub> = Terrain view factor</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#total-solar-radiation","level":3,"title":"Total Solar Radiation","text":"<p>The total radiation received by a surface is:</p> \\[I_{total} = I_{direct} + I_{diffuse} + I_{reflected}\\]","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#horizon-calculation","level":2,"title":"Horizon Calculation","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#horizon-effects","level":3,"title":"Horizon Effects","text":"<p>Terrain shadowing significantly affects solar radiation receipt. The horizon angle determines when direct sunlight is blocked:</p> \\[H(\\phi) = \\max_{d} \\left( \\arctan \\left( \\frac{z(d,\\phi) - z_0}{d} \\right) \\right)\\] <p>Where: - H(œÜ) = Horizon angle in direction œÜ - z(d,œÜ) = Elevation at distance d in direction œÜ - z‚ÇÄ = Elevation at calculation point - d = Distance from calculation point</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#shadow-calculation","level":3,"title":"Shadow Calculation","text":"<p>A point is in shadow when:</p> \\[h_{sun} &lt; H(\\phi_{sun})\\] <p>Where: - h<sub>sun</sub> = Solar elevation angle - H(œÜ<sub>sun</sub>) = Horizon angle in sun direction</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#implementation-details","level":2,"title":"Implementation Details","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#temporal-resolution","level":3,"title":"Temporal Resolution","text":"<p>The EEMT framework calculates solar radiation at regular time intervals throughout each day:</p> <pre><code>def calculate_daily_solar(dem, day_of_year, step_minutes=15):\n    \"\"\"\n    Calculate solar radiation for one day\n\n    Parameters:\n    - dem: Digital elevation model\n    - day_of_year: Julian day (1-365)\n    - step_minutes: Time step for calculation (3-60 minutes)\n\n    Returns:\n    - Daily total solar radiation [Wh m‚Åª¬≤]\n    \"\"\"\n\n    # Solar declination\n    declination = 23.45 * sin(360 * (284 + day_of_year) / 365)\n\n    # Calculate sunrise and sunset times\n    sunrise, sunset = calculate_sun_times(latitude, declination)\n\n    # Time loop\n    total_radiation = 0\n    for time in range(sunrise, sunset, step_minutes):\n\n        # Solar position\n        zenith, azimuth = solar_position(time, day_of_year, latitude, longitude)\n\n        # Check for shadows\n        if not in_shadow(zenith, azimuth, horizon):\n\n            # Calculate radiation components\n            direct = calculate_direct(zenith, azimuth, slope, aspect)\n            diffuse = calculate_diffuse(sky_view_factor)\n            reflected = calculate_reflected(albedo, terrain_view_factor)\n\n            # Sum components\n            instantaneous = direct + diffuse + reflected\n\n            # Integrate over time step\n            total_radiation += instantaneous * (step_minutes * 60)  # Convert to seconds\n\n    return total_radiation / 3600  # Convert to Wh\n</code></pre>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#annual-integration","level":3,"title":"Annual Integration","text":"<p>Annual solar radiation is calculated by summing daily values:</p> <pre><code>def calculate_annual_solar(dem, year, step_minutes=15):\n    \"\"\"\n    Calculate annual solar radiation\n\n    Returns:\n    - Annual solar radiation maps for each day\n    - Monthly summaries\n    - Annual total [MJ m‚Åª¬≤ yr‚Åª¬π]\n    \"\"\"\n\n    daily_radiation = []\n\n    # Calculate for each day\n    for day in range(1, 366):\n        daily = calculate_daily_solar(dem, day, step_minutes)\n        daily_radiation.append(daily)\n\n    # Monthly aggregation\n    monthly_radiation = aggregate_to_monthly(daily_radiation)\n\n    # Annual total\n    annual_total = sum(daily_radiation) * 0.0036  # Convert Wh to MJ\n\n    return {\n        'daily': daily_radiation,\n        'monthly': monthly_radiation, \n        'annual': annual_total\n    }\n</code></pre>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#parameter-optimization","level":2,"title":"Parameter Optimization","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#step-size-selection","level":3,"title":"Step Size Selection","text":"<p>The time step affects accuracy and computational cost:</p> Step Size Accuracy Computation Time Recommended Use 3 min Very High Very Long Research, small areas 5 min High Long Detailed analysis 10 min Good Moderate Standard analysis 15 min Adequate Fast Large areas 30 min Low Very Fast Preliminary analysis","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#atmospheric-parameters","level":3,"title":"Atmospheric Parameters","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#linke-turbidity-estimation","level":4,"title":"Linke Turbidity Estimation","text":"<p>For areas without measurements, estimate Linke turbidity from:</p> <pre><code>def estimate_linke_turbidity(elevation, latitude, month):\n    \"\"\"\n    Estimate Linke turbidity factor\n\n    Based on Remund et al. (2003) global climatology\n    \"\"\"\n\n    # Base turbidity at sea level\n    if abs(latitude) &gt; 60:\n        base_turbidity = 2.0  # Polar regions\n    elif abs(latitude) &gt; 35:\n        base_turbidity = 3.0  # Temperate regions\n    else:\n        base_turbidity = 4.0  # Tropical regions\n\n    # Elevation correction (decrease with altitude)\n    elevation_correction = -0.5 * (elevation / 1000)\n\n    # Seasonal variation\n    seasonal_factor = 1 + 0.3 * sin(2 * pi * (month - 3) / 12)\n\n    # Final estimate\n    linke = base_turbidity + elevation_correction\n    linke *= seasonal_factor\n\n    return max(1.0, min(8.0, linke))  # Constrain to valid range\n</code></pre>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#surface-albedo-values","level":4,"title":"Surface Albedo Values","text":"<p>Typical albedo values for different surfaces:</p> Surface Type Albedo Range EEMT Default Fresh snow 0.80-0.95 0.85 Old snow 0.40-0.70 0.55 Desert sand 0.30-0.40 0.35 Bare soil 0.15-0.25 0.20 Grassland 0.15-0.25 0.20 Forest 0.10-0.20 0.15 Water 0.05-0.10 0.07","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#topographic-effects","level":2,"title":"Topographic Effects","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#slope-and-aspect-modification","level":3,"title":"Slope and Aspect Modification","text":"<p>Radiation varies strongly with topography:</p> \\[R_{ratio} = \\frac{I_{slope}}{I_{flat}}\\] <p>This ratio can range from: - 0.0: Complete shading (north-facing cliffs) - 0.5: Reduced radiation (pole-facing slopes) - 1.0: Same as flat surface - 1.5: Enhanced radiation (equator-facing slopes) - 2.0+: Maximum enhancement (optimal slope/aspect)</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#elevation-effects","level":3,"title":"Elevation Effects","text":"<p>Solar radiation increases with elevation due to:</p> <ol> <li>Reduced atmospheric path length</li> <li>Lower atmospheric turbidity</li> <li>Decreased water vapor content</li> </ol> <p>Approximate increase: +7% per 1000m elevation</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#quality-control","level":2,"title":"Quality Control","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#validation-checks","level":3,"title":"Validation Checks","text":"<pre><code>def validate_solar_output(radiation_map):\n    \"\"\"\n    Quality control for solar radiation calculations\n    \"\"\"\n\n    checks = {\n        'range': check_physical_limits(radiation_map),\n        'spatial': check_spatial_consistency(radiation_map),\n        'temporal': check_temporal_patterns(radiation_map),\n        'topographic': check_topographic_effects(radiation_map)\n    }\n\n    return checks\n\ndef check_physical_limits(radiation):\n    \"\"\"\n    Ensure radiation values are physically reasonable\n    \"\"\"\n\n    # Maximum theoretical clear-sky radiation\n    max_theoretical = 1367  # W/m¬≤ (solar constant)\n\n    # Typical annual totals (MJ/m¬≤/yr)\n    min_annual = 1000  # Polar regions\n    max_annual = 9000  # Desert regions\n\n    return {\n        'instantaneous_valid': radiation.max() &lt; max_theoretical,\n        'annual_range_valid': min_annual &lt; radiation.sum() &lt; max_annual\n    }\n</code></pre>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#common-issues-and-solutions","level":2,"title":"Common Issues and Solutions","text":"","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#issue-unrealistic-radiation-values","level":3,"title":"Issue: Unrealistic radiation values","text":"<p>Symptoms: Radiation exceeds physical limits or shows artifacts</p> <p>Solutions: 1. Check DEM units and projection 2. Verify Linke turbidity is appropriate for region 3. Ensure time step is adequate for latitude 4. Check for DEM artifacts or errors</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#issue-edge-effects","level":3,"title":"Issue: Edge effects","text":"<p>Symptoms: Incorrect radiation near DEM boundaries</p> <p>Solutions: 1. Buffer DEM by horizon calculation distance 2. Use larger DEM extent than study area 3. Apply edge correction algorithms</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#issue-computational-performance","level":3,"title":"Issue: Computational performance","text":"<p>Symptoms: Calculations take excessive time</p> <p>Solutions: 1. Increase time step (reduce from 5 to 15 minutes) 2. Tile large DEMs for parallel processing 3. Use monthly representative days instead of full year 4. Enable GPU acceleration if available</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/solar-radiation/#references","level":2,"title":"References","text":"<ul> <li> <p>Hofierka, J., &amp; Suri, M. (2002). The solar radiation model for Open source GIS: implementation and applications. Proceedings of the Open source GIS-GRASS users conference.</p> </li> <li> <p>Remund, J., et al. (2003). Worldwide Linke turbidity information. Proceedings of ISES Solar World Congress.</p> </li> <li> <p>Ruiz‚ÄêArias, J. A., et al. (2009). A comparative analysis of DEM‚Äêbased models to estimate the solar radiation in mountainous terrain. International Journal of Geographical Information Science, 23(8), 1049-1076.</p> </li> </ul> <p>Next: Climate Integration ‚Üí</p>","path":["Algorithms","Solar Radiation Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/","level":1,"title":"Topographic Analysis Algorithms","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#overview","level":2,"title":"Overview","text":"<p>Topographic analysis in EEMT quantifies how landscape morphology controls water flow, energy distribution, and mass transfer processes. The framework implements multiple topographic indices including the Topographic Wetness Index (TWI), Mass Conservative Wetness Index (MCWI), and various flow routing algorithms.</p>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#topographic-wetness-index-twi","level":2,"title":"Topographic Wetness Index (TWI)","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#mathematical-definition","level":3,"title":"Mathematical Definition","text":"<p>The Topographic Wetness Index quantifies the tendency of water to accumulate at any point in the landscape:</p> \\[TWI = \\ln\\left(\\frac{A_s}{\\tan\\beta}\\right)\\] <p>Where: - A<sub>s</sub> = Specific contributing area [m¬≤/m] (upslope area per unit contour width) - Œ≤ = Local slope angle [radians] - tan(Œ≤) = Local slope gradient</p>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#physical-interpretation","level":3,"title":"Physical Interpretation","text":"<p>TWI values indicate moisture conditions:</p> TWI Range Interpretation Typical Locations &lt; 4 Very dry Ridges, steep slopes 4-6 Dry Upper hillslopes 6-8 Moderate Mid-slopes 8-10 Moist Lower slopes, flats 10-12 Wet Convergent areas &gt; 12 Very wet Valley bottoms, streams","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#implementation","level":3,"title":"Implementation","text":"<pre><code>def calculate_twi(dem, flow_accumulation=None):\n    \"\"\"\n    Calculate Topographic Wetness Index\n\n    Parameters:\n    - dem: Digital elevation model\n    - flow_accumulation: Pre-calculated flow accumulation (optional)\n\n    Returns:\n    - twi: Topographic wetness index\n    \"\"\"\n\n    # Calculate slope in radians\n    slope_degrees = calculate_slope(dem)\n    slope_radians = np.deg2rad(slope_degrees)\n\n    # Avoid division by zero\n    slope_radians = np.maximum(slope_radians, 0.001)\n\n    # Calculate flow accumulation if not provided\n    if flow_accumulation is None:\n        flow_accumulation = calculate_flow_accumulation(dem)\n\n    # Get cell size\n    cell_size = get_cell_size(dem)\n\n    # Specific contributing area (m¬≤/m)\n    # Flow accumulation √ó cell area / cell width\n    specific_area = flow_accumulation * cell_size\n\n    # TWI calculation\n    twi = np.log(specific_area / np.tan(slope_radians))\n\n    # Handle invalid values\n    twi[np.isinf(twi)] = np.nan\n    twi[twi &lt; 0] = 0\n\n    return twi\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#mass-conservative-wetness-index-mcwi","level":2,"title":"Mass Conservative Wetness Index (MCWI)","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#theoretical-foundation","level":3,"title":"Theoretical Foundation","text":"<p>The MCWI improves upon TWI by ensuring mass conservation across the landscape:</p> \\[MCWI_i = TWI_i \\times \\frac{\\bar{P}}{\\bar{TWI}}\\] <p>Where: - MCWI<sub>i</sub> = Mass conservative wetness index at location i - TWI<sub>i</sub> = Traditional wetness index at location i - PÃÑ = Mean precipitation over the domain - TWI = Mean TWI over the domain</p> <p>This normalization ensures:</p> \\[\\sum_{i=1}^{n} MCWI_i = \\sum_{i=1}^{n} P_i\\]","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#enhanced-mcwi-with-precipitation","level":3,"title":"Enhanced MCWI with Precipitation","text":"<p>For spatially variable precipitation:</p> <pre><code>def calculate_mcwi(twi, precipitation=None):\n    \"\"\"\n    Calculate Mass Conservative Wetness Index\n\n    Parameters:\n    - twi: Topographic wetness index\n    - precipitation: Spatial precipitation field (optional)\n\n    Returns:\n    - mcwi: Mass conservative wetness index\n    \"\"\"\n\n    if precipitation is None:\n        # Assume uniform precipitation\n        precipitation = np.ones_like(twi)\n\n    # Calculate means\n    mean_twi = np.nanmean(twi)\n    mean_precip = np.nanmean(precipitation)\n\n    # Normalize TWI to conserve mass\n    mcwi = twi * (mean_precip / mean_twi)\n\n    # Apply precipitation weighting\n    mcwi = mcwi * (precipitation / mean_precip)\n\n    # Verify mass conservation\n    total_input = np.nansum(precipitation)\n    total_output = np.nansum(mcwi)\n    conservation_error = abs(total_input - total_output) / total_input\n\n    if conservation_error &gt; 0.01:  # 1% tolerance\n        print(f\"Warning: Mass conservation error = {conservation_error:.2%}\")\n\n    return mcwi\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#lateral-redistribution","level":3,"title":"Lateral Redistribution","text":"<p>MCWI enables lateral water redistribution:</p> <pre><code>def redistribute_water_mcwi(precipitation, mcwi, convergence_factor=1.0):\n    \"\"\"\n    Redistribute water based on MCWI\n\n    Parameters:\n    - precipitation: Input precipitation field\n    - mcwi: Mass conservative wetness index\n    - convergence_factor: Strength of redistribution (0=none, 1=full)\n\n    Returns:\n    - redistributed: Water after lateral redistribution\n    \"\"\"\n\n    # Normalize MCWI to [0, 1]\n    mcwi_norm = (mcwi - np.nanmin(mcwi)) / (np.nanmax(mcwi) - np.nanmin(mcwi))\n\n    # Calculate redistribution weights\n    weights = convergence_factor * mcwi_norm + (1 - convergence_factor)\n\n    # Ensure mass conservation\n    weights = weights * (np.nansum(precipitation) / np.nansum(precipitation * weights))\n\n    # Apply redistribution\n    redistributed = precipitation * weights\n\n    return redistributed\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#flow-accumulation-algorithms","level":2,"title":"Flow Accumulation Algorithms","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#d8-eight-direction-flow","level":3,"title":"D8 (Eight-Direction) Flow","text":"<p>The simplest flow routing method, directing all flow to the steepest downslope neighbor:</p> <pre><code>def d8_flow_direction(dem):\n    \"\"\"\n    D8 flow direction algorithm\n\n    Returns flow direction codes:\n    32  64  128\n    16  X   1\n    8   4   2\n    \"\"\"\n\n    # Define neighbor offsets\n    neighbors = [\n        (-1, 1, 32), (0, 1, 64), (1, 1, 128),\n        (-1, 0, 16),              (1, 0, 1),\n        (-1, -1, 8), (0, -1, 4), (1, -1, 2)\n    ]\n\n    flow_dir = np.zeros_like(dem, dtype=np.int32)\n\n    for i in range(dem.shape[0]):\n        for j in range(dem.shape[1]):\n\n            max_drop = 0\n            direction = 0\n\n            for di, dj, code in neighbors:\n                ni, nj = i + di, j + dj\n\n                if 0 &lt;= ni &lt; dem.shape[0] and 0 &lt;= nj &lt; dem.shape[1]:\n                    # Calculate slope\n                    distance = np.sqrt(di**2 + dj**2) * cell_size\n                    drop = (dem[i, j] - dem[ni, nj]) / distance\n\n                    if drop &gt; max_drop:\n                        max_drop = drop\n                        direction = code\n\n            flow_dir[i, j] = direction\n\n    return flow_dir\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#d-infinity-continuous-flow-direction","level":3,"title":"D-Infinity (Continuous Flow Direction)","text":"<p>D-infinity allows flow dispersion across multiple neighbors:</p> <pre><code>def dinf_flow_direction(dem):\n    \"\"\"\n    D-infinity flow direction algorithm\n\n    Returns:\n    - flow_angle: Flow direction in radians (0-2œÄ)\n    - slope: Maximum downslope gradient\n    \"\"\"\n\n    # Calculate slopes to 8 neighbors\n    e0 = dem[1:-1, 2:] - dem[1:-1, 1:-1]    # E\n    e1 = dem[:-2, 2:] - dem[1:-1, 1:-1]     # NE\n    e2 = dem[:-2, 1:-1] - dem[1:-1, 1:-1]   # N\n    e3 = dem[:-2, :-2] - dem[1:-1, 1:-1]    # NW\n    e4 = dem[1:-1, :-2] - dem[1:-1, 1:-1]   # W\n    e5 = dem[2:, :-2] - dem[1:-1, 1:-1]     # SW\n    e6 = dem[2:, 1:-1] - dem[1:-1, 1:-1]    # S\n    e7 = dem[2:, 2:] - dem[1:-1, 1:-1]      # SE\n\n    # Calculate flow angle for each triangular facet\n    flow_angles = []\n    slopes = []\n\n    for k in range(8):\n        # Get adjacent edges\n        e1_k = eval(f'e{k}')\n        e2_k = eval(f'e{(k+1)%8}')\n\n        # Calculate flow direction within facet\n        angle, slope = calculate_facet_flow(e1_k, e2_k, k)\n        flow_angles.append(angle)\n        slopes.append(slope)\n\n    # Select steepest facet\n    max_slope_idx = np.argmax(slopes, axis=0)\n    flow_angle = np.choose(max_slope_idx, flow_angles)\n    max_slope = np.max(slopes, axis=0)\n\n    return flow_angle, max_slope\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#multiple-flow-direction-mfd","level":3,"title":"Multiple Flow Direction (MFD)","text":"<p>MFD distributes flow to all downslope neighbors:</p> <pre><code>def mfd_flow_distribution(dem, exponent=1.1):\n    \"\"\"\n    Multiple Flow Direction algorithm\n\n    Parameters:\n    - dem: Digital elevation model\n    - exponent: Flow partition exponent (higher = more convergent)\n\n    Returns:\n    - flow_fractions: Dict of flow fractions to each neighbor\n    \"\"\"\n\n    flow_fractions = {}\n\n    for i in range(dem.shape[0]):\n        for j in range(dem.shape[1]):\n\n            # Calculate slope to all neighbors\n            slopes = []\n            valid_neighbors = []\n\n            for di in [-1, 0, 1]:\n                for dj in [-1, 0, 1]:\n                    if di == 0 and dj == 0:\n                        continue\n\n                    ni, nj = i + di, j + dj\n\n                    if 0 &lt;= ni &lt; dem.shape[0] and 0 &lt;= nj &lt; dem.shape[1]:\n                        if dem[ni, nj] &lt; dem[i, j]:  # Downslope\n                            distance = np.sqrt(di**2 + dj**2) * cell_size\n                            slope = (dem[i, j] - dem[ni, nj]) / distance\n                            slopes.append(slope ** exponent)\n                            valid_neighbors.append((ni, nj))\n\n            # Normalize to sum to 1\n            if slopes:\n                total = sum(slopes)\n                fractions = [s / total for s in slopes]\n\n                flow_fractions[(i, j)] = dict(zip(valid_neighbors, fractions))\n            else:\n                flow_fractions[(i, j)] = {}  # Pit or flat\n\n    return flow_fractions\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#flow-accumulation-calculation","level":2,"title":"Flow Accumulation Calculation","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#recursive-algorithm","level":3,"title":"Recursive Algorithm","text":"<pre><code>def calculate_flow_accumulation(flow_direction, weights=None):\n    \"\"\"\n    Calculate flow accumulation from flow direction\n\n    Parameters:\n    - flow_direction: Flow direction grid (D8 codes or MFD fractions)\n    - weights: Optional weight grid (e.g., precipitation)\n\n    Returns:\n    - accumulation: Flow accumulation grid\n    \"\"\"\n\n    if weights is None:\n        weights = np.ones_like(flow_direction, dtype=np.float32)\n\n    accumulation = weights.copy()\n\n    # Find outlets (cells with no upstream neighbors)\n    outlets = find_outlets(flow_direction)\n\n    # Process from outlets upstream\n    processed = np.zeros_like(flow_direction, dtype=bool)\n\n    def accumulate_recursive(i, j):\n        if processed[i, j]:\n            return accumulation[i, j]\n\n        # Find upstream cells\n        upstream = find_upstream_cells(i, j, flow_direction)\n\n        # Accumulate from upstream\n        for ui, uj in upstream:\n            if not processed[ui, uj]:\n                accumulate_recursive(ui, uj)\n            accumulation[i, j] += accumulation[ui, uj]\n\n        processed[i, j] = True\n        return accumulation[i, j]\n\n    # Process all cells\n    for i in range(flow_direction.shape[0]):\n        for j in range(flow_direction.shape[1]):\n            accumulate_recursive(i, j)\n\n    return accumulation\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#slope-and-aspect-calculation","level":2,"title":"Slope and Aspect Calculation","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#slope-algorithms","level":3,"title":"Slope Algorithms","text":"<pre><code>def calculate_slope(dem, method='horn'):\n    \"\"\"\n    Calculate slope from DEM\n\n    Methods:\n    - 'horn': Horn (1981) 3rd-order finite difference\n    - 'zevenbergen': Zevenbergen &amp; Thorne (1987)\n    - 'average': Simple average method\n    \"\"\"\n\n    cell_size = get_cell_size(dem)\n\n    if method == 'horn':\n        # Horn's method (most accurate)\n        dz_dx = ((dem[:-2, 2:] + 2*dem[1:-1, 2:] + dem[2:, 2:]) -\n                 (dem[:-2, :-2] + 2*dem[1:-1, :-2] + dem[2:, :-2])) / (8 * cell_size)\n\n        dz_dy = ((dem[2:, :-2] + 2*dem[2:, 1:-1] + dem[2:, 2:]) -\n                 (dem[:-2, :-2] + 2*dem[:-2, 1:-1] + dem[:-2, 2:])) / (8 * cell_size)\n\n    elif method == 'zevenbergen':\n        # Zevenbergen &amp; Thorne method\n        dz_dx = (dem[1:-1, 2:] - dem[1:-1, :-2]) / (2 * cell_size)\n        dz_dy = (dem[2:, 1:-1] - dem[:-2, 1:-1]) / (2 * cell_size)\n\n    else:  # average\n        # Simple average\n        dz_dx = np.diff(dem, axis=1) / cell_size\n        dz_dy = np.diff(dem, axis=0) / cell_size\n\n    # Calculate slope magnitude\n    slope_radians = np.arctan(np.sqrt(dz_dx**2 + dz_dy**2))\n    slope_degrees = np.rad2deg(slope_radians)\n\n    return slope_degrees\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#aspect-calculation","level":3,"title":"Aspect Calculation","text":"<pre><code>def calculate_aspect(dem):\n    \"\"\"\n    Calculate aspect (flow direction azimuth)\n\n    Returns:\n    - aspect: Degrees clockwise from north (0-360)\n    \"\"\"\n\n    # Calculate gradients\n    dz_dx = np.gradient(dem, axis=1)\n    dz_dy = np.gradient(dem, axis=0)\n\n    # Calculate aspect (mathematical convention: CCW from East)\n    aspect_math = np.arctan2(dz_dy, -dz_dx)\n\n    # Convert to geographic convention (CW from North)\n    aspect_geo = np.rad2deg(aspect_math)\n    aspect_geo = 90 - aspect_geo\n\n    # Normalize to 0-360\n    aspect_geo[aspect_geo &lt; 0] += 360\n\n    # Flat areas have undefined aspect\n    flat_mask = (dz_dx == 0) &amp; (dz_dy == 0)\n    aspect_geo[flat_mask] = -1  # Undefined\n\n    return aspect_geo\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#curvature-analysis","level":2,"title":"Curvature Analysis","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#profile-and-plan-curvature","level":3,"title":"Profile and Plan Curvature","text":"<pre><code>def calculate_curvature(dem):\n    \"\"\"\n    Calculate terrain curvature\n\n    Returns:\n    - profile_curvature: Curvature in flow direction\n    - plan_curvature: Curvature perpendicular to flow\n    - mean_curvature: Average curvature\n    \"\"\"\n\n    cell_size = get_cell_size(dem)\n\n    # Second derivatives\n    d2z_dx2 = np.diff(dem, n=2, axis=1) / (cell_size**2)\n    d2z_dy2 = np.diff(dem, n=2, axis=0) / (cell_size**2)\n\n    # Cross derivative (using central differences)\n    dz_dx = np.gradient(dem, cell_size, axis=1)\n    dz_dy = np.gradient(dem, cell_size, axis=0)\n    d2z_dxdy = np.gradient(dz_dx, cell_size, axis=0)\n\n    # First derivatives squared\n    p = dz_dx**2\n    q = dz_dy**2\n\n    # Profile curvature (curvature in direction of maximum slope)\n    profile_curv = -(p * d2z_dx2 + 2 * dz_dx * dz_dy * d2z_dxdy + q * d2z_dy2) / \\\n                   ((p + q) * np.sqrt(1 + p + q)**3)\n\n    # Plan curvature (curvature perpendicular to slope direction)\n    plan_curv = -(q * d2z_dx2 - 2 * dz_dx * dz_dy * d2z_dxdy + p * d2z_dy2) / \\\n                ((p + q)**(3/2))\n\n    # Mean curvature\n    mean_curv = -(d2z_dx2 + d2z_dy2) / 2\n\n    return profile_curv, plan_curv, mean_curv\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#curvature-classification","level":3,"title":"Curvature Classification","text":"<pre><code>def classify_landforms(profile_curv, plan_curv):\n    \"\"\"\n    Classify landforms based on curvature\n\n    Returns landform classes:\n    1. Peak/Ridge (convex-convex)\n    2. Ridge (convex-linear)\n    3. Shoulder (convex-concave)\n    4. Planar (linear-linear)\n    5. Pass (linear-concave)\n    6. Channel (concave-concave)\n    7. Footslope (concave-linear)\n    8. Hollow (concave-convex)\n    9. Valley/Pit (linear-convex)\n    \"\"\"\n\n    # Define thresholds\n    threshold = 0.1  # Curvature threshold for classification\n\n    # Initialize landform grid\n    landforms = np.zeros_like(profile_curv, dtype=np.int8)\n\n    # Classify based on curvature combinations\n    landforms[(profile_curv &gt; threshold) &amp; (plan_curv &gt; threshold)] = 1  # Peak\n    landforms[(profile_curv &gt; threshold) &amp; (np.abs(plan_curv) &lt;= threshold)] = 2  # Ridge\n    landforms[(profile_curv &gt; threshold) &amp; (plan_curv &lt; -threshold)] = 3  # Shoulder\n\n    landforms[(np.abs(profile_curv) &lt;= threshold) &amp; (np.abs(plan_curv) &lt;= threshold)] = 4  # Planar\n    landforms[(np.abs(profile_curv) &lt;= threshold) &amp; (plan_curv &lt; -threshold)] = 5  # Pass\n\n    landforms[(profile_curv &lt; -threshold) &amp; (plan_curv &lt; -threshold)] = 6  # Channel\n    landforms[(profile_curv &lt; -threshold) &amp; (np.abs(plan_curv) &lt;= threshold)] = 7  # Footslope\n    landforms[(profile_curv &lt; -threshold) &amp; (plan_curv &gt; threshold)] = 8  # Hollow\n\n    landforms[(np.abs(profile_curv) &lt;= threshold) &amp; (plan_curv &gt; threshold)] = 9  # Valley\n\n    return landforms\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#topographic-position-index","level":2,"title":"Topographic Position Index","text":"<pre><code>def calculate_tpi(dem, outer_radius, inner_radius=0):\n    \"\"\"\n    Calculate Topographic Position Index\n\n    TPI = elevation - mean(neighborhood elevation)\n\n    Parameters:\n    - dem: Digital elevation model\n    - outer_radius: Outer radius of annulus (cells)\n    - inner_radius: Inner radius of annulus (cells)\n\n    Returns:\n    - tpi: Topographic position index\n    \"\"\"\n\n    from scipy.ndimage import generic_filter\n\n    # Create annulus kernel\n    y, x = np.ogrid[-outer_radius:outer_radius+1, -outer_radius:outer_radius+1]\n    mask = (x**2 + y**2 &lt;= outer_radius**2) &amp; (x**2 + y**2 &gt; inner_radius**2)\n\n    # Calculate neighborhood mean\n    def mean_filter(values):\n        return np.mean(values[mask.flatten()])\n\n    neighborhood_mean = generic_filter(dem, mean_filter, size=2*outer_radius+1)\n\n    # Calculate TPI\n    tpi = dem - neighborhood_mean\n\n    return tpi\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#integration-with-eemt","level":2,"title":"Integration with EEMT","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#topographic-controls-on-energy","level":3,"title":"Topographic Controls on Energy","text":"<pre><code>def apply_topographic_controls(solar_radiation, temperature, precipitation, twi):\n    \"\"\"\n    Apply topographic modifications to climate variables\n\n    Parameters:\n    - solar_radiation: Base solar radiation\n    - temperature: Base temperature\n    - precipitation: Base precipitation  \n    - twi: Topographic wetness index\n\n    Returns:\n    - Modified climate variables\n    \"\"\"\n\n    # Solar radiation already includes topographic effects\n    solar_modified = solar_radiation\n\n    # Temperature modification based on cold air pooling\n    # Cold air accumulates in high TWI areas\n    cold_pool_effect = np.where(twi &gt; 10, -2.0, 0.0)  # ¬∞C cooling\n    temp_modified = temperature + cold_pool_effect\n\n    # Precipitation enhancement in convergent zones\n    # Use MCWI for mass-conservative redistribution\n    mcwi = calculate_mcwi(twi, precipitation)\n    precip_modified = redistribute_water_mcwi(precipitation, mcwi)\n\n    return {\n        'solar': solar_modified,\n        'temperature': temp_modified,\n        'precipitation': precip_modified\n    }\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#quality-assurance","level":2,"title":"Quality Assurance","text":"","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#validation-checks","level":3,"title":"Validation Checks","text":"<pre><code>def validate_topographic_indices(dem, twi, flow_acc):\n    \"\"\"\n    Quality control for topographic calculations\n    \"\"\"\n\n    checks = {}\n\n    # Check TWI range\n    twi_range = (np.nanmin(twi), np.nanmax(twi))\n    checks['twi_range_valid'] = (0 &lt;= twi_range[0]) and (twi_range[1] &lt;= 20)\n\n    # Check flow accumulation consistency\n    total_cells = dem.size\n    max_accumulation = np.nanmax(flow_acc)\n    checks['flow_acc_valid'] = max_accumulation &lt;= total_cells\n\n    # Check for sinks/pits\n    sinks = identify_sinks(dem)\n    checks['sink_percentage'] = (np.sum(sinks) / dem.size) * 100\n\n    # Check slope calculation\n    slopes = calculate_slope(dem)\n    checks['max_slope_valid'] = np.nanmax(slopes) &lt;= 90\n\n    return checks\n</code></pre>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"algorithms/topographic-analysis/#references","level":2,"title":"References","text":"<ul> <li> <p>Beven, K. J., &amp; Kirkby, M. J. (1979). A physically based, variable contributing area model of basin hydrology. Hydrological Sciences Bulletin, 24(1), 43-69.</p> </li> <li> <p>Tarboton, D. G. (1997). A new method for the determination of flow directions and upslope areas in grid digital elevation models. Water Resources Research, 33(2), 309-319.</p> </li> <li> <p>Quinn, P., et al. (1991). The prediction of hillslope flow paths for distributed hydrological modelling using digital terrain models. Hydrological Processes, 5(1), 59-79.</p> </li> </ul> <p>Next: EEMT Calculations ‚Üí</p>","path":["Algorithms","Topographic Analysis Algorithms"],"tags":[]},{"location":"api/","level":1,"title":"API Documentation","text":"","path":["API Documentation"],"tags":[]},{"location":"api/#overview","level":2,"title":"Overview","text":"<p>The EEMT suite provides multiple interfaces for workflow execution and data processing. This documentation covers the REST API, web interface, command-line tools, and Python modules.</p>","path":["API Documentation"],"tags":[]},{"location":"api/#api-interfaces","level":2,"title":"API Interfaces","text":"","path":["API Documentation"],"tags":[]},{"location":"api/#rest-api-endpoints","level":3,"title":"üåê REST API Endpoints","text":"<p>Complete reference for the FastAPI-based REST interface: - Job submission and management - Status monitoring - Results retrieval - Health checks and metrics</p>","path":["API Documentation"],"tags":[]},{"location":"api/#web-interface","level":3,"title":"üìä Web Interface","text":"<p>Browser-based interface for workflow management: - Interactive job submission forms - Real-time progress monitoring - Result visualization and downloads - Multi-user support</p>","path":["API Documentation"],"tags":[]},{"location":"api/#workflow-parameters","level":3,"title":"üîß Workflow Parameters","text":"<p>Comprehensive parameter documentation: - Solar radiation parameters - EEMT calculation options - Climate data settings - Performance tuning</p>","path":["API Documentation"],"tags":[]},{"location":"api/#command-line-interface","level":3,"title":"üíª Command Line Interface","text":"<p>Direct workflow execution via command line: - Python workflow scripts - Shell script utilities - Batch processing options</p>","path":["API Documentation"],"tags":[]},{"location":"api/#python-modules","level":3,"title":"üêç Python Modules","text":"<p>Programmatic access to EEMT functions: - Core calculation functions - Data I/O utilities - Visualization tools - Custom workflow development</p>","path":["API Documentation"],"tags":[]},{"location":"api/#quick-start-examples","level":2,"title":"Quick Start Examples","text":"","path":["API Documentation"],"tags":[]},{"location":"api/#rest-api","level":3,"title":"REST API","text":"<pre><code># Submit a solar radiation job\ncurl -X POST \"http://localhost:5000/api/jobs\" \\\n  -F \"workflow_type=sol\" \\\n  -F \"dem_file=@your_dem.tif\" \\\n  -F \"parameters={\\\"step\\\": 15, \\\"num_threads\\\": 4}\"\n\n# Check job status\ncurl \"http://localhost:5000/api/jobs/JOB_ID/status\"\n\n# Download results\ncurl -O \"http://localhost:5000/api/jobs/JOB_ID/download\"\n</code></pre>","path":["API Documentation"],"tags":[]},{"location":"api/#python-sdk","level":3,"title":"Python SDK","text":"<pre><code>from eemt import EEMTClient\n\n# Initialize client\nclient = EEMTClient(base_url=\"http://localhost:5000\")\n\n# Submit job\njob_id = client.submit_job(\n    workflow_type=\"eemt\",\n    dem_file=\"path/to/dem.tif\",\n    parameters={\n        \"start_year\": 2020,\n        \"end_year\": 2022,\n        \"step\": 15\n    }\n)\n\n# Monitor progress\nstatus = client.get_job_status(job_id)\nprint(f\"Progress: {status.progress}%\")\n\n# Download results\nclient.download_results(job_id, output_dir=\"./results\")\n</code></pre>","path":["API Documentation"],"tags":[]},{"location":"api/#command-line","level":3,"title":"Command Line","text":"<pre><code># Run solar radiation workflow\npython /opt/eemt/sol/run-workflow \\\n  --dem input_dem.tif \\\n  --output ./output \\\n  --step 15 \\\n  --num-threads 8\n\n# Run full EEMT analysis\npython /opt/eemt/eemt/run-workflow \\\n  --dem input_dem.tif \\\n  --output ./output \\\n  --start-year 2020 \\\n  --end-year 2022 \\\n  --step 15\n</code></pre>","path":["API Documentation"],"tags":[]},{"location":"api/#api-features","level":2,"title":"API Features","text":"","path":["API Documentation"],"tags":[]},{"location":"api/#authentication-security","level":3,"title":"Authentication &amp; Security","text":"<ul> <li>API key authentication (optional)</li> <li>Rate limiting and quotas</li> <li>HTTPS support</li> <li>CORS configuration</li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#job-management","level":3,"title":"Job Management","text":"<ul> <li>Asynchronous job submission</li> <li>Queue management</li> <li>Priority scheduling</li> <li>Resource allocation</li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#data-handling","level":3,"title":"Data Handling","text":"<ul> <li>Large file uploads (chunked)</li> <li>Streaming downloads</li> <li>Compression support</li> <li>Format conversion</li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#monitoring-metrics","level":3,"title":"Monitoring &amp; Metrics","text":"<ul> <li>Real-time progress tracking</li> <li>Resource usage statistics</li> <li>Error logging and debugging</li> <li>Performance metrics</li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#response-formats","level":2,"title":"Response Formats","text":"","path":["API Documentation"],"tags":[]},{"location":"api/#standard-response-structure","level":3,"title":"Standard Response Structure","text":"<pre><code>{\n  \"status\": \"success\",\n  \"data\": {\n    // Response data\n  },\n  \"message\": \"Operation completed successfully\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre>","path":["API Documentation"],"tags":[]},{"location":"api/#error-response","level":3,"title":"Error Response","text":"<pre><code>{\n  \"status\": \"error\",\n  \"error\": {\n    \"code\": \"INVALID_PARAMETER\",\n    \"message\": \"Step value must be between 1 and 30\",\n    \"details\": {\n      \"parameter\": \"step\",\n      \"provided_value\": 45,\n      \"allowed_range\": [1, 30]\n    }\n  },\n  \"timestamp\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre>","path":["API Documentation"],"tags":[]},{"location":"api/#job-status-response","level":3,"title":"Job Status Response","text":"<pre><code>{\n  \"job_id\": \"20240115-103000-abc123\",\n  \"status\": \"running\",\n  \"progress\": 45,\n  \"current_task\": \"Processing day 165 of 365\",\n  \"started_at\": \"2024-01-15T10:30:00Z\",\n  \"estimated_completion\": \"2024-01-15T11:30:00Z\",\n  \"resources\": {\n    \"cpu_usage\": 85,\n    \"memory_usage\": 4.2,\n    \"disk_usage\": 12.5\n  }\n}\n</code></pre>","path":["API Documentation"],"tags":[]},{"location":"api/#rate-limits","level":2,"title":"Rate Limits","text":"Endpoint Rate Limit Burst Job submission 10/hour 2 Status checks 60/minute 10 Result downloads 20/hour 5 Health checks Unlimited -","path":["API Documentation"],"tags":[]},{"location":"api/#versioning","level":2,"title":"Versioning","text":"<p>The API follows semantic versioning:</p> <ul> <li>Current Version: v1.0</li> <li>Base URL: <code>/api/v1/</code></li> <li>Deprecation Policy: 6 months notice</li> <li>Backwards Compatibility: Maintained within major versions</li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#api-clients","level":2,"title":"API Clients","text":"<p>Official client libraries:</p> <ul> <li>Python: <code>pip install eemt-client</code></li> <li>JavaScript: <code>npm install @eemt/client</code></li> <li>R: <code>install.packages(\"eemtr\")</code></li> </ul> <p>Community clients:</p> <ul> <li>Julia Client</li> <li>MATLAB Interface</li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#openapi-specification","level":2,"title":"OpenAPI Specification","text":"<p>The complete OpenAPI 3.0 specification is available at:</p> <ul> <li>JSON: <code>/api/openapi.json</code></li> <li>YAML: <code>/api/openapi.yaml</code></li> <li>Interactive Docs: <code>/api/docs</code> (Swagger UI)</li> <li>ReDoc: <code>/api/redoc</code></li> </ul>","path":["API Documentation"],"tags":[]},{"location":"api/#support","level":2,"title":"Support","text":"<p>For API support:</p> <ol> <li>Check the FAQ</li> <li>Review Examples</li> <li>Post on Discussions</li> <li>Report Issues</li> </ol> <p>For installation instructions, see Installation Guide. For workflow details, see Workflows Documentation.</p>","path":["API Documentation"],"tags":[]},{"location":"api/cli/","level":1,"title":"EEMT API Reference","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#overview","level":2,"title":"Overview","text":"<p>This reference provides detailed documentation for all EEMT calculation functions, GRASS GIS commands, and configuration parameters.</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#core-eemt-functions","level":2,"title":"Core EEMT Functions","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#traditional-eemt-calculation","level":3,"title":"Traditional EEMT Calculation","text":"<pre><code>def calculate_eemt_traditional(temperature, precipitation, elevation=None):\n    \"\"\"\n    Calculate EEMT using traditional climate-based approach\n\n    Parameters:\n    -----------\n    temperature : array_like\n        Monthly mean temperature [¬∞C]\n    precipitation : array_like  \n        Monthly precipitation [mm]\n    elevation : array_like, optional\n        Elevation for lapse rate corrections [m]\n\n    Returns:\n    --------\n    eemt : array_like\n        Effective Energy and Mass Transfer [MJ m‚Åª¬≤ yr‚Åª¬π]\n    e_bio : array_like\n        Biological energy component [MJ m‚Åª¬≤ yr‚Åª¬π]\n    e_ppt : array_like\n        Precipitation energy component [MJ m‚Åª¬≤ yr‚Åª¬π]\n\n    Notes:\n    ------\n    Based on Rasmussen et al. (2005, 2014) methodology.\n    Uses Lieth (1975) NPP equation and Hamon PET estimation.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; temp = np.array([5, 10, 15, 20, 18, 12, 8])  # Monthly temps\n    &gt;&gt;&gt; precip = np.array([50, 60, 80, 40, 30, 45, 55])  # Monthly precip\n    &gt;&gt;&gt; eemt, e_bio, e_ppt = calculate_eemt_traditional(temp, precip)\n    &gt;&gt;&gt; print(f\"Annual EEMT: {eemt:.1f} MJ/m¬≤/yr\")\n    \"\"\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#topographic-eemt-calculation","level":3,"title":"Topographic EEMT Calculation","text":"<pre><code>def calculate_eemt_topographic(dem_file, climate_data, solar_data, \n                             output_dir, mcwi_method='d_infinity'):\n    \"\"\"\n    Calculate EEMT with topographic controls on energy and water balance\n\n    Parameters:\n    -----------\n    dem_file : str or Path\n        Path to digital elevation model (GeoTIFF)\n    climate_data : dict\n        Dictionary containing climate arrays:\n        - 'temperature': Monthly temperature [¬∞C] \n        - 'precipitation': Monthly precipitation [mm]\n        - 'humidity': Relative humidity [%]\n        - 'wind_speed': Wind speed [m/s]\n    solar_data : dict\n        Solar radiation data from r.sun calculations:\n        - 'global_radiation': Monthly solar [Wh/m¬≤]\n        - 'diffuse_radiation': Diffuse component [Wh/m¬≤]\n        - 'direct_radiation': Direct beam component [Wh/m¬≤]\n    output_dir : str or Path\n        Output directory for intermediate files\n    mcwi_method : str, default 'd_infinity'\n        Flow routing method: 'd_infinity', 'mfd', 'sfd'\n\n    Returns:\n    --------\n    eemt_result : dict\n        Results dictionary containing:\n        - 'eemt': Total EEMT [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'e_bio': Biological component [MJ m‚Åª¬≤ yr‚Åª¬π] \n        - 'e_ppt': Precipitation component [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'mcwi': Mass Conservative Wetness Index\n        - 'solar_ratio': Topographic solar modification factor\n\n    Notes:\n    ------\n    Implements Rasmussen et al. (2014) EEMT_TOPO methodology.\n    Requires GRASS GIS for terrain analysis and solar calculations.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; climate = load_daymet_data('study_area.shp', 2015, 2020)\n    &gt;&gt;&gt; solar = calculate_annual_solar('dem.tif', threads=8)\n    &gt;&gt;&gt; result = calculate_eemt_topographic('dem.tif', climate, solar, 'output/')\n    &gt;&gt;&gt; print(f\"Mean topographic EEMT: {np.mean(result['eemt']):.1f} MJ/m¬≤/yr\")\n    \"\"\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#vegetation-eemt-calculation","level":3,"title":"Vegetation EEMT Calculation","text":"<pre><code>def calculate_eemt_vegetation(dem_file, climate_data, vegetation_data,\n                           output_dir, lai_method='ndvi', resistance_model='kelliher'):\n    \"\"\"\n    Calculate EEMT with full vegetation and topographic integration\n\n    Parameters:\n    -----------\n    dem_file : str or Path\n        Digital elevation model file path\n    climate_data : dict\n        Complete climate dataset with:\n        - 'temperature': Temperature arrays [¬∞C]\n        - 'precipitation': Precipitation arrays [mm] \n        - 'humidity': Relative humidity [%]\n        - 'wind_speed': Wind speed [m/s]\n        - 'net_radiation': Net radiation [W/m¬≤]\n    vegetation_data : dict\n        Vegetation structure data:\n        - 'lai': Leaf Area Index [-] \n        - 'canopy_height': Canopy height [m]\n        - 'ndvi': Normalized Difference Vegetation Index [-]\n        - 'biomass': Aboveground biomass [Mg/ha] (optional)\n    output_dir : str or Path\n        Output directory\n    lai_method : str, default 'ndvi'\n        LAI calculation method: 'ndvi', 'modis', 'direct'\n    resistance_model : str, default 'kelliher' \n        Surface resistance model: 'kelliher', 'jarvis', 'stewart'\n\n    Returns:\n    --------\n    eemt_result : dict\n        Complete EEMT results:\n        - 'eemt': Total EEMT [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'e_bio': Biological energy [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'e_ppt': Precipitation energy [MJ m‚Åª¬≤ yr‚Åª¬π] \n        - 'aet': Actual evapotranspiration [mm/yr]\n        - 'npp': Net primary production [kg/m¬≤/yr]\n        - 'surface_resistance': Surface resistance [s/m]\n        - 'lai_effective': Effective LAI used in calculations\n\n    Notes:\n    ------\n    Implements Rasmussen et al. (2014) EEMT_TOPO-VEG methodology.\n    Uses Penman-Monteith equation with vegetation-specific parameters.\n    Accounts for canopy structure effects on energy and water balance.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; # Load vegetation data from satellite\n    &gt;&gt;&gt; vegetation = {\n    ...     'ndvi': load_landsat_ndvi('study_area.shp', 2020),\n    ...     'canopy_height': load_lidar_canopy('lidar_data.las')\n    ... }\n    &gt;&gt;&gt; result = calculate_eemt_vegetation('dem.tif', climate, vegetation, 'output/')\n    &gt;&gt;&gt; print(f\"Vegetation EEMT: {np.mean(result['eemt']):.1f} MJ/m¬≤/yr\")\n    \"\"\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#grass-gis-command-reference","level":2,"title":"GRASS GIS Command Reference","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#solar-radiation-rsun-family","level":3,"title":"Solar Radiation (r.sun family)","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#rsun-multi-processor-version","level":4,"title":"r.sun (Multi-processor version)","text":"<pre><code>r.sun elevation=dem aspect=aspect slope=slope \\\\\n         day=180 step=0.25 \\\\\n         linke_value=3.0 albedo_value=0.2 \\\\\n         threads=8 \\\\\n         glob_rad=global_radiation \\\\\n         insol_time=sunshine_hours \\\\\n         [beam_rad=beam_radiation] \\\\\n         [diff_rad=diffuse_radiation] \\\\\n         [refl_rad=reflected_radiation]\n</code></pre> <p>Parameters: - <code>elevation=name</code> - Input elevation raster - <code>aspect=name</code> - Aspect in degrees (0-360¬∞) - <code>slope=name</code> - Slope in degrees (0-90¬∞) - <code>day=integer</code> - Day of year (1-365) - <code>step=float</code> - Time step in hours (0.25-1.0) - <code>linke_value=float</code> - Linke atmospheric turbidity (1.0-8.0) - <code>albedo_value=float</code> - Ground albedo (0.0-1.0) - <code>threads=integer</code> - Number of OpenMP threads - <code>glob_rad=name</code> - Output global radiation [Wh/m¬≤] - <code>insol_time=name</code> - Output sunshine duration [hours]</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#rsun-single-processor-version","level":4,"title":"r.sun (Single-processor version)","text":"<pre><code>r.sun elevation=dem \\\\\n      [aspect=aspect] [slope=slope] \\\\\n      [lat=latitude] [lon=longitude] \\\\\n      [day=day_of_year] [time=decimal_time] \\\\\n      [step=time_step] \\\\\n      [glob_rad=output] [beam_rad=output] \\\\\n      [diff_rad=output] [refl_rad=output] \\\\\n      [insol_time=output]\n</code></pre> <p>Advanced Options: - <code>horizon_basename=basename</code> - Horizon angle rasters - <code>horizon_step=angle</code> - Horizon calculation step [degrees] - <code>civil_time=hour</code> - Local solar time - <code>solar_constant=value</code> - Solar constant [W/m¬≤] - <code>distance_step=value</code> - Sampling distance [m]</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#terrain-analysis","level":3,"title":"Terrain Analysis","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#rslopeaspect","level":4,"title":"r.slope.aspect","text":"<pre><code>r.slope.aspect elevation=dem \\\\\n               slope=slope_output \\\\\n               aspect=aspect_output \\\\\n               [format=degrees|percent] \\\\\n               [precision=FCELL|DCELL] \\\\\n               [zscale=factor] \\\\\n               [min_slope=degrees]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#rterraflow-flow-accumulation","level":4,"title":"r.terraflow (Flow accumulation)","text":"<pre><code>r.terraflow elevation=dem \\\\\n            filled=filled_dem \\\\\n            direction=flow_direction \\\\\n            swatershed=watersheds \\\\\n            accumulation=flow_accumulation \\\\\n            tci=topographic_convergence_index\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#rwatershed-alternative-flow-routing","level":4,"title":"r.watershed (Alternative flow routing)","text":"<pre><code>r.watershed elevation=dem \\\\\n            accumulation=flow_accum \\\\\n            drainage=flow_direction \\\\\n            basin=watersheds \\\\\n            stream=stream_network \\\\\n            [threshold=threshold_value] \\\\\n            [-s] [-4] [-a]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#data-importexport","level":3,"title":"Data Import/Export","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#ringdal-import-raster-data","level":4,"title":"r.in.gdal (Import raster data)","text":"<pre><code>r.in.gdal input=input_file.tif \\\\\n          output=grass_raster \\\\\n          [band=band_number] \\\\\n          [memory=memory_mb] \\\\\n          [target=target_crs] \\\\\n          [-o] [-e] [-f]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#routgdal-export-raster-data","level":4,"title":"r.out.gdal (Export raster data)","text":"<pre><code>r.out.gdal input=grass_raster \\\\\n           output=output_file.tif \\\\\n           format=GTiff \\\\\n           [type=data_type] \\\\\n           [nodata=nodata_value] \\\\\n           createopt=\"COMPRESS=LZW,TILED=YES\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#configuration-parameters","level":2,"title":"Configuration Parameters","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#solar-radiation-parameters","level":3,"title":"Solar Radiation Parameters","text":"Parameter Range Default Description <code>day</code> 1-365 - Day of year for calculation <code>step</code> 0.1-2.0 0.25 Time step interval [hours] <code>linke_value</code> 1.0-8.0 3.0 Atmospheric turbidity factor <code>albedo_value</code> 0.0-1.0 0.2 Surface albedo coefficient <code>lat</code> -90 to 90 auto Latitude [decimal degrees] <code>solar_constant</code> 1300-1400 1367 Solar constant [W/m¬≤]","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#processing-parameters","level":3,"title":"Processing Parameters","text":"Parameter Range Default Description <code>threads</code> 1-64 auto OpenMP thread count <code>memory</code> 256-8192 2048 Memory cache [MB] <code>precision</code> FCELL/DCELL FCELL Output precision <code>compress</code> LZW/DEFLATE LZW Output compression","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#climate-thresholds","level":3,"title":"Climate Thresholds","text":"Parameter Value Units Description <code>T_ref</code> 273.15 K Reference temperature (freezing) <code>h_BIO</code> 22√ó10‚Å∂ J/kg Specific biomass enthalpy <code>c_w</code> 4.18√ó10¬≥ J/kg/K Specific heat of water <code>EEMT_threshold</code> 70 MJ/m¬≤/yr Carbon/water dominance transition","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#error-handling","level":2,"title":"Error Handling","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#common-error-codes","level":3,"title":"Common Error Codes","text":"Error Cause Solution <code>GRASS: Location not found</code> Invalid GRASS database Check <code>GISDBASE</code> path <code>GDAL: Cannot open file</code> Missing input data Verify file paths <code>r.sun: Memory allocation failed</code> Insufficient RAM Reduce region size or tile processing <code>r.sun: Invalid day parameter</code> Day outside 1-365 range Check day parameter <code>Projection mismatch</code> CRS inconsistency Reproject data to common CRS","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#error-recovery-strategies","level":3,"title":"Error Recovery Strategies","text":"<pre><code>def robust_eemt_calculation(dem_file, climate_dir, output_dir, max_retries=3):\n    \"\"\"\n    EEMT calculation with error recovery\n\n    Implements automatic retry, fallback methods, and error logging\n    \"\"\"\n\n    import logging\n    from time import sleep\n\n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(f'{output_dir}/eemt_calculation.log'),\n            logging.StreamHandler()\n        ]\n    )\n\n    for attempt in range(max_retries):\n        try:\n            # Attempt EEMT calculation\n            result = calculate_eemt_complete(dem_file, climate_dir, output_dir)\n            logging.info(f\"EEMT calculation successful on attempt {attempt + 1}\")\n            return result\n\n        except MemoryError as e:\n            logging.warning(f\"Memory error on attempt {attempt + 1}: {e}\")\n            if attempt &lt; max_retries - 1:\n                # Try with reduced resolution\n                logging.info(\"Retrying with reduced spatial resolution...\")\n                dem_file = reduce_resolution(dem_file, factor=2)\n            else:\n                raise\n\n        except FileNotFoundError as e:\n            logging.error(f\"Missing input file: {e}\")\n            raise\n\n        except subprocess.CalledProcessError as e:\n            logging.warning(f\"GRASS command failed on attempt {attempt + 1}: {e}\")\n            if attempt &lt; max_retries - 1:\n                sleep(5)  # Wait before retry\n                logging.info(\"Retrying GRASS operation...\")\n            else:\n                raise\n\n        except Exception as e:\n            logging.error(f\"Unexpected error: {e}\")\n            if attempt &lt; max_retries - 1:\n                sleep(10)\n                logging.info(\"Retrying with fresh environment...\")\n            else:\n                raise\n\n    raise RuntimeError(f\"EEMT calculation failed after {max_retries} attempts\")\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#memory-management","level":3,"title":"Memory Management","text":"<pre><code>def optimize_grass_memory(max_memory_gb=16):\n    \"\"\"\n    Optimize GRASS GIS memory settings for large datasets\n\n    Parameters:\n    -----------\n    max_memory_gb : int\n        Maximum memory to allocate [GB]\n    \"\"\"\n\n    import os\n\n    # Set GRASS memory environment\n    cache_size = max_memory_gb * 1024  # Convert to MB\n\n    os.environ.update({\n        'GRASS_CACHE_SIZE': str(cache_size),\n        'GRASS_RASTER_TMPDIR_MAPSET': '/tmp',\n        'GRASS_VECTOR_TMPDIR_MAPSET': '/tmp',\n        'GRASS_COMPRESS_NULLS': '1',\n        'GRASS_RENDER_IMMEDIATE': 'FALSE'\n    })\n\n    print(f\"‚úì GRASS memory optimized for {max_memory_gb} GB\")\n\ndef calculate_optimal_tile_size(dem_file, available_memory_gb=8):\n    \"\"\"\n    Calculate optimal tile size for memory-constrained processing\n\n    Parameters:\n    -----------\n    dem_file : str\n        Path to DEM file\n    available_memory_gb : int  \n        Available system memory [GB]\n\n    Returns:\n    --------\n    tile_size : int\n        Optimal tile size in pixels\n    overlap : int\n        Recommended overlap in pixels\n    \"\"\"\n\n    import rasterio\n\n    with rasterio.open(dem_file) as src:\n        width, height = src.width, src.height\n        dtype_size = np.dtype(src.dtypes[0]).itemsize\n\n    # Estimate memory usage per pixel (including intermediate arrays)\n    memory_per_pixel = dtype_size * 20  # Factor for r.sun calculations\n\n    # Calculate tile size that fits in available memory\n    available_memory_bytes = available_memory_gb * 1024**3\n    max_pixels = available_memory_bytes // memory_per_pixel\n    tile_size = int(np.sqrt(max_pixels))\n\n    # Ensure reasonable tile size\n    tile_size = max(256, min(tile_size, 4096))\n    overlap = max(32, tile_size // 16)  # 6.25% overlap\n\n    return tile_size, overlap\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#parallel-processing-configuration","level":3,"title":"Parallel Processing Configuration","text":"<pre><code>def configure_parallel_processing(max_workers=None, threads_per_worker=4):\n    \"\"\"\n    Configure optimal parallel processing parameters\n\n    Parameters:\n    -----------\n    max_workers : int, optional\n        Maximum number of worker processes (default: CPU count // 4)\n    threads_per_worker : int\n        OpenMP threads per worker process\n\n    Returns:\n    --------\n    config : dict\n        Optimized processing configuration\n    \"\"\"\n\n    import multiprocessing as mp\n    import psutil\n\n    # Detect system capabilities\n    cpu_count = mp.cpu_count()\n    memory_gb = psutil.virtual_memory().total // (1024**3)\n\n    # Calculate optimal configuration\n    if max_workers is None:\n        max_workers = max(1, cpu_count // threads_per_worker)\n\n    # Memory per worker (reserve 2 GB for system)\n    memory_per_worker = max(2, (memory_gb - 2) // max_workers)\n\n    config = {\n        'max_workers': max_workers,\n        'threads_per_worker': threads_per_worker,\n        'memory_per_worker_gb': memory_per_worker,\n        'total_threads': max_workers * threads_per_worker,\n        'memory_efficiency': memory_per_worker / (memory_gb / max_workers)\n    }\n\n    print(f\"Parallel Processing Configuration:\")\n    print(f\"  Workers: {config['max_workers']}\")\n    print(f\"  Threads per worker: {config['threads_per_worker']}\")\n    print(f\"  Total threads: {config['total_threads']}\")\n    print(f\"  Memory per worker: {config['memory_per_worker_gb']} GB\")\n\n    return config\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#validation-functions","level":2,"title":"Validation Functions","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#statistical-validation","level":3,"title":"Statistical Validation","text":"<pre><code>def validate_eemt_results(eemt_results, validation_data, method='pearson'):\n    \"\"\"\n    Validate EEMT results against field measurements\n\n    Parameters:\n    -----------\n    eemt_results : dict\n        EEMT calculation results\n    validation_data : dict\n        Validation datasets:\n        - 'soil_depth': Measured soil depths [cm]\n        - 'biomass': Measured biomass [Mg/ha]\n        - 'npp': Measured NPP [kg/m¬≤/yr] \n        - 'coordinates': Sample locations\n    method : str\n        Validation method: 'pearson', 'spearman', 'rmse'\n\n    Returns:\n    --------\n    validation_results : dict\n        Validation statistics and plots\n    \"\"\"\n\n    from scipy import stats\n    import matplotlib.pyplot as plt\n\n    validation_results = {}\n\n    # Extract EEMT values at validation points\n    for data_type, data in validation_data.items():\n        if data_type == 'coordinates':\n            continue\n\n        # Extract EEMT values at measurement locations\n        eemt_at_points = extract_values_at_points(\n            eemt_results['eemt'], \n            validation_data['coordinates']\n        )\n\n        # Calculate validation statistics\n        if method == 'pearson':\n            r, p = stats.pearsonr(eemt_at_points, data)\n            validation_results[data_type] = {\n                'correlation': r,\n                'p_value': p,\n                'r_squared': r**2\n            }\n        elif method == 'rmse':\n            rmse = np.sqrt(np.mean((eemt_at_points - data)**2))\n            mae = np.mean(np.abs(eemt_at_points - data))\n            validation_results[data_type] = {\n                'rmse': rmse,\n                'mae': mae,\n                'bias': np.mean(eemt_at_points - data)\n            }\n\n    return validation_results\n\ndef cross_validate_eemt_methods(dem_file, climate_data, validation_points):\n    \"\"\"\n    Cross-validation of different EEMT calculation methods\n\n    Compares Traditional, Topographic, and Vegetation approaches\n    against field validation data\n    \"\"\"\n\n    methods = ['traditional', 'topographic', 'vegetation']\n    results = {}\n\n    for method in methods:\n        print(f\"Cross-validating {method} EEMT...\")\n\n        # Calculate EEMT using specific method\n        if method == 'traditional':\n            eemt = calculate_eemt_traditional(climate_data)\n        elif method == 'topographic': \n            eemt = calculate_eemt_topographic(dem_file, climate_data)\n        else:\n            eemt = calculate_eemt_vegetation(dem_file, climate_data)\n\n        # Validate against field data\n        validation = validate_eemt_results(eemt, validation_points)\n        results[method] = validation\n\n    # Compare methods\n    print(\"\\\\nMethod Comparison:\")\n    print(\"-\" * 50)\n    for method, validation in results.items():\n        if 'soil_depth' in validation:\n            r2 = validation['soil_depth']['r_squared']\n            print(f\"{method.capitalize():12} | R¬≤ = {r2:.3f}\")\n\n    return results\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#utility-functions","level":2,"title":"Utility Functions","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#data-processing-utilities","level":3,"title":"Data Processing Utilities","text":"<pre><code>def extract_values_at_points(raster_file, coordinates, method='bilinear'):\n    \"\"\"Extract raster values at point locations\"\"\"\n\n    import rasterio\n    from rasterio.sample import sample_gen\n\n    with rasterio.open(raster_file) as src:\n        values = list(sample_gen(src, coordinates, indexes=1))\n\n    return np.array([val[0] for val in values])\n\ndef calculate_zonal_statistics(raster_file, zones_file, statistics=['mean', 'std']):\n    \"\"\"Calculate statistics by zones (e.g., elevation bands, watersheds)\"\"\"\n\n    from rasterstats import zonal_stats\n    import geopandas as gpd\n\n    # Load zones\n    zones = gpd.read_file(zones_file)\n\n    # Calculate statistics\n    stats_result = zonal_stats(\n        zones, \n        raster_file, \n        stats=statistics,\n        geojson_out=True\n    )\n\n    return gpd.GeoDataFrame.from_features(stats_result)\n\ndef resample_to_common_grid(file_list, reference_file, output_dir, method='bilinear'):\n    \"\"\"Resample all rasters to common grid\"\"\"\n\n    import subprocess\n    from pathlib import Path\n\n    output_dir = Path(output_dir)\n    output_dir.mkdir(exist_ok=True)\n\n    # Get reference grid parameters\n    with rasterio.open(reference_file) as src:\n        ref_transform = src.transform\n        ref_crs = src.crs\n        ref_width = src.width\n        ref_height = src.height\n\n    resampled_files = []\n\n    for input_file in file_list:\n\n        output_file = output_dir / f\"resampled_{Path(input_file).name}\"\n\n        # Use gdalwarp for resampling\n        cmd = [\n            'gdalwarp',\n            '-tr', str(ref_transform[0]), str(-ref_transform[4]),  # Resolution\n            '-te', str(ref_transform[2]), str(ref_transform[5]),   # Extent  \n                   str(ref_transform[2] + ref_width * ref_transform[0]),\n                   str(ref_transform[5] + ref_height * ref_transform[4]),\n            '-t_srs', str(ref_crs),  # Target CRS\n            '-r', method,            # Resampling method\n            '-co', 'COMPRESS=LZW',   # Compression\n            str(input_file),\n            str(output_file)\n        ]\n\n        subprocess.run(cmd, check=True)\n        resampled_files.append(str(output_file))\n\n    return resampled_files\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#command-line-interface","level":2,"title":"Command Line Interface","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#main-eemt-calculator-script","level":3,"title":"Main EEMT Calculator Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nCommand line interface for EEMT calculations\nUsage: python eemt_calculator.py [options] dem_file\n\"\"\"\n\nusage_examples = '''\nExamples:\n  # Basic EEMT calculation\n  python eemt_calculator.py dem.tif --climate climate_data/ --output results/\n\n  # Topographic EEMT with parallel processing\n  python eemt_calculator.py dem.tif --method topographic --threads 16 \\\\\n    --climate daymet_data/ --output topo_results/\n\n  # Full vegetation EEMT with validation\n  python eemt_calculator.py dem.tif --method vegetation \\\\\n    --climate climate/ --vegetation ndvi.tif,lidar.las \\\\\n    --validate soil_samples.shp --output veg_results/\n\n  # Time series analysis\n  python eemt_calculator.py dem.tif --method topographic \\\\\n    --start-year 2000 --end-year 2020 --time-series \\\\\n    --output timeseries_results/\n'''\n\n# Command line argument definitions\nCLI_ARGUMENTS = {\n    'dem_file': {\n        'type': str,\n        'help': 'Input digital elevation model (GeoTIFF format)'\n    },\n    '--method': {\n        'choices': ['traditional', 'topographic', 'vegetation', 'all'],\n        'default': 'topographic',\n        'help': 'EEMT calculation method'\n    },\n    '--climate': {\n        'type': str, \n        'required': True,\n        'help': 'Climate data directory (DAYMET NetCDF files)'\n    },\n    '--output': {\n        'type': str,\n        'required': True, \n        'help': 'Output directory for results'\n    },\n    '--threads': {\n        'type': int,\n        'default': 4,\n        'help': 'Number of parallel processing threads'\n    },\n    '--step': {\n        'type': float,\n        'default': 0.25,\n        'help': 'Solar calculation time step [hours]'\n    },\n    '--linke': {\n        'type': float,\n        'default': 3.0,\n        'help': 'Linke atmospheric turbidity factor [1.0-8.0]'\n    },\n    '--albedo': {\n        'type': float, \n        'default': 0.2,\n        'help': 'Surface albedo coefficient [0.0-1.0]'\n    },\n    '--vegetation': {\n        'type': str,\n        'help': 'Vegetation data files (comma-separated): ndvi.tif,lidar.las'\n    },\n    '--start-year': {\n        'type': int,\n        'default': 2015,\n        'help': 'Start year for time series analysis'\n    },\n    '--end-year': {\n        'type': int, \n        'default': 2020,\n        'help': 'End year for time series analysis'\n    },\n    '--validate': {\n        'type': str,\n        'help': 'Validation data file (point shapefile with measurements)'\n    },\n    '--time-series': {\n        'action': 'store_true',\n        'help': 'Generate annual time series output'\n    },\n    '--tile-size': {\n        'type': int,\n        'default': 2048,\n        'help': 'Tile size for large dataset processing [pixels]'\n    },\n    '--verbose': {\n        'action': 'store_true',\n        'help': 'Enable verbose output'\n    }\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#testing-framework","level":2,"title":"Testing Framework","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#unit-tests","level":3,"title":"Unit Tests","text":"<pre><code>import unittest\nimport numpy as np\nfrom pathlib import Path\n\nclass TestEEMTCalculations(unittest.TestCase):\n    \"\"\"Unit tests for EEMT calculation functions\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test data\"\"\"\n        self.test_data_dir = Path('test_data')\n        self.test_data_dir.mkdir(exist_ok=True)\n\n        # Create synthetic test DEM\n        self.create_test_dem()\n\n        # Create synthetic climate data\n        self.create_test_climate()\n\n    def test_traditional_eemt(self):\n        \"\"\"Test traditional EEMT calculation\"\"\"\n\n        # Simple test case\n        temp = np.array([15.0])  # ¬∞C\n        precip = np.array([50.0])  # mm/month\n\n        eemt, e_bio, e_ppt = calculate_eemt_traditional(temp, precip)\n\n        # Check output ranges\n        self.assertGreater(eemt[0], 0, \"EEMT should be positive\")\n        self.assertLess(eemt[0], 100, \"EEMT should be reasonable (&lt;100 MJ/m¬≤/yr)\")\n\n        # Check components\n        self.assertGreater(e_bio[0], 0, \"E_BIO should be positive\")\n        self.assertGreaterEqual(e_ppt[0], 0, \"E_PPT should be non-negative\")\n\n    def test_solar_radiation_range(self):\n        \"\"\"Test solar radiation calculations produce reasonable values\"\"\"\n\n        # Test with synthetic DEM\n        dem_file = self.test_data_dir / 'test_dem.tif'\n\n        # Should complete without errors\n        try:\n            solar_result = calculate_annual_solar(dem_file, days=[180])  # Summer solstice\n            self.assertTrue(True, \"Solar calculation completed\")\n        except Exception as e:\n            self.fail(f\"Solar calculation failed: {e}\")\n\n    def test_aspect_effects(self):\n        \"\"\"Test that north-facing slopes have higher EEMT\"\"\"\n\n        # Create test data with known aspect effects\n        north_slope_eemt = calculate_eemt_topographic(\n            self.create_test_slope(aspect=0)  # North-facing\n        )\n\n        south_slope_eemt = calculate_eemt_topographic(\n            self.create_test_slope(aspect=180)  # South-facing\n        )\n\n        # North slopes should have higher EEMT in water-limited environments\n        self.assertGreater(\n            np.mean(north_slope_eemt),\n            np.mean(south_slope_eemt),\n            \"North-facing slopes should have higher EEMT\"\n        )\n\n    def create_test_dem(self):\n        \"\"\"Create synthetic DEM for testing\"\"\"\n\n        # Create elevation gradient\n        x, y = np.meshgrid(np.linspace(0, 1000, 100), np.linspace(0, 1000, 100))\n        elevation = 1000 + x * 0.5 + y * 0.3 + np.random.normal(0, 10, (100, 100))\n\n        # Save as GeoTIFF\n        profile = {\n            'driver': 'GTiff',\n            'height': 100,\n            'width': 100,\n            'count': 1,\n            'dtype': 'float32',\n            'crs': 'EPSG:4326',\n            'transform': rasterio.transform.from_bounds(-111, 32, -110, 33, 100, 100)\n        }\n\n        with rasterio.open(self.test_data_dir / 'test_dem.tif', 'w', **profile) as dst:\n            dst.write(elevation.astype(np.float32), 1)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/cli/#integration-tests","level":3,"title":"Integration Tests","text":"<pre><code>#!/bin/bash\n# Integration test suite for EEMT workflows\n\nset -e\n\necho \"=== EEMT Integration Tests ===\"\n\n# Test 1: Basic workflow with sample data\necho \"Test 1: Basic EEMT workflow...\"\npython eemt_calculator.py test_data/sample_dem.tif \\\\\n  --climate test_data/climate/ \\\\\n  --output test_results/basic/ \\\\\n  --method traditional\n\n# Test 2: Parallel processing\necho \"Test 2: Parallel solar calculation...\"\npython eemt_calculator.py test_data/sample_dem.tif \\\\\n  --climate test_data/climate/ \\\\\n  --output test_results/parallel/ \\\\\n  --method topographic \\\\\n  --threads 4\n\n# Test 3: Large dataset handling\necho \"Test 3: Large dataset processing...\"\npython eemt_calculator.py test_data/large_dem.tif \\\\\n  --climate test_data/climate/ \\\\\n  --output test_results/large/ \\\\\n  --tile-size 1024\n\n# Test 4: Validation\necho \"Test 4: Results validation...\"\npython validate_results.py test_results/ test_data/validation/\n\necho \"‚úì All integration tests passed\"\n</code></pre> <p>This API reference provides the complete technical foundation for implementing and extending EEMT calculations with modern computational approaches and comprehensive error handling.</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/","level":1,"title":"API Reference","text":"<p>The EEMT Web Interface exposes a comprehensive REST API for programmatic workflow submission and monitoring.</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#base-url","level":2,"title":"Base URL","text":"<pre><code>http://127.0.0.1:5000\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#authentication","level":2,"title":"Authentication","text":"<p>Currently, the API does not require authentication. Future versions may include:</p> <ul> <li>API key authentication</li> <li>JWT token-based access</li> <li>Role-based permissions</li> </ul>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#content-types","level":2,"title":"Content Types","text":"<ul> <li>Request: <code>multipart/form-data</code> for file uploads, <code>application/json</code> for data</li> <li>Response: <code>application/json</code> for all endpoints except file downloads</li> </ul>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#job-management","level":2,"title":"Job Management","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#submit-workflow-job","level":3,"title":"Submit Workflow Job","text":"<p>Submit a new EEMT or solar radiation workflow for processing.</p> <pre><code>POST /api/submit-job\nContent-Type: multipart/form-data\n</code></pre> <p>Parameters:</p> Parameter Type Required Description <code>workflow_type</code> string Yes <code>\"sol\"</code> for solar radiation, <code>\"eemt\"</code> for full analysis <code>dem_file</code> file Yes GeoTIFF elevation model file <code>step</code> float No Time step in minutes (default: 15.0, range: 3-60) <code>linke_value</code> float No Atmospheric turbidity (default: 3.0, range: 1.0-8.0) <code>albedo_value</code> float No Surface reflectance (default: 0.2, range: 0.0-1.0) <code>num_threads</code> integer No CPU threads (default: 4, range: 1-32) <code>start_year</code> integer No Start year for EEMT (default: 2020, range: 1980-2024) <code>end_year</code> integer No End year for EEMT (default: 2020, range: 1980-2024) <p>Example Request:</p> <pre><code>curl -X POST \"http://127.0.0.1:5000/api/submit-job\" \\\n  -F \"workflow_type=sol\" \\\n  -F \"dem_file=@my_elevation_data.tif\" \\\n  -F \"step=15\" \\\n  -F \"linke_value=3.5\" \\\n  -F \"albedo_value=0.2\" \\\n  -F \"num_threads=8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"job_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n  \"status\": \"submitted\"\n}\n</code></pre> <p>Error Responses:</p> <pre><code>// Invalid file format\n{\n  \"detail\": \"DEM file must be a GeoTIFF (.tif)\"\n}\n\n// Missing required parameter\n{\n  \"detail\": \"workflow_type is required\"\n}\n\n// Docker not available\n{\n  \"detail\": \"Container execution failed: Docker daemon not available\"\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#list-all-jobs","level":3,"title":"List All Jobs","text":"<p>Retrieve a list of all workflow jobs.</p> <pre><code>GET /api/jobs\n</code></pre> <p>Response:</p> <pre><code>[\n  {\n    \"id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"workflow_type\": \"sol\",\n    \"status\": \"completed\",\n    \"created_at\": \"2025-01-15T10:30:00Z\",\n    \"dem_filename\": \"my_elevation_data.tif\",\n    \"progress\": 100\n  },\n  {\n    \"id\": \"b2c3d4e5-f6g7-8901-bcde-f23456789012\",\n    \"workflow_type\": \"eemt\", \n    \"status\": \"running\",\n    \"created_at\": \"2025-01-15T11:15:00Z\",\n    \"dem_filename\": \"large_dem.tif\",\n    \"progress\": 65\n  }\n]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#get-job-details","level":3,"title":"Get Job Details","text":"<p>Retrieve detailed information about a specific job.</p> <pre><code>GET /api/jobs/{job_id}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n  \"workflow_type\": \"sol\",\n  \"status\": \"completed\",\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"started_at\": \"2025-01-15T10:30:15Z\",\n  \"completed_at\": \"2025-01-15T10:45:30Z\",\n  \"parameters\": {\n    \"step\": 15.0,\n    \"linke_value\": 3.5,\n    \"albedo_value\": 0.2,\n    \"num_threads\": 8\n  },\n  \"dem_filename\": \"my_elevation_data.tif\",\n  \"error_message\": null,\n  \"progress\": 100\n}\n</code></pre> <p>Error Responses:</p> <pre><code>// Job not found\n{\n  \"detail\": \"Job not found\"\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#download-job-results","level":3,"title":"Download Job Results","text":"<p>Download the complete results of a completed job as a ZIP archive.</p> <pre><code>GET /api/jobs/{job_id}/results\n</code></pre> <p>Response:</p> <ul> <li>Success: ZIP file download with filename <code>eemt_results_{job_id}.zip</code></li> <li>Content-Type: <code>application/zip</code></li> </ul> <p>Example Request:</p> <pre><code>curl -O \"http://127.0.0.1:5000/api/jobs/a1b2c3d4-e5f6-7890-abcd-ef1234567890/results\"\n</code></pre> <p>Error Responses:</p> <pre><code>// Job not completed\n{\n  \"detail\": \"Job not completed\"\n}\n\n// Results not found\n{\n  \"detail\": \"Results not found\"\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#system-information","level":2,"title":"System Information","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#get-system-status","level":3,"title":"Get System Status","text":"<p>Retrieve Docker availability and container statistics.</p> <pre><code>GET /api/system/status\n</code></pre> <p>Response:</p> <pre><code>{\n  \"docker_available\": true,\n  \"container_stats\": {\n    \"total_containers\": 2,\n    \"running_jobs\": [\n      \"job-a1b2c3d4\",\n      \"job-b2c3d4e5\"\n    ],\n    \"system_stats\": {\n      \"cpus\": 8,\n      \"memory\": 16777216000,\n      \"containers_running\": 2\n    }\n  },\n  \"image_name\": \"eemt:ubuntu24.04\"\n}\n</code></pre> <p>Error Response (Docker unavailable):</p> <pre><code>{\n  \"docker_available\": false,\n  \"error\": \"Docker daemon not reachable\",\n  \"image_name\": \"eemt:ubuntu24.04\"\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#data-management","level":2,"title":"Data Management","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#trigger-job-cleanup","level":3,"title":"Trigger Job Cleanup","text":"<p>Execute cleanup of old job data based on retention policies.</p> <pre><code>POST /api/cleanup\nContent-Type: application/json\n</code></pre> <p>Parameters:</p> Parameter Type Required Description <code>dry_run</code> boolean No Preview cleanup without deletion (default: false) <code>success_retention_days</code> integer No Days to keep successful job data (default: 7) <code>failed_retention_hours</code> integer No Hours to keep failed job data (default: 12) <p>Example Request:</p> <pre><code># Trigger cleanup with defaults\ncurl -X POST http://127.0.0.1:5000/api/cleanup\n\n# Dry run with custom retention\ncurl -X POST http://127.0.0.1:5000/api/cleanup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dry_run\": true,\n    \"success_retention_days\": 3,\n    \"failed_retention_hours\": 6\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"summary\": {\n    \"start_time\": \"2024-01-20T14:30:00.123456\",\n    \"end_time\": \"2024-01-20T14:30:05.789012\",\n    \"dry_run\": false,\n    \"successful_jobs_processed\": 3,\n    \"failed_jobs_processed\": 2,\n    \"total_size_freed_mb\": 15678.9,\n    \"configs_preserved\": 3,\n    \"configs_deleted\": 2,\n    \"errors\": [],\n    \"job_details\": [\n      {\n        \"job_id\": \"job-20240113-123456\",\n        \"status\": \"completed\",\n        \"completed_at\": \"2024-01-13T14:23:45\",\n        \"data_deleted\": true,\n        \"config_preserved\": true,\n        \"size_freed_mb\": 8234.5\n      }\n    ]\n  },\n  \"timestamp\": \"2024-01-20T14:30:05.789012\"\n}\n</code></pre> <p>Error Response:</p> <pre><code>{\n  \"success\": false,\n  \"error\": \"Cleanup process failed\",\n  \"details\": \"Database is locked by another process\"\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#get-cleanup-status","level":3,"title":"Get Cleanup Status","text":"<p>Retrieve information about the last cleanup run.</p> <pre><code>GET /api/cleanup/status\n</code></pre> <p>Response:</p> <pre><code>{\n  \"last_run\": \"2024-01-20T02:00:00\",\n  \"next_scheduled\": \"2024-01-21T02:00:00\",\n  \"auto_cleanup_enabled\": true,\n  \"retention_settings\": {\n    \"success_retention_days\": 7,\n    \"failed_retention_hours\": 12\n  },\n  \"disk_usage\": {\n    \"uploads_mb\": 1234.5,\n    \"results_mb\": 45678.9,\n    \"total_mb\": 46913.4\n  }\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#job-status-values","level":2,"title":"Job Status Values","text":"Status Description <code>pending</code> Job queued, waiting to start <code>running</code> Container executing workflow <code>completed</code> Job finished successfully <code>failed</code> Job terminated with error","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#progress-tracking","level":2,"title":"Progress Tracking","text":"<p>Progress is reported as integer percentage (0-100):</p> <ul> <li>0-10: Job initialization and container startup</li> <li>10-90: Workflow execution (varies by complexity)</li> <li>90-100: Results collection and cleanup</li> </ul>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#rate-limiting","level":2,"title":"Rate Limiting","text":"<p>Currently no rate limiting is implemented. Consider implementing:</p> <ul> <li>Max concurrent jobs per client</li> <li>File upload size limits (currently ~1GB recommended)</li> <li>API request frequency limits</li> </ul>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#error-handling","level":2,"title":"Error Handling","text":"<p>All errors follow consistent format:</p> <pre><code>{\n  \"detail\": \"Human-readable error message\"\n}\n</code></pre> <p>Common HTTP status codes:</p> <ul> <li><code>200</code>: Success</li> <li><code>400</code>: Bad Request (invalid parameters)</li> <li><code>404</code>: Not Found (job/resource doesn't exist)</li> <li><code>422</code>: Unprocessable Entity (validation error)</li> <li><code>500</code>: Internal Server Error (system/container issue)</li> </ul>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#examples","level":2,"title":"Examples","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#python-client-example","level":3,"title":"Python Client Example","text":"<pre><code>import requests\nimport time\n\n# Submit job\nwith open('my_dem.tif', 'rb') as f:\n    response = requests.post(\n        'http://127.0.0.1:5000/api/submit-job',\n        files={'dem_file': f},\n        data={\n            'workflow_type': 'sol',\n            'step': 15,\n            'num_threads': 4\n        }\n    )\n\njob_id = response.json()['job_id']\nprint(f\"Job submitted: {job_id}\")\n\n# Monitor progress\nwhile True:\n    status = requests.get(f'http://127.0.0.1:5000/api/jobs/{job_id}').json()\n    print(f\"Status: {status['status']} ({status['progress']}%)\")\n\n    if status['status'] in ['completed', 'failed']:\n        break\n    time.sleep(10)\n\n# Download results if successful\nif status['status'] == 'completed':\n    results = requests.get(f'http://127.0.0.1:5000/api/jobs/{job_id}/results')\n    with open(f'results_{job_id}.zip', 'wb') as f:\n        f.write(results.content)\n    print(\"Results downloaded!\")\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#javascript-client-example","level":3,"title":"JavaScript Client Example","text":"<pre><code>// Submit job\nconst formData = new FormData();\nformData.append('workflow_type', 'sol');\nformData.append('dem_file', demFileInput.files[0]);\nformData.append('step', '15');\nformData.append('num_threads', '4');\n\nconst submitResponse = await fetch('/api/submit-job', {\n    method: 'POST',\n    body: formData\n});\n\nconst submitResult = await submitResponse.json();\nconsole.log('Job submitted:', submitResult.job_id);\n\n// Monitor progress\nconst jobId = submitResult.job_id;\nconst checkStatus = async () =&gt; {\n    const response = await fetch(`/api/jobs/${jobId}`);\n    const job = await response.json();\n\n    console.log(`Status: ${job.status} (${job.progress}%)`);\n\n    if (job.status === 'completed') {\n        // Download results\n        window.open(`/api/jobs/${jobId}/results`);\n    } else if (job.status === 'failed') {\n        console.error('Job failed:', job.error_message);\n    } else {\n        setTimeout(checkStatus, 5000); // Check again in 5 seconds\n    }\n};\n\ncheckStatus();\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#bash-script-example","level":3,"title":"Bash Script Example","text":"<pre><code>#!/bin/bash\n\n# Submit job\nRESPONSE=$(curl -s -X POST \"http://127.0.0.1:5000/api/submit-job\" \\\n  -F \"workflow_type=sol\" \\\n  -F \"dem_file=@dem.tif\" \\\n  -F \"step=15\" \\\n  -F \"num_threads=4\")\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n\n# Monitor progress\nwhile true; do\n    STATUS=$(curl -s \"http://127.0.0.1:5000/api/jobs/$JOB_ID\" | jq -r '.status')\n    PROGRESS=$(curl -s \"http://127.0.0.1:5000/api/jobs/$JOB_ID\" | jq -r '.progress')\n\n    echo \"Status: $STATUS ($PROGRESS%)\"\n\n    if [[ \"$STATUS\" == \"completed\" ]]; then\n        echo \"Downloading results...\"\n        curl -O \"http://127.0.0.1:5000/api/jobs/$JOB_ID/results\"\n        break\n    elif [[ \"$STATUS\" == \"failed\" ]]; then\n        echo \"Job failed!\"\n        break\n    fi\n\n    sleep 10\ndone\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/endpoints/#openapi-documentation","level":2,"title":"OpenAPI Documentation","text":"<p>Interactive API documentation is automatically generated and available at:</p> <ul> <li>Swagger UI: http://127.0.0.1:5000/docs</li> <li>ReDoc: http://127.0.0.1:5000/redoc </li> <li>OpenAPI JSON: http://127.0.0.1:5000/openapi.json</li> </ul> <p>The interactive documentation allows you to:</p> <ul> <li>Browse all available endpoints</li> <li>Test API calls directly in the browser</li> <li>View detailed parameter descriptions</li> <li>See example requests and responses</li> </ul>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api/parameters/","level":1,"title":"Workflow Parameters Reference","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#overview","level":2,"title":"Overview","text":"<p>This comprehensive reference documents all parameters available for EEMT and Solar Radiation workflows, including their scientific basis, valid ranges, and impact on calculations.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#solar-radiation-workflow-parameters","level":2,"title":"Solar Radiation Workflow Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#core-parameters","level":3,"title":"Core Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#step-time-step","level":4,"title":"<code>step</code> - Time Step","text":"<p>Type: Float Default: 15.0 Range: 3.0 - 60.0 minutes Required: No</p> <p>Description: Time interval for solar radiation calculations throughout the day. Smaller values provide higher temporal resolution but increase computation time.</p> <p>Scientific Basis: The time step determines how frequently the sun's position is calculated and solar radiation is computed. This affects the accuracy of daily radiation totals, especially in complex terrain.</p> <p>Impact on Results: - 3-5 minutes: High precision, captures rapid shadow changes in complex terrain - 10-15 minutes: Good balance of accuracy and performance for most applications - 30-60 minutes: Faster computation, suitable for regional assessments</p> <p>Example Usage: <pre><code># High-resolution analysis for complex terrain\nparameters = {\n    'step': 3.0,  # 3-minute intervals\n    'num_threads': 8\n}\n\n# Regional assessment with moderate resolution\nparameters = {\n    'step': 15.0,  # 15-minute intervals (default)\n    'num_threads': 4\n}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#linke_value-atmospheric-turbidity","level":4,"title":"<code>linke_value</code> - Atmospheric Turbidity","text":"<p>Type: Float Default: 3.0 Range: 1.0 - 8.0 Required: No</p> <p>Description: Linke turbidity factor representing atmospheric optical thickness due to absorption and scattering.</p> <p>Scientific Basis: The Linke turbidity factor accounts for the attenuation of solar radiation as it passes through the atmosphere. It combines the effects of: - Water vapor absorption - Aerosol scattering - Molecular (Rayleigh) scattering</p> <p>Typical Values by Environment:</p> Environment Linke Value Description Clean mountain air 1.0 - 2.0 Very clear atmosphere, high elevation Rural areas 2.5 - 3.5 Clean continental atmosphere Urban areas 3.5 - 5.0 Moderate pollution and aerosols Industrial zones 5.0 - 8.0 Heavy pollution, high aerosol content <p>Seasonal Variations: <pre><code># Winter (clearer atmosphere)\nwinter_params = {'linke_value': 2.5}\n\n# Summer (more water vapor and aerosols)\nsummer_params = {'linke_value': 3.5}\n\n# Monsoon/humid season\nhumid_params = {'linke_value': 4.5}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#albedo_value-surface-reflectance","level":4,"title":"<code>albedo_value</code> - Surface Reflectance","text":"<p>Type: Float Default: 0.2 Range: 0.0 - 1.0 Required: No</p> <p>Description: Fraction of incident solar radiation reflected by the surface.</p> <p>Scientific Basis: Albedo affects the amount of diffuse radiation through multiple reflections between the surface and atmosphere. Higher albedo increases the total radiation received through these interactions.</p> <p>Typical Values by Surface Type:</p> Surface Type Albedo Example Fresh snow 0.80 - 0.95 Alpine environments Old snow 0.50 - 0.70 Late season snowpack Desert sand 0.30 - 0.45 Arid regions Grassland 0.15 - 0.25 Natural vegetation Forest 0.10 - 0.20 Dense canopy Water 0.05 - 0.10 Lakes, oceans Asphalt 0.05 - 0.15 Urban surfaces <p>Land Cover Specific Examples: <pre><code># Forest analysis\nforest_params = {\n    'albedo_value': 0.15,\n    'linke_value': 2.8  # Some canopy filtering\n}\n\n# Snow-covered terrain\nsnow_params = {\n    'albedo_value': 0.85,\n    'linke_value': 2.0  # Clear winter air\n}\n\n# Urban environment\nurban_params = {\n    'albedo_value': 0.12,\n    'linke_value': 4.5  # Urban pollution\n}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#computational-parameters","level":3,"title":"Computational Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#num_threads-cpu-threads","level":4,"title":"<code>num_threads</code> - CPU Threads","text":"<p>Type: Integer Default: 4 Range: 1 - 32 Required: No</p> <p>Description: Number of parallel processing threads for workflow execution.</p> <p>Performance Impact: <pre><code># Estimated processing times (10km x 10km @ 10m resolution)\n# 1 thread:  ~4 hours\n# 4 threads: ~1 hour (default)\n# 8 threads: ~35 minutes\n# 16 threads: ~20 minutes (diminishing returns)\n</code></pre></p> <p>Resource Considerations: - Each thread requires ~2GB RAM - I/O becomes bottleneck beyond 8-16 threads - Leave 1-2 cores for system processes</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#advanced-parameters","level":3,"title":"Advanced Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#day-specific-day","level":4,"title":"<code>day</code> - Specific Day","text":"<p>Type: Integer Range: 1 - 365 Required: No (processes all days if not specified)</p> <p>Description: Calculate solar radiation for a specific day of year.</p> <pre><code># Summer solstice analysis\nparams = {'day': 172}  # June 21 (day 172)\n\n# Winter solstice analysis  \nparams = {'day': 355}  # December 21 (day 355)\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#beam_rad-beam-radiation-output","level":4,"title":"<code>beam_rad</code> - Beam Radiation Output","text":"<p>Type: Boolean Default: True Required: No</p> <p>Description: Output direct beam radiation component separately.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#diff_rad-diffuse-radiation-output","level":4,"title":"<code>diff_rad</code> - Diffuse Radiation Output","text":"<p>Type: Boolean Default: True Required: No</p> <p>Description: Output diffuse radiation component separately.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#refl_rad-reflected-radiation-output","level":4,"title":"<code>refl_rad</code> - Reflected Radiation Output","text":"<p>Type: Boolean Default: False Required: No</p> <p>Description: Output ground-reflected radiation component.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#eemt-workflow-parameters","level":2,"title":"EEMT Workflow Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#temporal-parameters","level":3,"title":"Temporal Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#start_year-start-year","level":4,"title":"<code>start_year</code> - Start Year","text":"<p>Type: Integer Default: 2020 Range: 1980 - 2024 Required: Yes for EEMT workflow</p> <p>Description: First year of climate data to process.</p> <p>Data Availability: - DAYMET v4: 1980 - present (1-2 year lag) - Quality varies by year and region - Recent years may have provisional data</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#end_year-end-year","level":4,"title":"<code>end_year</code> - End Year","text":"<p>Type: Integer Default: 2020 Range: 1980 - 2024 Required: Yes for EEMT workflow</p> <p>Description: Last year of climate data to process.</p> <p>Multi-Year Processing: <pre><code># Single year analysis\nparams = {\n    'start_year': 2020,\n    'end_year': 2020\n}\n\n# 5-year climatology\nparams = {\n    'start_year': 2016,\n    'end_year': 2020\n}\n\n# Long-term analysis (30+ years)\nparams = {\n    'start_year': 1990,\n    'end_year': 2020,\n    'num_threads': 16  # Use more threads for large datasets\n}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#climate-data-parameters","level":3,"title":"Climate Data Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#daymet_variables-climate-variables","level":4,"title":"<code>daymet_variables</code> - Climate Variables","text":"<p>Type: List[str] Default: ['tmin', 'tmax', 'prcp', 'vp'] Options: tmin, tmax, prcp, vp, srad, swe, dayl Required: No</p> <p>Description: DAYMET climate variables to download and process.</p> <p>Variable Descriptions:</p> Variable Description Units Use in EEMT tmin Daily minimum temperature ¬∞C Energy calculations tmax Daily maximum temperature ¬∞C Energy calculations prcp Daily precipitation mm/day Water flux vp Daily average vapor pressure Pa Humidity effects srad Incoming shortwave radiation W/m¬≤ Validation swe Snow water equivalent kg/m¬≤ Snow dynamics dayl Day length seconds Photoperiod","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#climate_buffer-spatial-buffer","level":4,"title":"<code>climate_buffer</code> - Spatial Buffer","text":"<p>Type: Float Default: 0.1 Range: 0.0 - 1.0 degrees Required: No</p> <p>Description: Buffer around DEM extent for climate data download.</p> <pre><code># Tight boundary (minimize download)\nparams = {'climate_buffer': 0.01}\n\n# Include surrounding area for edge effects\nparams = {'climate_buffer': 0.25}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#eemt-calculation-parameters","level":3,"title":"EEMT Calculation Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#eemt_method-calculation-method","level":4,"title":"<code>eemt_method</code> - Calculation Method","text":"<p>Type: String Default: 'topographic' Options: 'traditional', 'topographic', 'vegetation' Required: No</p> <p>Description: EEMT calculation methodology.</p> <p>Method Comparison:</p> Method Description Inputs Required Best For traditional Climate-based only Climate data Regional comparison topographic Terrain-modified DEM + climate Complex terrain vegetation Full ecosystem DEM + climate + LAI Detailed analysis <p>Method Selection: <pre><code># Simple regional assessment\nparams = {'eemt_method': 'traditional'}\n\n# Mountain watershed analysis\nparams = {'eemt_method': 'topographic'}\n\n# Ecosystem carbon studies\nparams = {'eemt_method': 'vegetation'}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#npp_model-npp-calculation","level":4,"title":"<code>npp_model</code> - NPP Calculation","text":"<p>Type: String Default: 'miami' Options: 'miami', 'thornthwaite', 'user_defined' Required: No</p> <p>Description: Net Primary Production model for biological energy calculation.</p> <p>Model Equations:</p> <p>Miami Model: <pre><code># Temperature-limited\nNPP_t = 3000 * (1 - exp(1.315 - 0.119 * T))\n\n# Precipitation-limited  \nNPP_p = 3000 * (1 - exp(-0.000664 * P))\n\n# Actual NPP (minimum)\nNPP = min(NPP_t, NPP_p)\n</code></pre></p> <p>Thornthwaite Model: <pre><code># Based on evapotranspiration\nNPP = 3000 * (AET / PET)\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#topographic-parameters","level":3,"title":"Topographic Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#slope_threshold-maximum-slope","level":4,"title":"<code>slope_threshold</code> - Maximum Slope","text":"<p>Type: Float Default: 45.0 Range: 0.0 - 90.0 degrees Required: No</p> <p>Description: Maximum slope angle for stable soil formation.</p> <p>Geomorphological Context: - &lt; 15¬∞: Minimal erosion, stable soils - 15-30¬∞: Moderate erosion potential - 30-45¬∞: High erosion, thin soils - &gt; 45¬∞: Bedrock exposure likely</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#twi_threshold-wetness-threshold","level":4,"title":"<code>twi_threshold</code> - Wetness Threshold","text":"<p>Type: Float Default: 10.0 Range: 0.0 - 20.0 Required: No</p> <p>Description: Topographic Wetness Index threshold for water accumulation zones.</p> <p>TWI Interpretation: - &lt; 5: Ridge tops, dry areas - 5-10: Hillslopes, normal drainage - 10-15: Convergent areas, seasonal wetness - &gt; 15: Valley bottoms, persistent wetness</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#output-control-parameters","level":3,"title":"Output Control Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#output_format-file-format","level":4,"title":"<code>output_format</code> - File Format","text":"<p>Type: String Default: 'geotiff' Options: 'geotiff', 'netcdf', 'zarr' Required: No</p> <p>Format Characteristics:</p> Format Pros Cons Best For GeoTIFF Wide compatibility Single variable per file GIS integration NetCDF Multi-dimensional Requires special tools Time series Zarr Cloud-optimized New format Big data","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#output_compression-compression","level":4,"title":"<code>output_compression</code> - Compression","text":"<p>Type: String Default: 'lzw' Options: 'none', 'lzw', 'deflate', 'zstd' Required: No</p> <p>Compression Trade-offs: <pre><code># No compression (fastest write, largest files)\nparams = {'output_compression': 'none'}\n\n# LZW (good balance, wide support)\nparams = {'output_compression': 'lzw'}\n\n# Deflate (better compression, slower)\nparams = {'output_compression': 'deflate'}\n\n# Zstd (best compression, newest)\nparams = {'output_compression': 'zstd'}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#output_resolution-output-resolution","level":4,"title":"<code>output_resolution</code> - Output Resolution","text":"<p>Type: Float Default: Same as input DEM Range: 1.0 - 1000.0 meters Required: No</p> <p>Description: Resample output to different resolution.</p> <pre><code># Maintain input resolution\nparams = {}  # Default behavior\n\n# Aggregate to coarser resolution\nparams = {'output_resolution': 30.0}  # 30m output\n\n# Regional product\nparams = {'output_resolution': 100.0}  # 100m output\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#parameter-validation","level":2,"title":"Parameter Validation","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#input-validation-rules","level":3,"title":"Input Validation Rules","text":"<pre><code>def validate_parameters(params: dict, workflow_type: str) -&gt; dict:\n    \"\"\"Validate and sanitize workflow parameters\"\"\"\n\n    # Common validations\n    if params.get('step'):\n        assert 3.0 &lt;= params['step'] &lt;= 60.0\n\n    if params.get('linke_value'):\n        assert 1.0 &lt;= params['linke_value'] &lt;= 8.0\n\n    if params.get('albedo_value'):\n        assert 0.0 &lt;= params['albedo_value'] &lt;= 1.0\n\n    if params.get('num_threads'):\n        assert 1 &lt;= params['num_threads'] &lt;= 32\n\n    # EEMT-specific validations\n    if workflow_type == 'eemt':\n        assert params.get('start_year'), \"start_year required for EEMT\"\n        assert params.get('end_year'), \"end_year required for EEMT\"\n        assert params['start_year'] &lt;= params['end_year']\n        assert 1980 &lt;= params['start_year'] &lt;= 2024\n        assert 1980 &lt;= params['end_year'] &lt;= 2024\n\n    return params\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#parameter-combinations","level":3,"title":"Parameter Combinations","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#high-accuracy-solar-analysis","level":4,"title":"High-Accuracy Solar Analysis","text":"<pre><code>high_accuracy = {\n    'step': 3.0,\n    'linke_value': 3.0,\n    'albedo_value': 0.2,\n    'num_threads': 16,\n    'beam_rad': True,\n    'diff_rad': True,\n    'refl_rad': True\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#quick-regional-assessment","level":4,"title":"Quick Regional Assessment","text":"<pre><code>quick_regional = {\n    'step': 30.0,\n    'linke_value': 3.0,\n    'albedo_value': 0.2,\n    'num_threads': 4,\n    'output_resolution': 100.0,\n    'output_compression': 'lzw'\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#climate-change-analysis","level":4,"title":"Climate Change Analysis","text":"<pre><code>climate_analysis = {\n    'start_year': 1990,\n    'end_year': 2020,\n    'step': 15.0,\n    'eemt_method': 'topographic',\n    'daymet_variables': ['tmin', 'tmax', 'prcp', 'vp', 'srad'],\n    'output_format': 'netcdf'\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#snow-dominated-watershed","level":4,"title":"Snow-Dominated Watershed","text":"<pre><code>snow_watershed = {\n    'step': 10.0,\n    'linke_value': 2.5,  # Clear mountain air\n    'albedo_value': 0.65,  # Snow average\n    'start_year': 2020,\n    'end_year': 2020,\n    'daymet_variables': ['tmin', 'tmax', 'prcp', 'swe'],\n    'slope_threshold': 35.0  # Avalanche consideration\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#performance-optimization-guide","level":2,"title":"Performance Optimization Guide","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#memory-requirements","level":3,"title":"Memory Requirements","text":"<pre><code>def estimate_memory(dem_size_mb: float, params: dict) -&gt; float:\n    \"\"\"Estimate memory requirements in GB\"\"\"\n\n    base_memory = 2.0  # OS and runtime\n\n    # DEM memory (multiple copies for processing)\n    dem_memory = dem_size_mb * 4 / 1024  \n\n    # Thread memory (2GB per thread)\n    thread_memory = params.get('num_threads', 4) * 2\n\n    # Time series memory (for EEMT)\n    if params.get('start_year'):\n        years = params['end_year'] - params['start_year'] + 1\n        timeseries_memory = dem_size_mb * years * 365 / 1024\n    else:\n        timeseries_memory = 0\n\n    return base_memory + dem_memory + thread_memory + timeseries_memory\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#processing-time-estimates","level":3,"title":"Processing Time Estimates","text":"<pre><code>def estimate_runtime(dem_pixels: int, params: dict) -&gt; float:\n    \"\"\"Estimate runtime in hours\"\"\"\n\n    # Base rate: pixels per second per thread\n    if params.get('step', 15) &lt;= 5:\n        rate = 1000  # High resolution\n    elif params.get('step', 15) &lt;= 15:\n        rate = 5000  # Medium resolution\n    else:\n        rate = 10000  # Low resolution\n\n    threads = params.get('num_threads', 4)\n\n    # Solar workflow\n    days = 365\n    solar_time = (dem_pixels * days) / (rate * threads * 3600)\n\n    # EEMT additions\n    if params.get('start_year'):\n        years = params['end_year'] - params['start_year'] + 1\n        eemt_time = solar_time * years * 1.5  # Climate data overhead\n    else:\n        eemt_time = solar_time\n\n    return eemt_time\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#troubleshooting-parameters","level":2,"title":"Troubleshooting Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#common-parameter-issues","level":3,"title":"Common Parameter Issues","text":"Issue Cause Solution Out of memory Too many threads Reduce <code>num_threads</code> Slow processing Fine time step Increase <code>step</code> to 15-30 Poor results in shadows Coarse time step Decrease <code>step</code> to 3-5 Unrealistic radiation Wrong turbidity Adjust <code>linke_value</code> for conditions Missing diffuse radiation Wrong albedo Set appropriate <code>albedo_value</code> DAYMET download fails Invalid year range Check data availability Large output files No compression Enable <code>output_compression</code>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#parameter-debugging","level":3,"title":"Parameter Debugging","text":"<pre><code># Enable verbose logging\ndebug_params = {\n    'step': 15.0,\n    'num_threads': 1,  # Single thread for debugging\n    'verbose': True,\n    'debug': True,\n    'log_level': 'DEBUG'\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/parameters/#related-documentation","level":2,"title":"Related Documentation","text":"<ul> <li>Workflow Examples</li> <li>API Reference</li> <li>Scientific Background</li> <li>Web Interface Guide</li> </ul>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api/python-modules/","level":1,"title":"Python Modules API Reference","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#overview","level":2,"title":"Overview","text":"<p>The EEMT Python package provides programmatic access to all calculation functions, data utilities, and workflow components. This reference documents the public API for custom workflow development and integration.</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#core-modules","level":2,"title":"Core Modules","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemtcalculations","level":3,"title":"eemt.calculations","text":"<p>Core EEMT calculation functions.</p> <pre><code>from eemt import calculations\n\n# Traditional EEMT calculation\neemt_trad = calculations.calculate_eemt_traditional(\n    temperature=temp_array,\n    precipitation=precip_array,\n    elevation=dem_array\n)\n\n# Topographic EEMT calculation\neemt_topo = calculations.calculate_eemt_topographic(\n    temperature=temp_array,\n    precipitation=precip_array,\n    solar_radiation=solar_array,\n    twi=twi_array,\n    slope=slope_array\n)\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#functions","level":4,"title":"Functions","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_eemt_traditional","level":5,"title":"<code>calculate_eemt_traditional()</code>","text":"<p>Calculate EEMT using climate-based approach.</p> <p>Parameters: - <code>temperature</code> (ndarray): Monthly mean temperature [¬∞C] - <code>precipitation</code> (ndarray): Monthly precipitation [mm] - <code>elevation</code> (ndarray, optional): Elevation for corrections [m] - <code>lapse_rate</code> (float): Temperature lapse rate [¬∞C/km], default -6.5</p> <p>Returns: - <code>dict</code>: Contains 'eemt', 'e_bio', 'e_ppt' arrays [MJ m‚Åª¬≤ yr‚Åª¬π]</p> <p>Example: <pre><code>import numpy as np\nfrom eemt import calculations\n\n# Monthly climate data\ntemp = np.array([5, 8, 12, 16, 20, 24, 26, 25, 22, 17, 11, 6])\nprecip = np.array([45, 52, 68, 84, 95, 78, 65, 72, 85, 73, 58, 48])\n\n# Calculate EEMT\nresult = calculations.calculate_eemt_traditional(temp, precip)\nprint(f\"Annual EEMT: {result['eemt']:.2f} MJ/m¬≤/yr\")\nprint(f\"Biological Energy: {result['e_bio']:.2f} MJ/m¬≤/yr\")\nprint(f\"Precipitation Energy: {result['e_ppt']:.2f} MJ/m¬≤/yr\")\n</code></pre></p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_eemt_topographic","level":5,"title":"<code>calculate_eemt_topographic()</code>","text":"<p>Calculate EEMT with topographic corrections.</p> <p>Parameters: - <code>temperature</code> (ndarray): Temperature data [¬∞C] - <code>precipitation</code> (ndarray): Precipitation data [mm] - <code>solar_radiation</code> (ndarray): Annual solar radiation [MJ/m¬≤] - <code>twi</code> (ndarray): Topographic wetness index - <code>slope</code> (ndarray): Slope angle [degrees] - <code>aspect</code> (ndarray, optional): Aspect angle [degrees]</p> <p>Returns: - <code>dict</code>: Enhanced EEMT components with topographic effects</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemtsolar","level":3,"title":"eemt.solar","text":"<p>Solar radiation calculation utilities.</p> <pre><code>from eemt import solar\n\n# Calculate daily solar radiation\ndaily_solar = solar.calculate_daily_radiation(\n    dem=elevation_data,\n    day_of_year=180,\n    latitude=32.5,\n    step_minutes=15\n)\n\n# Annual solar radiation\nannual_solar = solar.calculate_annual_radiation(\n    dem=elevation_data,\n    latitude=32.5,\n    step_minutes=15,\n    num_threads=8\n)\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#functions_1","level":4,"title":"Functions","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_daily_radiation","level":5,"title":"<code>calculate_daily_radiation()</code>","text":"<p>Calculate solar radiation for a single day.</p> <p>Parameters: - <code>dem</code> (ndarray): Digital elevation model [m] - <code>day_of_year</code> (int): Julian day (1-365) - <code>latitude</code> (float): Site latitude [degrees] - <code>step_minutes</code> (int): Time step for calculation [minutes] - <code>linke_turbidity</code> (float): Atmospheric turbidity (1-8) - <code>albedo</code> (float): Surface albedo (0-1)</p> <p>Returns: - <code>ndarray</code>: Daily solar radiation [MJ/m¬≤/day]</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_annual_radiation","level":5,"title":"<code>calculate_annual_radiation()</code>","text":"<p>Calculate solar radiation for entire year.</p> <p>Parameters: - <code>dem</code> (ndarray): Digital elevation model [m] - <code>latitude</code> (float): Site latitude [degrees] - <code>step_minutes</code> (int): Time step [minutes] - <code>num_threads</code> (int): Parallel threads to use - <code>output_dir</code> (str, optional): Directory for intermediate files</p> <p>Returns: - <code>dict</code>: Contains daily and monthly radiation arrays</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemtclimate","level":3,"title":"eemt.climate","text":"<p>Climate data retrieval and processing.</p> <pre><code>from eemt import climate\n\n# Download DAYMET data\nclimate_data = climate.download_daymet(\n    bounds=(-111.0, 32.0, -110.5, 32.5),\n    years=range(2020, 2023),\n    variables=['tmin', 'tmax', 'prcp']\n)\n\n# Process climate data\nprocessed = climate.process_climate_data(\n    climate_data,\n    target_crs='EPSG:32612'\n)\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#functions_2","level":4,"title":"Functions","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#download_daymet","level":5,"title":"<code>download_daymet()</code>","text":"<p>Download DAYMET climate data.</p> <p>Parameters: - <code>bounds</code> (tuple): Bounding box (west, south, east, north) - <code>years</code> (list): Years to download - <code>variables</code> (list): Climate variables to retrieve - <code>output_dir</code> (str): Download directory</p> <p>Returns: - <code>xarray.Dataset</code>: Climate data</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#process_climate_data","level":5,"title":"<code>process_climate_data()</code>","text":"<p>Process and reproject climate data.</p> <p>Parameters: - <code>climate_data</code> (xarray.Dataset): Raw climate data - <code>target_crs</code> (str): Target coordinate system - <code>target_resolution</code> (float): Target resolution [m]</p> <p>Returns: - <code>xarray.Dataset</code>: Processed climate data</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemtio","level":3,"title":"eemt.io","text":"<p>Input/output utilities for various data formats.</p> <pre><code>from eemt import io\n\n# Read GeoTIFF\ndem = io.read_geotiff('elevation.tif')\n\n# Write results\nio.write_geotiff(\n    data=eemt_result,\n    filepath='eemt_output.tif',\n    crs=dem.crs,\n    transform=dem.transform\n)\n\n# Read NetCDF\nclimate = io.read_netcdf('daymet_2020.nc')\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#functions_3","level":4,"title":"Functions","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#read_geotiff","level":5,"title":"<code>read_geotiff()</code>","text":"<p>Read GeoTIFF file with metadata.</p> <p>Parameters: - <code>filepath</code> (str): Path to GeoTIFF file - <code>band</code> (int): Band number to read (default 1) - <code>as_dataset</code> (bool): Return xarray Dataset</p> <p>Returns: - <code>GeoTIFFData</code>: Object with data, CRS, and transform</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#write_geotiff","level":5,"title":"<code>write_geotiff()</code>","text":"<p>Write data to GeoTIFF format.</p> <p>Parameters: - <code>data</code> (ndarray): Data array to write - <code>filepath</code> (str): Output file path - <code>crs</code> (CRS): Coordinate reference system - <code>transform</code> (Affine): Affine transformation - <code>compress</code> (str): Compression method ('lzw', 'deflate')</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemtutils","level":3,"title":"eemt.utils","text":"<p>Utility functions for data processing.</p> <pre><code>from eemt import utils\n\n# Calculate topographic metrics\nslope, aspect = utils.calculate_slope_aspect(dem)\ntwi = utils.calculate_twi(dem, flow_accumulation)\n\n# Reproject data\nreprojected = utils.reproject_raster(\n    source_data,\n    source_crs='EPSG:4326',\n    target_crs='EPSG:32612'\n)\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#functions_4","level":4,"title":"Functions","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_slope_aspect","level":5,"title":"<code>calculate_slope_aspect()</code>","text":"<p>Calculate slope and aspect from DEM.</p> <p>Parameters: - <code>dem</code> (ndarray): Digital elevation model - <code>resolution</code> (float): Pixel resolution [m] - <code>algorithm</code> (str): 'horn' or 'zevenbergen'</p> <p>Returns: - <code>tuple</code>: (slope, aspect) arrays</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_twi","level":5,"title":"<code>calculate_twi()</code>","text":"<p>Calculate topographic wetness index.</p> <p>Parameters: - <code>dem</code> (ndarray): Digital elevation model - <code>resolution</code> (float): Pixel resolution [m]</p> <p>Returns: - <code>ndarray</code>: TWI values</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#workflow-classes","level":2,"title":"Workflow Classes","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemtworkflow","level":3,"title":"EEMTWorkflow","text":"<p>High-level workflow orchestration.</p> <pre><code>from eemt import EEMTWorkflow\n\n# Initialize workflow\nworkflow = EEMTWorkflow(\n    dem_file='input_dem.tif',\n    output_dir='./results',\n    config={\n        'start_year': 2020,\n        'end_year': 2022,\n        'step_minutes': 15,\n        'num_threads': 8\n    }\n)\n\n# Run complete workflow\nworkflow.run()\n\n# Or run individual steps\nworkflow.prepare_inputs()\nworkflow.calculate_solar()\nworkflow.download_climate()\nworkflow.calculate_eemt()\nworkflow.generate_outputs()\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#methods","level":4,"title":"Methods","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#__init__","level":5,"title":"<code>__init__()</code>","text":"<p>Initialize EEMT workflow.</p> <p>Parameters: - <code>dem_file</code> (str): Path to input DEM - <code>output_dir</code> (str): Output directory - <code>config</code> (dict): Workflow configuration</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#run","level":5,"title":"<code>run()</code>","text":"<p>Execute complete workflow.</p> <p>Parameters: - <code>skip_existing</code> (bool): Skip completed steps - <code>cleanup</code> (bool): Remove intermediate files</p> <p>Returns: - <code>dict</code>: Workflow results and metadata</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#calculate_solar","level":5,"title":"<code>calculate_solar()</code>","text":"<p>Run solar radiation calculations.</p> <p>Parameters: - <code>days</code> (list, optional): Specific days to calculate</p> <p>Returns: - <code>dict</code>: Solar radiation results</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#solarworkflow","level":3,"title":"SolarWorkflow","text":"<p>Solar radiation workflow management.</p> <pre><code>from eemt import SolarWorkflow\n\n# Configure solar workflow\nsolar_workflow = SolarWorkflow(\n    dem_file='dem.tif',\n    output_dir='./solar_output'\n)\n\n# Set parameters\nsolar_workflow.set_parameters(\n    step_minutes=15,\n    linke_turbidity=2.5,\n    albedo=0.2\n)\n\n# Run calculations\nresults = solar_workflow.run(num_threads=8)\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#data-classes","level":2,"title":"Data Classes","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#geotiffdata","level":3,"title":"GeoTIFFData","text":"<p>Container for GeoTIFF data and metadata.</p> <pre><code>from eemt.io import GeoTIFFData\n\n# Create from arrays\ngeotiff = GeoTIFFData(\n    data=data_array,\n    crs='EPSG:32612',\n    transform=affine_transform,\n    bounds=(-111.0, 32.0, -110.5, 32.5)\n)\n\n# Access properties\nprint(f\"Shape: {geotiff.shape}\")\nprint(f\"Resolution: {geotiff.resolution}\")\nprint(f\"CRS: {geotiff.crs}\")\n\n# Export to file\ngeotiff.to_file('output.tif', compress='lzw')\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#climatedata","level":3,"title":"ClimateData","text":"<p>Climate data container with utilities.</p> <pre><code>from eemt.climate import ClimateData\n\n# Load climate data\nclimate = ClimateData.from_daymet(\n    bounds=bbox,\n    years=[2020, 2021, 2022]\n)\n\n# Access variables\ntemperature = climate.temperature\nprecipitation = climate.precipitation\n\n# Calculate derived metrics\npet = climate.calculate_pet()\nvpd = climate.calculate_vpd()\n\n# Resample to match DEM\nclimate_resampled = climate.resample_to(dem_grid)\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#exceptions","level":2,"title":"Exceptions","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#eemterror","level":3,"title":"EEMTError","text":"<p>Base exception for EEMT errors.</p> <pre><code>from eemt.exceptions import (\n    EEMTError,\n    InvalidParameterError,\n    DataNotFoundError,\n    WorkflowError\n)\n\ntry:\n    workflow.run()\nexcept InvalidParameterError as e:\n    print(f\"Invalid parameter: {e.parameter} = {e.value}\")\nexcept DataNotFoundError as e:\n    print(f\"Missing data: {e.data_type}\")\nexcept WorkflowError as e:\n    print(f\"Workflow failed at step: {e.step}\")\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#configuration","level":2,"title":"Configuration","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#default-configuration","level":3,"title":"Default Configuration","text":"<pre><code>from eemt import config\n\n# Access default configuration\ndefaults = config.get_defaults()\nprint(defaults['solar']['step_minutes'])  # 15\nprint(defaults['solar']['linke_turbidity'])  # 2.0\n\n# Override defaults\nconfig.set_default('solar.step_minutes', 10)\n\n# Load from file\nconfig.load_config('eemt_config.yaml')\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#configuration-schema","level":3,"title":"Configuration Schema","text":"<pre><code># eemt_config.yaml\nsolar:\n  step_minutes: 15\n  linke_turbidity: 2.0\n  albedo: 0.2\n\nclimate:\n  source: \"daymet\"\n  variables:\n    - tmin\n    - tmax\n    - prcp\n\neemt:\n  methods:\n    - traditional\n    - topographic\n\nperformance:\n  num_threads: 8\n  chunk_size: 1000\n  use_gpu: false\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#logging","level":2,"title":"Logging","text":"<pre><code>import logging\nfrom eemt import setup_logging\n\n# Configure logging\nsetup_logging(\n    level=logging.INFO,\n    log_file='eemt.log',\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Use logger in modules\nlogger = logging.getLogger('eemt.solar')\nlogger.info('Starting solar calculations')\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#examples","level":2,"title":"Examples","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#complete-analysis-pipeline","level":3,"title":"Complete Analysis Pipeline","text":"<pre><code>import numpy as np\nfrom eemt import (\n    EEMTWorkflow, \n    io, \n    utils,\n    visualize\n)\n\n# Load input data\ndem = io.read_geotiff('study_area_dem.tif')\n\n# Calculate topographic metrics\nslope, aspect = utils.calculate_slope_aspect(dem.data)\ntwi = utils.calculate_twi(dem.data)\n\n# Initialize workflow\nworkflow = EEMTWorkflow(\n    dem_file='study_area_dem.tif',\n    output_dir='./analysis_results',\n    config={\n        'start_year': 2015,\n        'end_year': 2020,\n        'step_minutes': 15,\n        'num_threads': 16\n    }\n)\n\n# Run workflow with progress callback\ndef progress_callback(step, percent):\n    print(f\"{step}: {percent}% complete\")\n\nresults = workflow.run(progress_callback=progress_callback)\n\n# Visualize results\nfig = visualize.plot_eemt_maps(\n    results['eemt_traditional'],\n    results['eemt_topographic'],\n    title='EEMT Comparison'\n)\nfig.savefig('eemt_comparison.png', dpi=300)\n\n# Generate statistics\nstats = utils.calculate_statistics(\n    results['eemt_topographic'],\n    zones=vegetation_map\n)\nprint(stats.to_dataframe())\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#custom-workflow","level":3,"title":"Custom Workflow","text":"<pre><code>from eemt import calculations, climate, solar\nimport concurrent.futures\n\ndef custom_eemt_analysis(dem_file, climate_bounds, years):\n    \"\"\"Custom EEMT workflow with parallel processing.\"\"\"\n\n    # Load DEM\n    dem = io.read_geotiff(dem_file)\n\n    # Parallel solar calculation\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        # Submit solar calculations for each year\n        solar_futures = {\n            year: executor.submit(\n                solar.calculate_annual_radiation,\n                dem.data,\n                latitude=climate_bounds[1],\n                step_minutes=15\n            )\n            for year in years\n        }\n\n        # Collect results\n        solar_results = {\n            year: future.result()\n            for year, future in solar_futures.items()\n        }\n\n    # Download climate data\n    climate_data = climate.download_daymet(\n        bounds=climate_bounds,\n        years=years,\n        variables=['tmin', 'tmax', 'prcp', 'vp']\n    )\n\n    # Calculate EEMT for each year\n    eemt_results = {}\n    for year in years:\n        eemt_results[year] = calculations.calculate_eemt_topographic(\n            temperature=climate_data['tmax'].sel(time=str(year)),\n            precipitation=climate_data['prcp'].sel(time=str(year)),\n            solar_radiation=solar_results[year]['annual'],\n            twi=utils.calculate_twi(dem.data)\n        )\n\n    return eemt_results\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#gpu-acceleration","level":3,"title":"GPU Acceleration","text":"<pre><code>from eemt import gpu\n\n# Check GPU availability\nif gpu.is_available():\n    print(f\"GPU: {gpu.get_device_name()}\")\n\n    # Enable GPU acceleration\n    workflow = EEMTWorkflow(\n        dem_file='dem.tif',\n        config={'use_gpu': True}\n    )\n</code></pre>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/python-modules/#dask-integration","level":3,"title":"Dask Integration","text":"<pre><code>import dask.array as da\nfrom eemt import calculations\n\n# Create Dask arrays\ndem_chunked = da.from_array(dem_data, chunks=(1000, 1000))\ntemp_chunked = da.from_array(temp_data, chunks=(100, 1000, 1000))\n\n# Parallel computation\neemt_lazy = calculations.calculate_eemt_traditional(\n    temperature=temp_chunked,\n    precipitation=precip_chunked,\n    elevation=dem_chunked\n)\n\n# Compute with progress bar\nfrom dask.diagnostics import ProgressBar\nwith ProgressBar():\n    eemt_result = eemt_lazy.compute()\n</code></pre> <p>For more examples, see the Examples Documentation. For installation, see the Installation Guide.</p>","path":["API Documentation","Python Modules API"],"tags":[]},{"location":"api/web-interface/","level":1,"title":"Web Interface","text":"<p>The EEMT Web Interface provides a modern, user-friendly way to submit and monitor EEMT and solar radiation workflows through a browser-based application with full Docker integration.</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#overview","level":2,"title":"Overview","text":"<p>The web interface is built with FastAPI and provides:</p> <ul> <li>üåê Browser-based Job Submission: Upload DEM files and configure parameters</li> <li>üìä Real-time Monitoring: Track progress with live updates</li> <li>üê≥ Containerized Execution: Full Docker integration with workflow containers</li> <li>üíæ Results Management: Download processed data as ZIP archives</li> <li>üèóÔ∏è Scalable Deployment: Single-node or distributed multi-container architecture</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#docker-deployment-recommended","level":2,"title":"Docker Deployment (Recommended)","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#quick-start-with-docker-compose","level":3,"title":"Quick Start with Docker Compose","text":"<p>The easiest way to deploy EEMT is using Docker Compose, which handles all dependencies and container orchestration:</p> <pre><code># Clone repository\ngit clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n\n# Start local mode (web interface + single worker)\ndocker-compose up\n\n# Access web interface at http://localhost:5000\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#deployment-modes","level":3,"title":"Deployment Modes","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#1-local-mode-default","level":4,"title":"1. Local Mode (Default)","text":"<p>Single-container deployment for development and small jobs:</p> <pre><code># Start local web interface\ndocker-compose up eemt-web\n\n# Or build and run manually\ndocker build -t eemt-web -f docker/web-interface/Dockerfile .\ndocker run -p 5000:5000 -v $(pwd)/data:/app/data eemt-web\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#2-distributed-mode","level":4,"title":"2. Distributed Mode","text":"<p>Multi-container deployment with master and worker nodes:</p> <pre><code># Start distributed cluster\ndocker-compose --profile distributed up\n\n# Scale workers\ndocker-compose --profile distributed --profile scale up --scale eemt-worker-2=2 --scale eemt-worker-3=3\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#3-documentation-server","level":4,"title":"3. Documentation Server","text":"<pre><code># Start documentation alongside web interface\ndocker-compose --profile docs up eemt-docs\n\n# Access docs at http://localhost:8000\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#docker-environment-variables","level":3,"title":"Docker Environment Variables","text":"<p>Configure deployment through environment variables:</p> <pre><code># Local mode configuration\nEEMT_MODE=local\nEEMT_HOST=0.0.0.0\nEEMT_PORT=5000\n\n# Distributed mode configuration\nEEMT_MODE=master\nWORK_QUEUE_PORT=9123\nWORK_QUEUE_PROJECT=EEMT-Production\nMAX_WORKERS=50\n\n# Worker configuration\nMASTER_HOST=eemt-master\nMASTER_PORT=9123\nWORKER_CORES=8\nWORKER_MEMORY=16G\nWORKER_DISK=100G\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#manual-installation","level":2,"title":"Manual Installation","text":"<p>For development or custom deployments:</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#1-build-base-container","level":3,"title":"1. Build Base Container","text":"<pre><code># Build EEMT base container with all dependencies\ncd docker/ubuntu/24.04/\n./build.sh\n\n# Verify image\ndocker images | grep eemt\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#2-build-web-interface-container","level":3,"title":"2. Build Web Interface Container","text":"<pre><code># Build web interface container\ndocker build -t eemt-web -f docker/web-interface/Dockerfile .\n\n# Or use pre-built image (if available)\ndocker pull eemt/web-interface:latest\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#3-run-web-interface","level":3,"title":"3. Run Web Interface","text":"<pre><code># Create data directories\nmkdir -p data/{uploads,results,temp,cache,shared}\n\n# Run web interface container\ndocker run -d \\\n  --name eemt-web \\\n  -p 5000:5000 \\\n  -v $(pwd)/data:/app/data \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  eemt-web\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#4-access-interface","level":3,"title":"4. Access Interface","text":"<ul> <li>Web Interface: http://localhost:5000</li> <li>Job Monitor: http://localhost:5000/monitor</li> <li>API Documentation: http://localhost:5000/docs</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#features","level":2,"title":"Features","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#job-submission-interface","level":3,"title":"Job Submission Interface","text":"<ul> <li>Workflow Selection: Choose between Solar Radiation and Full EEMT workflows</li> <li>DEM Upload: Drag-and-drop or select GeoTIFF files</li> <li>Parameter Configuration: <ul> <li>Time step (3-15 minutes)</li> <li>Atmospheric turbidity (Linke value)</li> <li>Surface albedo</li> <li>CPU threads</li> <li>Climate data range (EEMT only)</li> </ul> </li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#real-time-monitoring","level":3,"title":"Real-time Monitoring","text":"<ul> <li>Summary Dashboard: Overview of pending, running, completed, and failed jobs</li> <li>Job Table: Detailed status with progress bars</li> <li>Auto-refresh: Updates every 5 seconds</li> <li>Job Details: Click for detailed information and logs</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#system-status","level":3,"title":"System Status","text":"<p>The interface automatically checks:</p> <ul> <li>‚úÖ Docker daemon availability</li> <li>‚úÖ Container image presence</li> <li>‚úÖ Resource availability</li> <li>‚ö†Ô∏è Setup warnings and instructions</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#workflow-types","level":2,"title":"Workflow Types","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#solar-radiation-modeling","level":3,"title":"Solar Radiation Modeling","text":"<p>Purpose: Calculate daily solar irradiation for topographic analysis</p> <p>Process: 1. Processes DEM through GRASS GIS r.sun 2. Calculates 365 daily solar radiation maps 3. Generates monthly aggregated products 4. Outputs global and direct solar radiation</p> <p>Typical Runtime: 5-30 minutes depending on DEM resolution</p> <p>Outputs: - <code>global/daily/total_sun_day_*.tif</code> - Daily solar radiation (365 files) - <code>global/monthly/total_sun_*_sum.tif</code> - Monthly aggregates (12 files) - <code>insol/daily/hours_sun_day_*.tif</code> - Daily sunshine hours (365 files)</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#full-eemt-analysis","level":3,"title":"Full EEMT Analysis","text":"<p>Purpose: Complete energy-mass transfer calculation with climate integration</p> <p>Process: 1. Performs solar radiation calculations 2. Downloads DAYMET climate data 3. Calculates topographic indices (slope, aspect, TWI) 4. Computes EEMT values combining solar and climate data 5. Generates multi-year energy transfer maps</p> <p>Typical Runtime: 1-4 hours depending on time period and resolution</p> <p>Outputs: - All solar radiation products (above) - <code>eemt/EEMT_Topo_*_*.tif</code> - Topographic EEMT values - <code>eemt/EEMT_Trad_*_*.tif</code> - Traditional EEMT values - <code>daymet/</code> - Downloaded climate data</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#container-architecture","level":2,"title":"Container Architecture","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#execution-flow","level":3,"title":"Execution Flow","text":"<pre><code>graph TB\n    subgraph \"Docker Host\"\n        subgraph \"Web Interface Container\"\n            WI[FastAPI Web App]\n            WM[Workflow Manager]\n            DB[SQLite Database]\n        end\n\n        subgraph \"Workflow Containers\"\n            WC1[EEMT Worker 1]\n            WC2[EEMT Worker 2]\n            WC3[EEMT Worker N]\n        end\n\n        subgraph \"Data Volumes\"\n            UP[Uploads Volume]\n            RES[Results Volume]\n            TMP[Temp Volume]\n            CACHE[Cache Volume]\n        end\n    end\n\n    subgraph \"External\"\n        USER[User Browser]\n        DOCKER[Docker Daemon]\n    end\n\n    USER --&gt; WI\n    WI --&gt; WM\n    WM --&gt; DOCKER\n    DOCKER --&gt; WC1\n    DOCKER --&gt; WC2\n    DOCKER --&gt; WC3\n\n    WC1 --&gt; UP\n    WC1 --&gt; RES\n    WC2 --&gt; TMP\n    WC3 --&gt; CACHE</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#container-images","level":3,"title":"Container Images","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#base-container-eemtubuntu2404","level":4,"title":"Base Container (<code>eemt:ubuntu24.04</code>)","text":"<p>Contains all scientific computing dependencies: - GRASS GIS 8.4+: With r.sun extensions for solar modeling - CCTools 7.8.2: Makeflow + Work Queue for distributed processing - Python 3.12: Complete geospatial environment with scientific libraries - GDAL 3.11: Modern geospatial data access and format support - Workflow Scripts: Container entry points and scientific computing utilities</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#web-interface-container-eemt-web","level":4,"title":"Web Interface Container (<code>eemt-web</code>)","text":"<p>Extends base container with web application: - FastAPI Application: Web interface and REST API - Workflow Manager: Docker orchestration and job management - Monitoring Tools: Real-time progress tracking and log aggregation - Docker Integration: Direct container management capabilities</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#volume-management","level":3,"title":"Volume Management","text":"<p>Docker Compose automatically creates and manages data volumes:</p> <pre><code># Volume mounts in docker-compose.yml\nvolumes:\n  - ./data/uploads:/app/uploads      # DEM file uploads\n  - ./data/results:/app/results      # Workflow outputs\n  - ./data/temp:/app/temp            # Temporary processing data\n  - ./data/cache:/app/cache          # Workflow caching\n  - ./data/shared:/app/shared        # Shared data (distributed mode)\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#container-networking","level":3,"title":"Container Networking","text":"<pre><code># Docker network configuration\nnetworks:\n  eemt-network:\n    driver: bridge\n</code></pre> <p>This allows containers to communicate using service names (e.g., <code>eemt-master</code>, <code>eemt-worker</code>).</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#rest-api","level":2,"title":"REST API","text":"<p>The web interface exposes a REST API for programmatic access:</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#submit-job","level":3,"title":"Submit Job","text":"<pre><code>POST /api/submit-job\nContent-Type: multipart/form-data\n\nParameters:\n- workflow_type: \"sol\" or \"eemt\"\n- dem_file: GeoTIFF file upload\n- step: float (time step in minutes)\n- linke_value: float (atmospheric turbidity)\n- albedo_value: float (surface reflectance)  \n- num_threads: int (CPU threads)\n- start_year: int (EEMT only)\n- end_year: int (EEMT only)\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#monitor-jobs","level":3,"title":"Monitor Jobs","text":"<pre><code># List all jobs\nGET /api/jobs\n\n# Get specific job details\nGET /api/jobs/{job_id}\n\n# Download results\nGET /api/jobs/{job_id}/results\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#system-status_1","level":3,"title":"System Status","text":"<pre><code>GET /api/system/status\n</code></pre> <p>Returns: <pre><code>{\n  \"docker_available\": true,\n  \"container_stats\": {\n    \"total_containers\": 2,\n    \"running_jobs\": [\"job-123\"],\n    \"system_stats\": {\n      \"cpus\": 8,\n      \"memory\": 16777216000\n    }\n  },\n  \"image_name\": \"eemt:ubuntu24.04\"\n}\n</code></pre></p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#configuration","level":2,"title":"Configuration","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#environment-variables","level":3,"title":"Environment Variables","text":"<pre><code># Host and port configuration\nEEMT_HOST=\"127.0.0.1\"        # Bind address\nEEMT_PORT=\"5000\"             # Service port\n\n# Directory configuration\nEEMT_UPLOAD_DIR=\"./uploads\"  # DEM upload directory\nEEMT_RESULTS_DIR=\"./results\" # Job output directory\nEEMT_TEMP_DIR=\"./temp\"       # Temporary processing\nEEMT_CACHE_DIR=\"./cache\"     # Workflow cache\n\n# Container configuration  \nDOCKER_IMAGE=\"eemt:ubuntu24.04\"  # Container image name\nCONTAINER_CPU_LIMIT=\"4\"          # Default CPU limit\nCONTAINER_MEMORY_LIMIT=\"8G\"      # Default memory limit\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#database-configuration","level":3,"title":"Database Configuration","text":"<p>The interface uses SQLite for job tracking:</p> <ul> <li>Location: <code>./jobs.db</code> (auto-created)</li> <li>Schema: Automatically initialized on first run</li> <li>Backup: Standard SQLite tools (<code>sqlite3 .backup</code>)</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#common-issues","level":3,"title":"Common Issues","text":"<ol> <li> <p>\"Docker not available\" <pre><code># Check Docker daemon\ndocker info\n\n# Ensure Docker service is running\nsudo systemctl start docker\n</code></pre></p> </li> <li> <p>\"Container image not found\" <pre><code># Build the container\ncd docker/ubuntu/24.04/\n./build.sh\n\n# Verify image exists\ndocker images | grep eemt\n</code></pre></p> </li> <li> <p>\"Job execution failed\"</p> </li> <li>Check job details in monitor for container logs</li> <li>Verify DEM is valid GeoTIFF with proper projection</li> <li> <p>Ensure adequate disk space and memory</p> </li> <li> <p>\"Web interface not accessible\" <pre><code># Check if port is available\nnetstat -an | grep 5000\n\n# Try alternative port\nuvicorn app:app --host 127.0.0.1 --port 8080\n</code></pre></p> </li> </ol>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#performance-optimization","level":3,"title":"Performance Optimization","text":"<ul> <li>Large DEMs: Consider processing smaller tiles</li> <li>Concurrent Jobs: Limit based on available CPU/memory</li> <li>Container Resources: Adjust limits in <code>workflow_manager.py</code></li> <li>Disk Space: Monitor usage in uploads/, results/, temp/</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#logging","level":3,"title":"Logging","text":"<p>Application logs are available:</p> <pre><code># Start with debug logging\nuvicorn app:app --log-level debug\n\n# Container execution logs  \ndocker logs &lt;container_id&gt;\n\n# Job-specific logs in web interface monitor\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#development","level":2,"title":"Development","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#api-development","level":3,"title":"API Development","text":"<p>The FastAPI application supports:</p> <ul> <li>Auto-documentation: Available at <code>/docs</code></li> <li>Interactive testing: Try API endpoints in browser</li> <li>OpenAPI schema: Available at <code>/openapi.json</code></li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#frontend-development","level":3,"title":"Frontend Development","text":"<p>The HTML interface uses:</p> <ul> <li>Bootstrap 5: Responsive CSS framework</li> <li>Vanilla JavaScript: No heavy frontend dependencies</li> <li>WebSocket: For real-time progress updates (planned)</li> </ul>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#container-development","level":3,"title":"Container Development","text":"<p>To modify container workflows:</p> <ol> <li>Edit scripts in <code>docker/ubuntu/24.04/container-scripts/</code></li> <li>Rebuild container: <code>./build.sh</code></li> <li>Test with web interface</li> </ol>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#integration","level":2,"title":"Integration","text":"","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#distributed-mode","level":3,"title":"Distributed Mode","text":"<p>The web interface can be integrated with distributed computing environments:</p> <ul> <li>Multi-host execution: Deploy master node with web interface, connect remote workers</li> <li>Load balancing: Work Queue automatically distributes tasks across available workers</li> <li>Fault tolerance: Automatic retry and worker replacement mechanisms</li> <li>HPC Integration: Connect to SLURM, PBS, LSF batch schedulers</li> </ul> <p>See Distributed Deployment for complete setup guide.</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#master-node-configuration","level":3,"title":"Master Node Configuration","text":"<p>To run the web interface as a master node:</p> <pre><code>from containers.workflow_manager import DistributedWorkflowManager, NodeType, MasterConfig\n\n# Configure master with web interface\nmaster_config = MasterConfig(\n    port=9123,\n    max_workers=100,\n    work_queue_project=\"EEMT-Cluster\"\n)\n\n# Initialize master workflow manager  \nmaster = DistributedWorkflowManager(\n    base_dir=Path(\"/data/eemt-master\"),\n    node_type=NodeType.MASTER,\n    master_config=master_config\n)\n\n# Start master node\nmaster.start_master_node()\n</code></pre> <p>Workers can then connect from remote machines:</p> <pre><code># Start worker on remote machine\npython scripts/start-worker.py \\\n    --master-host your-master-ip \\\n    --master-port 9123 \\\n    --cores 8 --memory 16G\n</code></pre>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api/web-interface/#cloud-deployment","level":3,"title":"Cloud Deployment","text":"<p>Cloud integration examples:</p> <ul> <li>Kubernetes: Container orchestration with persistent volumes</li> <li>AWS/GCP/Azure: VM clusters with shared storage</li> <li>Singularity: HPC container deployment</li> </ul> <p>See PLAN.md for complete modernization roadmap.</p>","path":["API Documentation","Web Interface"],"tags":[]},{"location":"api-reference/","level":1,"title":"EEMT API Reference","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#overview","level":2,"title":"Overview","text":"<p>This reference provides detailed documentation for all EEMT calculation functions, GRASS GIS commands, and configuration parameters.</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#core-eemt-functions","level":2,"title":"Core EEMT Functions","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#traditional-eemt-calculation","level":3,"title":"Traditional EEMT Calculation","text":"<pre><code>def calculate_eemt_traditional(temperature, precipitation, elevation=None):\n    \"\"\"\n    Calculate EEMT using traditional climate-based approach\n\n    Parameters:\n    -----------\n    temperature : array_like\n        Monthly mean temperature [¬∞C]\n    precipitation : array_like  \n        Monthly precipitation [mm]\n    elevation : array_like, optional\n        Elevation for lapse rate corrections [m]\n\n    Returns:\n    --------\n    eemt : array_like\n        Effective Energy and Mass Transfer [MJ m‚Åª¬≤ yr‚Åª¬π]\n    e_bio : array_like\n        Biological energy component [MJ m‚Åª¬≤ yr‚Åª¬π]\n    e_ppt : array_like\n        Precipitation energy component [MJ m‚Åª¬≤ yr‚Åª¬π]\n\n    Notes:\n    ------\n    Based on Rasmussen et al. (2005, 2014) methodology.\n    Uses Lieth (1975) NPP equation and Hamon PET estimation.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; temp = np.array([5, 10, 15, 20, 18, 12, 8])  # Monthly temps\n    &gt;&gt;&gt; precip = np.array([50, 60, 80, 40, 30, 45, 55])  # Monthly precip\n    &gt;&gt;&gt; eemt, e_bio, e_ppt = calculate_eemt_traditional(temp, precip)\n    &gt;&gt;&gt; print(f\"Annual EEMT: {eemt:.1f} MJ/m¬≤/yr\")\n    \"\"\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#topographic-eemt-calculation","level":3,"title":"Topographic EEMT Calculation","text":"<pre><code>def calculate_eemt_topographic(dem_file, climate_data, solar_data, \n                             output_dir, mcwi_method='d_infinity'):\n    \"\"\"\n    Calculate EEMT with topographic controls on energy and water balance\n\n    Parameters:\n    -----------\n    dem_file : str or Path\n        Path to digital elevation model (GeoTIFF)\n    climate_data : dict\n        Dictionary containing climate arrays:\n        - 'temperature': Monthly temperature [¬∞C] \n        - 'precipitation': Monthly precipitation [mm]\n        - 'humidity': Relative humidity [%]\n        - 'wind_speed': Wind speed [m/s]\n    solar_data : dict\n        Solar radiation data from r.sun calculations:\n        - 'global_radiation': Monthly solar [Wh/m¬≤]\n        - 'diffuse_radiation': Diffuse component [Wh/m¬≤]\n        - 'direct_radiation': Direct beam component [Wh/m¬≤]\n    output_dir : str or Path\n        Output directory for intermediate files\n    mcwi_method : str, default 'd_infinity'\n        Flow routing method: 'd_infinity', 'mfd', 'sfd'\n\n    Returns:\n    --------\n    eemt_result : dict\n        Results dictionary containing:\n        - 'eemt': Total EEMT [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'e_bio': Biological component [MJ m‚Åª¬≤ yr‚Åª¬π] \n        - 'e_ppt': Precipitation component [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'mcwi': Mass Conservative Wetness Index\n        - 'solar_ratio': Topographic solar modification factor\n\n    Notes:\n    ------\n    Implements Rasmussen et al. (2014) EEMT_TOPO methodology.\n    Requires GRASS GIS for terrain analysis and solar calculations.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; climate = load_daymet_data('study_area.shp', 2015, 2020)\n    &gt;&gt;&gt; solar = calculate_annual_solar('dem.tif', threads=8)\n    &gt;&gt;&gt; result = calculate_eemt_topographic('dem.tif', climate, solar, 'output/')\n    &gt;&gt;&gt; print(f\"Mean topographic EEMT: {np.mean(result['eemt']):.1f} MJ/m¬≤/yr\")\n    \"\"\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#vegetation-eemt-calculation","level":3,"title":"Vegetation EEMT Calculation","text":"<pre><code>def calculate_eemt_vegetation(dem_file, climate_data, vegetation_data,\n                           output_dir, lai_method='ndvi', resistance_model='kelliher'):\n    \"\"\"\n    Calculate EEMT with full vegetation and topographic integration\n\n    Parameters:\n    -----------\n    dem_file : str or Path\n        Digital elevation model file path\n    climate_data : dict\n        Complete climate dataset with:\n        - 'temperature': Temperature arrays [¬∞C]\n        - 'precipitation': Precipitation arrays [mm] \n        - 'humidity': Relative humidity [%]\n        - 'wind_speed': Wind speed [m/s]\n        - 'net_radiation': Net radiation [W/m¬≤]\n    vegetation_data : dict\n        Vegetation structure data:\n        - 'lai': Leaf Area Index [-] \n        - 'canopy_height': Canopy height [m]\n        - 'ndvi': Normalized Difference Vegetation Index [-]\n        - 'biomass': Aboveground biomass [Mg/ha] (optional)\n    output_dir : str or Path\n        Output directory\n    lai_method : str, default 'ndvi'\n        LAI calculation method: 'ndvi', 'modis', 'direct'\n    resistance_model : str, default 'kelliher' \n        Surface resistance model: 'kelliher', 'jarvis', 'stewart'\n\n    Returns:\n    --------\n    eemt_result : dict\n        Complete EEMT results:\n        - 'eemt': Total EEMT [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'e_bio': Biological energy [MJ m‚Åª¬≤ yr‚Åª¬π]\n        - 'e_ppt': Precipitation energy [MJ m‚Åª¬≤ yr‚Åª¬π] \n        - 'aet': Actual evapotranspiration [mm/yr]\n        - 'npp': Net primary production [kg/m¬≤/yr]\n        - 'surface_resistance': Surface resistance [s/m]\n        - 'lai_effective': Effective LAI used in calculations\n\n    Notes:\n    ------\n    Implements Rasmussen et al. (2014) EEMT_TOPO-VEG methodology.\n    Uses Penman-Monteith equation with vegetation-specific parameters.\n    Accounts for canopy structure effects on energy and water balance.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; # Load vegetation data from satellite\n    &gt;&gt;&gt; vegetation = {\n    ...     'ndvi': load_landsat_ndvi('study_area.shp', 2020),\n    ...     'canopy_height': load_lidar_canopy('lidar_data.las')\n    ... }\n    &gt;&gt;&gt; result = calculate_eemt_vegetation('dem.tif', climate, vegetation, 'output/')\n    &gt;&gt;&gt; print(f\"Vegetation EEMT: {np.mean(result['eemt']):.1f} MJ/m¬≤/yr\")\n    \"\"\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#grass-gis-command-reference","level":2,"title":"GRASS GIS Command Reference","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#solar-radiation-rsun-family","level":3,"title":"Solar Radiation (r.sun family)","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#rsun-multi-processor-version","level":4,"title":"r.sun (Multi-processor version)","text":"<pre><code>r.sun elevation=dem aspect=aspect slope=slope \\\\\n         day=180 step=0.25 \\\\\n         linke_value=3.0 albedo_value=0.2 \\\\\n         threads=8 \\\\\n         glob_rad=global_radiation \\\\\n         insol_time=sunshine_hours \\\\\n         [beam_rad=beam_radiation] \\\\\n         [diff_rad=diffuse_radiation] \\\\\n         [refl_rad=reflected_radiation]\n</code></pre> <p>Parameters: - <code>elevation=name</code> - Input elevation raster - <code>aspect=name</code> - Aspect in degrees (0-360¬∞) - <code>slope=name</code> - Slope in degrees (0-90¬∞) - <code>day=integer</code> - Day of year (1-365) - <code>step=float</code> - Time step in hours (0.25-1.0) - <code>linke_value=float</code> - Linke atmospheric turbidity (1.0-8.0) - <code>albedo_value=float</code> - Ground albedo (0.0-1.0) - <code>threads=integer</code> - Number of OpenMP threads - <code>glob_rad=name</code> - Output global radiation [Wh/m¬≤] - <code>insol_time=name</code> - Output sunshine duration [hours]</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#rsun-single-processor-version","level":4,"title":"r.sun (Single-processor version)","text":"<pre><code>r.sun elevation=dem \\\\\n      [aspect=aspect] [slope=slope] \\\\\n      [lat=latitude] [lon=longitude] \\\\\n      [day=day_of_year] [time=decimal_time] \\\\\n      [step=time_step] \\\\\n      [glob_rad=output] [beam_rad=output] \\\\\n      [diff_rad=output] [refl_rad=output] \\\\\n      [insol_time=output]\n</code></pre> <p>Advanced Options: - <code>horizon_basename=basename</code> - Horizon angle rasters - <code>horizon_step=angle</code> - Horizon calculation step [degrees] - <code>civil_time=hour</code> - Local solar time - <code>solar_constant=value</code> - Solar constant [W/m¬≤] - <code>distance_step=value</code> - Sampling distance [m]</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#terrain-analysis","level":3,"title":"Terrain Analysis","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#rslopeaspect","level":4,"title":"r.slope.aspect","text":"<pre><code>r.slope.aspect elevation=dem \\\\\n               slope=slope_output \\\\\n               aspect=aspect_output \\\\\n               [format=degrees|percent] \\\\\n               [precision=FCELL|DCELL] \\\\\n               [zscale=factor] \\\\\n               [min_slope=degrees]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#rterraflow-flow-accumulation","level":4,"title":"r.terraflow (Flow accumulation)","text":"<pre><code>r.terraflow elevation=dem \\\\\n            filled=filled_dem \\\\\n            direction=flow_direction \\\\\n            swatershed=watersheds \\\\\n            accumulation=flow_accumulation \\\\\n            tci=topographic_convergence_index\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#rwatershed-alternative-flow-routing","level":4,"title":"r.watershed (Alternative flow routing)","text":"<pre><code>r.watershed elevation=dem \\\\\n            accumulation=flow_accum \\\\\n            drainage=flow_direction \\\\\n            basin=watersheds \\\\\n            stream=stream_network \\\\\n            [threshold=threshold_value] \\\\\n            [-s] [-4] [-a]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#data-importexport","level":3,"title":"Data Import/Export","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#ringdal-import-raster-data","level":4,"title":"r.in.gdal (Import raster data)","text":"<pre><code>r.in.gdal input=input_file.tif \\\\\n          output=grass_raster \\\\\n          [band=band_number] \\\\\n          [memory=memory_mb] \\\\\n          [target=target_crs] \\\\\n          [-o] [-e] [-f]\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#routgdal-export-raster-data","level":4,"title":"r.out.gdal (Export raster data)","text":"<pre><code>r.out.gdal input=grass_raster \\\\\n           output=output_file.tif \\\\\n           format=GTiff \\\\\n           [type=data_type] \\\\\n           [nodata=nodata_value] \\\\\n           createopt=\"COMPRESS=LZW,TILED=YES\"\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#configuration-parameters","level":2,"title":"Configuration Parameters","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#solar-radiation-parameters","level":3,"title":"Solar Radiation Parameters","text":"Parameter Range Default Description <code>day</code> 1-365 - Day of year for calculation <code>step</code> 0.1-2.0 0.25 Time step interval [hours] <code>linke_value</code> 1.0-8.0 3.0 Atmospheric turbidity factor <code>albedo_value</code> 0.0-1.0 0.2 Surface albedo coefficient <code>lat</code> -90 to 90 auto Latitude [decimal degrees] <code>solar_constant</code> 1300-1400 1367 Solar constant [W/m¬≤]","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#processing-parameters","level":3,"title":"Processing Parameters","text":"Parameter Range Default Description <code>threads</code> 1-64 auto OpenMP thread count <code>memory</code> 256-8192 2048 Memory cache [MB] <code>precision</code> FCELL/DCELL FCELL Output precision <code>compress</code> LZW/DEFLATE LZW Output compression","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#climate-thresholds","level":3,"title":"Climate Thresholds","text":"Parameter Value Units Description <code>T_ref</code> 273.15 K Reference temperature (freezing) <code>h_BIO</code> 22√ó10‚Å∂ J/kg Specific biomass enthalpy <code>c_w</code> 4.18√ó10¬≥ J/kg/K Specific heat of water <code>EEMT_threshold</code> 70 MJ/m¬≤/yr Carbon/water dominance transition","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#error-handling","level":2,"title":"Error Handling","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#common-error-codes","level":3,"title":"Common Error Codes","text":"Error Cause Solution <code>GRASS: Location not found</code> Invalid GRASS database Check <code>GISDBASE</code> path <code>GDAL: Cannot open file</code> Missing input data Verify file paths <code>r.sun: Memory allocation failed</code> Insufficient RAM Reduce region size or tile processing <code>r.sun: Invalid day parameter</code> Day outside 1-365 range Check day parameter <code>Projection mismatch</code> CRS inconsistency Reproject data to common CRS","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#error-recovery-strategies","level":3,"title":"Error Recovery Strategies","text":"<pre><code>def robust_eemt_calculation(dem_file, climate_dir, output_dir, max_retries=3):\n    \"\"\"\n    EEMT calculation with error recovery\n\n    Implements automatic retry, fallback methods, and error logging\n    \"\"\"\n\n    import logging\n    from time import sleep\n\n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(f'{output_dir}/eemt_calculation.log'),\n            logging.StreamHandler()\n        ]\n    )\n\n    for attempt in range(max_retries):\n        try:\n            # Attempt EEMT calculation\n            result = calculate_eemt_complete(dem_file, climate_dir, output_dir)\n            logging.info(f\"EEMT calculation successful on attempt {attempt + 1}\")\n            return result\n\n        except MemoryError as e:\n            logging.warning(f\"Memory error on attempt {attempt + 1}: {e}\")\n            if attempt &lt; max_retries - 1:\n                # Try with reduced resolution\n                logging.info(\"Retrying with reduced spatial resolution...\")\n                dem_file = reduce_resolution(dem_file, factor=2)\n            else:\n                raise\n\n        except FileNotFoundError as e:\n            logging.error(f\"Missing input file: {e}\")\n            raise\n\n        except subprocess.CalledProcessError as e:\n            logging.warning(f\"GRASS command failed on attempt {attempt + 1}: {e}\")\n            if attempt &lt; max_retries - 1:\n                sleep(5)  # Wait before retry\n                logging.info(\"Retrying GRASS operation...\")\n            else:\n                raise\n\n        except Exception as e:\n            logging.error(f\"Unexpected error: {e}\")\n            if attempt &lt; max_retries - 1:\n                sleep(10)\n                logging.info(\"Retrying with fresh environment...\")\n            else:\n                raise\n\n    raise RuntimeError(f\"EEMT calculation failed after {max_retries} attempts\")\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#memory-management","level":3,"title":"Memory Management","text":"<pre><code>def optimize_grass_memory(max_memory_gb=16):\n    \"\"\"\n    Optimize GRASS GIS memory settings for large datasets\n\n    Parameters:\n    -----------\n    max_memory_gb : int\n        Maximum memory to allocate [GB]\n    \"\"\"\n\n    import os\n\n    # Set GRASS memory environment\n    cache_size = max_memory_gb * 1024  # Convert to MB\n\n    os.environ.update({\n        'GRASS_CACHE_SIZE': str(cache_size),\n        'GRASS_RASTER_TMPDIR_MAPSET': '/tmp',\n        'GRASS_VECTOR_TMPDIR_MAPSET': '/tmp',\n        'GRASS_COMPRESS_NULLS': '1',\n        'GRASS_RENDER_IMMEDIATE': 'FALSE'\n    })\n\n    print(f\"‚úì GRASS memory optimized for {max_memory_gb} GB\")\n\ndef calculate_optimal_tile_size(dem_file, available_memory_gb=8):\n    \"\"\"\n    Calculate optimal tile size for memory-constrained processing\n\n    Parameters:\n    -----------\n    dem_file : str\n        Path to DEM file\n    available_memory_gb : int  \n        Available system memory [GB]\n\n    Returns:\n    --------\n    tile_size : int\n        Optimal tile size in pixels\n    overlap : int\n        Recommended overlap in pixels\n    \"\"\"\n\n    import rasterio\n\n    with rasterio.open(dem_file) as src:\n        width, height = src.width, src.height\n        dtype_size = np.dtype(src.dtypes[0]).itemsize\n\n    # Estimate memory usage per pixel (including intermediate arrays)\n    memory_per_pixel = dtype_size * 20  # Factor for r.sun calculations\n\n    # Calculate tile size that fits in available memory\n    available_memory_bytes = available_memory_gb * 1024**3\n    max_pixels = available_memory_bytes // memory_per_pixel\n    tile_size = int(np.sqrt(max_pixels))\n\n    # Ensure reasonable tile size\n    tile_size = max(256, min(tile_size, 4096))\n    overlap = max(32, tile_size // 16)  # 6.25% overlap\n\n    return tile_size, overlap\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#parallel-processing-configuration","level":3,"title":"Parallel Processing Configuration","text":"<pre><code>def configure_parallel_processing(max_workers=None, threads_per_worker=4):\n    \"\"\"\n    Configure optimal parallel processing parameters\n\n    Parameters:\n    -----------\n    max_workers : int, optional\n        Maximum number of worker processes (default: CPU count // 4)\n    threads_per_worker : int\n        OpenMP threads per worker process\n\n    Returns:\n    --------\n    config : dict\n        Optimized processing configuration\n    \"\"\"\n\n    import multiprocessing as mp\n    import psutil\n\n    # Detect system capabilities\n    cpu_count = mp.cpu_count()\n    memory_gb = psutil.virtual_memory().total // (1024**3)\n\n    # Calculate optimal configuration\n    if max_workers is None:\n        max_workers = max(1, cpu_count // threads_per_worker)\n\n    # Memory per worker (reserve 2 GB for system)\n    memory_per_worker = max(2, (memory_gb - 2) // max_workers)\n\n    config = {\n        'max_workers': max_workers,\n        'threads_per_worker': threads_per_worker,\n        'memory_per_worker_gb': memory_per_worker,\n        'total_threads': max_workers * threads_per_worker,\n        'memory_efficiency': memory_per_worker / (memory_gb / max_workers)\n    }\n\n    print(f\"Parallel Processing Configuration:\")\n    print(f\"  Workers: {config['max_workers']}\")\n    print(f\"  Threads per worker: {config['threads_per_worker']}\")\n    print(f\"  Total threads: {config['total_threads']}\")\n    print(f\"  Memory per worker: {config['memory_per_worker_gb']} GB\")\n\n    return config\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#validation-functions","level":2,"title":"Validation Functions","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#statistical-validation","level":3,"title":"Statistical Validation","text":"<pre><code>def validate_eemt_results(eemt_results, validation_data, method='pearson'):\n    \"\"\"\n    Validate EEMT results against field measurements\n\n    Parameters:\n    -----------\n    eemt_results : dict\n        EEMT calculation results\n    validation_data : dict\n        Validation datasets:\n        - 'soil_depth': Measured soil depths [cm]\n        - 'biomass': Measured biomass [Mg/ha]\n        - 'npp': Measured NPP [kg/m¬≤/yr] \n        - 'coordinates': Sample locations\n    method : str\n        Validation method: 'pearson', 'spearman', 'rmse'\n\n    Returns:\n    --------\n    validation_results : dict\n        Validation statistics and plots\n    \"\"\"\n\n    from scipy import stats\n    import matplotlib.pyplot as plt\n\n    validation_results = {}\n\n    # Extract EEMT values at validation points\n    for data_type, data in validation_data.items():\n        if data_type == 'coordinates':\n            continue\n\n        # Extract EEMT values at measurement locations\n        eemt_at_points = extract_values_at_points(\n            eemt_results['eemt'], \n            validation_data['coordinates']\n        )\n\n        # Calculate validation statistics\n        if method == 'pearson':\n            r, p = stats.pearsonr(eemt_at_points, data)\n            validation_results[data_type] = {\n                'correlation': r,\n                'p_value': p,\n                'r_squared': r**2\n            }\n        elif method == 'rmse':\n            rmse = np.sqrt(np.mean((eemt_at_points - data)**2))\n            mae = np.mean(np.abs(eemt_at_points - data))\n            validation_results[data_type] = {\n                'rmse': rmse,\n                'mae': mae,\n                'bias': np.mean(eemt_at_points - data)\n            }\n\n    return validation_results\n\ndef cross_validate_eemt_methods(dem_file, climate_data, validation_points):\n    \"\"\"\n    Cross-validation of different EEMT calculation methods\n\n    Compares Traditional, Topographic, and Vegetation approaches\n    against field validation data\n    \"\"\"\n\n    methods = ['traditional', 'topographic', 'vegetation']\n    results = {}\n\n    for method in methods:\n        print(f\"Cross-validating {method} EEMT...\")\n\n        # Calculate EEMT using specific method\n        if method == 'traditional':\n            eemt = calculate_eemt_traditional(climate_data)\n        elif method == 'topographic': \n            eemt = calculate_eemt_topographic(dem_file, climate_data)\n        else:\n            eemt = calculate_eemt_vegetation(dem_file, climate_data)\n\n        # Validate against field data\n        validation = validate_eemt_results(eemt, validation_points)\n        results[method] = validation\n\n    # Compare methods\n    print(\"\\\\nMethod Comparison:\")\n    print(\"-\" * 50)\n    for method, validation in results.items():\n        if 'soil_depth' in validation:\n            r2 = validation['soil_depth']['r_squared']\n            print(f\"{method.capitalize():12} | R¬≤ = {r2:.3f}\")\n\n    return results\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#utility-functions","level":2,"title":"Utility Functions","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#data-processing-utilities","level":3,"title":"Data Processing Utilities","text":"<pre><code>def extract_values_at_points(raster_file, coordinates, method='bilinear'):\n    \"\"\"Extract raster values at point locations\"\"\"\n\n    import rasterio\n    from rasterio.sample import sample_gen\n\n    with rasterio.open(raster_file) as src:\n        values = list(sample_gen(src, coordinates, indexes=1))\n\n    return np.array([val[0] for val in values])\n\ndef calculate_zonal_statistics(raster_file, zones_file, statistics=['mean', 'std']):\n    \"\"\"Calculate statistics by zones (e.g., elevation bands, watersheds)\"\"\"\n\n    from rasterstats import zonal_stats\n    import geopandas as gpd\n\n    # Load zones\n    zones = gpd.read_file(zones_file)\n\n    # Calculate statistics\n    stats_result = zonal_stats(\n        zones, \n        raster_file, \n        stats=statistics,\n        geojson_out=True\n    )\n\n    return gpd.GeoDataFrame.from_features(stats_result)\n\ndef resample_to_common_grid(file_list, reference_file, output_dir, method='bilinear'):\n    \"\"\"Resample all rasters to common grid\"\"\"\n\n    import subprocess\n    from pathlib import Path\n\n    output_dir = Path(output_dir)\n    output_dir.mkdir(exist_ok=True)\n\n    # Get reference grid parameters\n    with rasterio.open(reference_file) as src:\n        ref_transform = src.transform\n        ref_crs = src.crs\n        ref_width = src.width\n        ref_height = src.height\n\n    resampled_files = []\n\n    for input_file in file_list:\n\n        output_file = output_dir / f\"resampled_{Path(input_file).name}\"\n\n        # Use gdalwarp for resampling\n        cmd = [\n            'gdalwarp',\n            '-tr', str(ref_transform[0]), str(-ref_transform[4]),  # Resolution\n            '-te', str(ref_transform[2]), str(ref_transform[5]),   # Extent  \n                   str(ref_transform[2] + ref_width * ref_transform[0]),\n                   str(ref_transform[5] + ref_height * ref_transform[4]),\n            '-t_srs', str(ref_crs),  # Target CRS\n            '-r', method,            # Resampling method\n            '-co', 'COMPRESS=LZW',   # Compression\n            str(input_file),\n            str(output_file)\n        ]\n\n        subprocess.run(cmd, check=True)\n        resampled_files.append(str(output_file))\n\n    return resampled_files\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#command-line-interface","level":2,"title":"Command Line Interface","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#main-eemt-calculator-script","level":3,"title":"Main EEMT Calculator Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nCommand line interface for EEMT calculations\nUsage: python eemt_calculator.py [options] dem_file\n\"\"\"\n\nusage_examples = '''\nExamples:\n  # Basic EEMT calculation\n  python eemt_calculator.py dem.tif --climate climate_data/ --output results/\n\n  # Topographic EEMT with parallel processing\n  python eemt_calculator.py dem.tif --method topographic --threads 16 \\\\\n    --climate daymet_data/ --output topo_results/\n\n  # Full vegetation EEMT with validation\n  python eemt_calculator.py dem.tif --method vegetation \\\\\n    --climate climate/ --vegetation ndvi.tif,lidar.las \\\\\n    --validate soil_samples.shp --output veg_results/\n\n  # Time series analysis\n  python eemt_calculator.py dem.tif --method topographic \\\\\n    --start-year 2000 --end-year 2020 --time-series \\\\\n    --output timeseries_results/\n'''\n\n# Command line argument definitions\nCLI_ARGUMENTS = {\n    'dem_file': {\n        'type': str,\n        'help': 'Input digital elevation model (GeoTIFF format)'\n    },\n    '--method': {\n        'choices': ['traditional', 'topographic', 'vegetation', 'all'],\n        'default': 'topographic',\n        'help': 'EEMT calculation method'\n    },\n    '--climate': {\n        'type': str, \n        'required': True,\n        'help': 'Climate data directory (DAYMET NetCDF files)'\n    },\n    '--output': {\n        'type': str,\n        'required': True, \n        'help': 'Output directory for results'\n    },\n    '--threads': {\n        'type': int,\n        'default': 4,\n        'help': 'Number of parallel processing threads'\n    },\n    '--step': {\n        'type': float,\n        'default': 0.25,\n        'help': 'Solar calculation time step [hours]'\n    },\n    '--linke': {\n        'type': float,\n        'default': 3.0,\n        'help': 'Linke atmospheric turbidity factor [1.0-8.0]'\n    },\n    '--albedo': {\n        'type': float, \n        'default': 0.2,\n        'help': 'Surface albedo coefficient [0.0-1.0]'\n    },\n    '--vegetation': {\n        'type': str,\n        'help': 'Vegetation data files (comma-separated): ndvi.tif,lidar.las'\n    },\n    '--start-year': {\n        'type': int,\n        'default': 2015,\n        'help': 'Start year for time series analysis'\n    },\n    '--end-year': {\n        'type': int, \n        'default': 2020,\n        'help': 'End year for time series analysis'\n    },\n    '--validate': {\n        'type': str,\n        'help': 'Validation data file (point shapefile with measurements)'\n    },\n    '--time-series': {\n        'action': 'store_true',\n        'help': 'Generate annual time series output'\n    },\n    '--tile-size': {\n        'type': int,\n        'default': 2048,\n        'help': 'Tile size for large dataset processing [pixels]'\n    },\n    '--verbose': {\n        'action': 'store_true',\n        'help': 'Enable verbose output'\n    }\n}\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#testing-framework","level":2,"title":"Testing Framework","text":"","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#unit-tests","level":3,"title":"Unit Tests","text":"<pre><code>import unittest\nimport numpy as np\nfrom pathlib import Path\n\nclass TestEEMTCalculations(unittest.TestCase):\n    \"\"\"Unit tests for EEMT calculation functions\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test data\"\"\"\n        self.test_data_dir = Path('test_data')\n        self.test_data_dir.mkdir(exist_ok=True)\n\n        # Create synthetic test DEM\n        self.create_test_dem()\n\n        # Create synthetic climate data\n        self.create_test_climate()\n\n    def test_traditional_eemt(self):\n        \"\"\"Test traditional EEMT calculation\"\"\"\n\n        # Simple test case\n        temp = np.array([15.0])  # ¬∞C\n        precip = np.array([50.0])  # mm/month\n\n        eemt, e_bio, e_ppt = calculate_eemt_traditional(temp, precip)\n\n        # Check output ranges\n        self.assertGreater(eemt[0], 0, \"EEMT should be positive\")\n        self.assertLess(eemt[0], 100, \"EEMT should be reasonable (&lt;100 MJ/m¬≤/yr)\")\n\n        # Check components\n        self.assertGreater(e_bio[0], 0, \"E_BIO should be positive\")\n        self.assertGreaterEqual(e_ppt[0], 0, \"E_PPT should be non-negative\")\n\n    def test_solar_radiation_range(self):\n        \"\"\"Test solar radiation calculations produce reasonable values\"\"\"\n\n        # Test with synthetic DEM\n        dem_file = self.test_data_dir / 'test_dem.tif'\n\n        # Should complete without errors\n        try:\n            solar_result = calculate_annual_solar(dem_file, days=[180])  # Summer solstice\n            self.assertTrue(True, \"Solar calculation completed\")\n        except Exception as e:\n            self.fail(f\"Solar calculation failed: {e}\")\n\n    def test_aspect_effects(self):\n        \"\"\"Test that north-facing slopes have higher EEMT\"\"\"\n\n        # Create test data with known aspect effects\n        north_slope_eemt = calculate_eemt_topographic(\n            self.create_test_slope(aspect=0)  # North-facing\n        )\n\n        south_slope_eemt = calculate_eemt_topographic(\n            self.create_test_slope(aspect=180)  # South-facing\n        )\n\n        # North slopes should have higher EEMT in water-limited environments\n        self.assertGreater(\n            np.mean(north_slope_eemt),\n            np.mean(south_slope_eemt),\n            \"North-facing slopes should have higher EEMT\"\n        )\n\n    def create_test_dem(self):\n        \"\"\"Create synthetic DEM for testing\"\"\"\n\n        # Create elevation gradient\n        x, y = np.meshgrid(np.linspace(0, 1000, 100), np.linspace(0, 1000, 100))\n        elevation = 1000 + x * 0.5 + y * 0.3 + np.random.normal(0, 10, (100, 100))\n\n        # Save as GeoTIFF\n        profile = {\n            'driver': 'GTiff',\n            'height': 100,\n            'width': 100,\n            'count': 1,\n            'dtype': 'float32',\n            'crs': 'EPSG:4326',\n            'transform': rasterio.transform.from_bounds(-111, 32, -110, 33, 100, 100)\n        }\n\n        with rasterio.open(self.test_data_dir / 'test_dem.tif', 'w', **profile) as dst:\n            dst.write(elevation.astype(np.float32), 1)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/#integration-tests","level":3,"title":"Integration Tests","text":"<pre><code>#!/bin/bash\n# Integration test suite for EEMT workflows\n\nset -e\n\necho \"=== EEMT Integration Tests ===\"\n\n# Test 1: Basic workflow with sample data\necho \"Test 1: Basic EEMT workflow...\"\npython eemt_calculator.py test_data/sample_dem.tif \\\\\n  --climate test_data/climate/ \\\\\n  --output test_results/basic/ \\\\\n  --method traditional\n\n# Test 2: Parallel processing\necho \"Test 2: Parallel solar calculation...\"\npython eemt_calculator.py test_data/sample_dem.tif \\\\\n  --climate test_data/climate/ \\\\\n  --output test_results/parallel/ \\\\\n  --method topographic \\\\\n  --threads 4\n\n# Test 3: Large dataset handling\necho \"Test 3: Large dataset processing...\"\npython eemt_calculator.py test_data/large_dem.tif \\\\\n  --climate test_data/climate/ \\\\\n  --output test_results/large/ \\\\\n  --tile-size 1024\n\n# Test 4: Validation\necho \"Test 4: Results validation...\"\npython validate_results.py test_results/ test_data/validation/\n\necho \"‚úì All integration tests passed\"\n</code></pre> <p>This API reference provides the complete technical foundation for implementing and extending EEMT calculations with modern computational approaches and comprehensive error handling.</p>","path":["API Documentation","API Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/","level":1,"title":"Workflow Parameters Reference","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#overview","level":2,"title":"Overview","text":"<p>This comprehensive reference documents all parameters available for EEMT and Solar Radiation workflows, including their scientific basis, valid ranges, and impact on calculations.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#solar-radiation-workflow-parameters","level":2,"title":"Solar Radiation Workflow Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#core-parameters","level":3,"title":"Core Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#step-time-step","level":4,"title":"<code>step</code> - Time Step","text":"<p>Type: Float Default: 15.0 Range: 3.0 - 60.0 minutes Required: No</p> <p>Description: Time interval for solar radiation calculations throughout the day. Smaller values provide higher temporal resolution but increase computation time.</p> <p>Scientific Basis: The time step determines how frequently the sun's position is calculated and solar radiation is computed. This affects the accuracy of daily radiation totals, especially in complex terrain.</p> <p>Impact on Results: - 3-5 minutes: High precision, captures rapid shadow changes in complex terrain - 10-15 minutes: Good balance of accuracy and performance for most applications - 30-60 minutes: Faster computation, suitable for regional assessments</p> <p>Example Usage: <pre><code># High-resolution analysis for complex terrain\nparameters = {\n    'step': 3.0,  # 3-minute intervals\n    'num_threads': 8\n}\n\n# Regional assessment with moderate resolution\nparameters = {\n    'step': 15.0,  # 15-minute intervals (default)\n    'num_threads': 4\n}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#linke_value-atmospheric-turbidity","level":4,"title":"<code>linke_value</code> - Atmospheric Turbidity","text":"<p>Type: Float Default: 3.0 Range: 1.0 - 8.0 Required: No</p> <p>Description: Linke turbidity factor representing atmospheric optical thickness due to absorption and scattering.</p> <p>Scientific Basis: The Linke turbidity factor accounts for the attenuation of solar radiation as it passes through the atmosphere. It combines the effects of: - Water vapor absorption - Aerosol scattering - Molecular (Rayleigh) scattering</p> <p>Typical Values by Environment:</p> Environment Linke Value Description Clean mountain air 1.0 - 2.0 Very clear atmosphere, high elevation Rural areas 2.5 - 3.5 Clean continental atmosphere Urban areas 3.5 - 5.0 Moderate pollution and aerosols Industrial zones 5.0 - 8.0 Heavy pollution, high aerosol content <p>Seasonal Variations: <pre><code># Winter (clearer atmosphere)\nwinter_params = {'linke_value': 2.5}\n\n# Summer (more water vapor and aerosols)\nsummer_params = {'linke_value': 3.5}\n\n# Monsoon/humid season\nhumid_params = {'linke_value': 4.5}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#albedo_value-surface-reflectance","level":4,"title":"<code>albedo_value</code> - Surface Reflectance","text":"<p>Type: Float Default: 0.2 Range: 0.0 - 1.0 Required: No</p> <p>Description: Fraction of incident solar radiation reflected by the surface.</p> <p>Scientific Basis: Albedo affects the amount of diffuse radiation through multiple reflections between the surface and atmosphere. Higher albedo increases the total radiation received through these interactions.</p> <p>Typical Values by Surface Type:</p> Surface Type Albedo Example Fresh snow 0.80 - 0.95 Alpine environments Old snow 0.50 - 0.70 Late season snowpack Desert sand 0.30 - 0.45 Arid regions Grassland 0.15 - 0.25 Natural vegetation Forest 0.10 - 0.20 Dense canopy Water 0.05 - 0.10 Lakes, oceans Asphalt 0.05 - 0.15 Urban surfaces <p>Land Cover Specific Examples: <pre><code># Forest analysis\nforest_params = {\n    'albedo_value': 0.15,\n    'linke_value': 2.8  # Some canopy filtering\n}\n\n# Snow-covered terrain\nsnow_params = {\n    'albedo_value': 0.85,\n    'linke_value': 2.0  # Clear winter air\n}\n\n# Urban environment\nurban_params = {\n    'albedo_value': 0.12,\n    'linke_value': 4.5  # Urban pollution\n}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#computational-parameters","level":3,"title":"Computational Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#num_threads-cpu-threads","level":4,"title":"<code>num_threads</code> - CPU Threads","text":"<p>Type: Integer Default: 4 Range: 1 - 32 Required: No</p> <p>Description: Number of parallel processing threads for workflow execution.</p> <p>Performance Impact: <pre><code># Estimated processing times (10km x 10km @ 10m resolution)\n# 1 thread:  ~4 hours\n# 4 threads: ~1 hour (default)\n# 8 threads: ~35 minutes\n# 16 threads: ~20 minutes (diminishing returns)\n</code></pre></p> <p>Resource Considerations: - Each thread requires ~2GB RAM - I/O becomes bottleneck beyond 8-16 threads - Leave 1-2 cores for system processes</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#advanced-parameters","level":3,"title":"Advanced Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#day-specific-day","level":4,"title":"<code>day</code> - Specific Day","text":"<p>Type: Integer Range: 1 - 365 Required: No (processes all days if not specified)</p> <p>Description: Calculate solar radiation for a specific day of year.</p> <pre><code># Summer solstice analysis\nparams = {'day': 172}  # June 21 (day 172)\n\n# Winter solstice analysis  \nparams = {'day': 355}  # December 21 (day 355)\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#beam_rad-beam-radiation-output","level":4,"title":"<code>beam_rad</code> - Beam Radiation Output","text":"<p>Type: Boolean Default: True Required: No</p> <p>Description: Output direct beam radiation component separately.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#diff_rad-diffuse-radiation-output","level":4,"title":"<code>diff_rad</code> - Diffuse Radiation Output","text":"<p>Type: Boolean Default: True Required: No</p> <p>Description: Output diffuse radiation component separately.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#refl_rad-reflected-radiation-output","level":4,"title":"<code>refl_rad</code> - Reflected Radiation Output","text":"<p>Type: Boolean Default: False Required: No</p> <p>Description: Output ground-reflected radiation component.</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#eemt-workflow-parameters","level":2,"title":"EEMT Workflow Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#temporal-parameters","level":3,"title":"Temporal Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#start_year-start-year","level":4,"title":"<code>start_year</code> - Start Year","text":"<p>Type: Integer Default: 2020 Range: 1980 - 2024 Required: Yes for EEMT workflow</p> <p>Description: First year of climate data to process.</p> <p>Data Availability: - DAYMET v4: 1980 - present (1-2 year lag) - Quality varies by year and region - Recent years may have provisional data</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#end_year-end-year","level":4,"title":"<code>end_year</code> - End Year","text":"<p>Type: Integer Default: 2020 Range: 1980 - 2024 Required: Yes for EEMT workflow</p> <p>Description: Last year of climate data to process.</p> <p>Multi-Year Processing: <pre><code># Single year analysis\nparams = {\n    'start_year': 2020,\n    'end_year': 2020\n}\n\n# 5-year climatology\nparams = {\n    'start_year': 2016,\n    'end_year': 2020\n}\n\n# Long-term analysis (30+ years)\nparams = {\n    'start_year': 1990,\n    'end_year': 2020,\n    'num_threads': 16  # Use more threads for large datasets\n}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#climate-data-parameters","level":3,"title":"Climate Data Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#daymet_variables-climate-variables","level":4,"title":"<code>daymet_variables</code> - Climate Variables","text":"<p>Type: List[str] Default: ['tmin', 'tmax', 'prcp', 'vp'] Options: tmin, tmax, prcp, vp, srad, swe, dayl Required: No</p> <p>Description: DAYMET climate variables to download and process.</p> <p>Variable Descriptions:</p> Variable Description Units Use in EEMT tmin Daily minimum temperature ¬∞C Energy calculations tmax Daily maximum temperature ¬∞C Energy calculations prcp Daily precipitation mm/day Water flux vp Daily average vapor pressure Pa Humidity effects srad Incoming shortwave radiation W/m¬≤ Validation swe Snow water equivalent kg/m¬≤ Snow dynamics dayl Day length seconds Photoperiod","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#climate_buffer-spatial-buffer","level":4,"title":"<code>climate_buffer</code> - Spatial Buffer","text":"<p>Type: Float Default: 0.1 Range: 0.0 - 1.0 degrees Required: No</p> <p>Description: Buffer around DEM extent for climate data download.</p> <pre><code># Tight boundary (minimize download)\nparams = {'climate_buffer': 0.01}\n\n# Include surrounding area for edge effects\nparams = {'climate_buffer': 0.25}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#eemt-calculation-parameters","level":3,"title":"EEMT Calculation Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#eemt_method-calculation-method","level":4,"title":"<code>eemt_method</code> - Calculation Method","text":"<p>Type: String Default: 'topographic' Options: 'traditional', 'topographic', 'vegetation' Required: No</p> <p>Description: EEMT calculation methodology.</p> <p>Method Comparison:</p> Method Description Inputs Required Best For traditional Climate-based only Climate data Regional comparison topographic Terrain-modified DEM + climate Complex terrain vegetation Full ecosystem DEM + climate + LAI Detailed analysis <p>Method Selection: <pre><code># Simple regional assessment\nparams = {'eemt_method': 'traditional'}\n\n# Mountain watershed analysis\nparams = {'eemt_method': 'topographic'}\n\n# Ecosystem carbon studies\nparams = {'eemt_method': 'vegetation'}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#npp_model-npp-calculation","level":4,"title":"<code>npp_model</code> - NPP Calculation","text":"<p>Type: String Default: 'miami' Options: 'miami', 'thornthwaite', 'user_defined' Required: No</p> <p>Description: Net Primary Production model for biological energy calculation.</p> <p>Model Equations:</p> <p>Miami Model: <pre><code># Temperature-limited\nNPP_t = 3000 * (1 - exp(1.315 - 0.119 * T))\n\n# Precipitation-limited  \nNPP_p = 3000 * (1 - exp(-0.000664 * P))\n\n# Actual NPP (minimum)\nNPP = min(NPP_t, NPP_p)\n</code></pre></p> <p>Thornthwaite Model: <pre><code># Based on evapotranspiration\nNPP = 3000 * (AET / PET)\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#topographic-parameters","level":3,"title":"Topographic Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#slope_threshold-maximum-slope","level":4,"title":"<code>slope_threshold</code> - Maximum Slope","text":"<p>Type: Float Default: 45.0 Range: 0.0 - 90.0 degrees Required: No</p> <p>Description: Maximum slope angle for stable soil formation.</p> <p>Geomorphological Context: - &lt; 15¬∞: Minimal erosion, stable soils - 15-30¬∞: Moderate erosion potential - 30-45¬∞: High erosion, thin soils - &gt; 45¬∞: Bedrock exposure likely</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#twi_threshold-wetness-threshold","level":4,"title":"<code>twi_threshold</code> - Wetness Threshold","text":"<p>Type: Float Default: 10.0 Range: 0.0 - 20.0 Required: No</p> <p>Description: Topographic Wetness Index threshold for water accumulation zones.</p> <p>TWI Interpretation: - &lt; 5: Ridge tops, dry areas - 5-10: Hillslopes, normal drainage - 10-15: Convergent areas, seasonal wetness - &gt; 15: Valley bottoms, persistent wetness</p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#output-control-parameters","level":3,"title":"Output Control Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#output_format-file-format","level":4,"title":"<code>output_format</code> - File Format","text":"<p>Type: String Default: 'geotiff' Options: 'geotiff', 'netcdf', 'zarr' Required: No</p> <p>Format Characteristics:</p> Format Pros Cons Best For GeoTIFF Wide compatibility Single variable per file GIS integration NetCDF Multi-dimensional Requires special tools Time series Zarr Cloud-optimized New format Big data","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#output_compression-compression","level":4,"title":"<code>output_compression</code> - Compression","text":"<p>Type: String Default: 'lzw' Options: 'none', 'lzw', 'deflate', 'zstd' Required: No</p> <p>Compression Trade-offs: <pre><code># No compression (fastest write, largest files)\nparams = {'output_compression': 'none'}\n\n# LZW (good balance, wide support)\nparams = {'output_compression': 'lzw'}\n\n# Deflate (better compression, slower)\nparams = {'output_compression': 'deflate'}\n\n# Zstd (best compression, newest)\nparams = {'output_compression': 'zstd'}\n</code></pre></p>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#output_resolution-output-resolution","level":4,"title":"<code>output_resolution</code> - Output Resolution","text":"<p>Type: Float Default: Same as input DEM Range: 1.0 - 1000.0 meters Required: No</p> <p>Description: Resample output to different resolution.</p> <pre><code># Maintain input resolution\nparams = {}  # Default behavior\n\n# Aggregate to coarser resolution\nparams = {'output_resolution': 30.0}  # 30m output\n\n# Regional product\nparams = {'output_resolution': 100.0}  # 100m output\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#parameter-validation","level":2,"title":"Parameter Validation","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#input-validation-rules","level":3,"title":"Input Validation Rules","text":"<pre><code>def validate_parameters(params: dict, workflow_type: str) -&gt; dict:\n    \"\"\"Validate and sanitize workflow parameters\"\"\"\n\n    # Common validations\n    if params.get('step'):\n        assert 3.0 &lt;= params['step'] &lt;= 60.0\n\n    if params.get('linke_value'):\n        assert 1.0 &lt;= params['linke_value'] &lt;= 8.0\n\n    if params.get('albedo_value'):\n        assert 0.0 &lt;= params['albedo_value'] &lt;= 1.0\n\n    if params.get('num_threads'):\n        assert 1 &lt;= params['num_threads'] &lt;= 32\n\n    # EEMT-specific validations\n    if workflow_type == 'eemt':\n        assert params.get('start_year'), \"start_year required for EEMT\"\n        assert params.get('end_year'), \"end_year required for EEMT\"\n        assert params['start_year'] &lt;= params['end_year']\n        assert 1980 &lt;= params['start_year'] &lt;= 2024\n        assert 1980 &lt;= params['end_year'] &lt;= 2024\n\n    return params\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#parameter-combinations","level":3,"title":"Parameter Combinations","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#high-accuracy-solar-analysis","level":4,"title":"High-Accuracy Solar Analysis","text":"<pre><code>high_accuracy = {\n    'step': 3.0,\n    'linke_value': 3.0,\n    'albedo_value': 0.2,\n    'num_threads': 16,\n    'beam_rad': True,\n    'diff_rad': True,\n    'refl_rad': True\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#quick-regional-assessment","level":4,"title":"Quick Regional Assessment","text":"<pre><code>quick_regional = {\n    'step': 30.0,\n    'linke_value': 3.0,\n    'albedo_value': 0.2,\n    'num_threads': 4,\n    'output_resolution': 100.0,\n    'output_compression': 'lzw'\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#climate-change-analysis","level":4,"title":"Climate Change Analysis","text":"<pre><code>climate_analysis = {\n    'start_year': 1990,\n    'end_year': 2020,\n    'step': 15.0,\n    'eemt_method': 'topographic',\n    'daymet_variables': ['tmin', 'tmax', 'prcp', 'vp', 'srad'],\n    'output_format': 'netcdf'\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#snow-dominated-watershed","level":4,"title":"Snow-Dominated Watershed","text":"<pre><code>snow_watershed = {\n    'step': 10.0,\n    'linke_value': 2.5,  # Clear mountain air\n    'albedo_value': 0.65,  # Snow average\n    'start_year': 2020,\n    'end_year': 2020,\n    'daymet_variables': ['tmin', 'tmax', 'prcp', 'swe'],\n    'slope_threshold': 35.0  # Avalanche consideration\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#performance-optimization-guide","level":2,"title":"Performance Optimization Guide","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#memory-requirements","level":3,"title":"Memory Requirements","text":"<pre><code>def estimate_memory(dem_size_mb: float, params: dict) -&gt; float:\n    \"\"\"Estimate memory requirements in GB\"\"\"\n\n    base_memory = 2.0  # OS and runtime\n\n    # DEM memory (multiple copies for processing)\n    dem_memory = dem_size_mb * 4 / 1024  \n\n    # Thread memory (2GB per thread)\n    thread_memory = params.get('num_threads', 4) * 2\n\n    # Time series memory (for EEMT)\n    if params.get('start_year'):\n        years = params['end_year'] - params['start_year'] + 1\n        timeseries_memory = dem_size_mb * years * 365 / 1024\n    else:\n        timeseries_memory = 0\n\n    return base_memory + dem_memory + thread_memory + timeseries_memory\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#processing-time-estimates","level":3,"title":"Processing Time Estimates","text":"<pre><code>def estimate_runtime(dem_pixels: int, params: dict) -&gt; float:\n    \"\"\"Estimate runtime in hours\"\"\"\n\n    # Base rate: pixels per second per thread\n    if params.get('step', 15) &lt;= 5:\n        rate = 1000  # High resolution\n    elif params.get('step', 15) &lt;= 15:\n        rate = 5000  # Medium resolution\n    else:\n        rate = 10000  # Low resolution\n\n    threads = params.get('num_threads', 4)\n\n    # Solar workflow\n    days = 365\n    solar_time = (dem_pixels * days) / (rate * threads * 3600)\n\n    # EEMT additions\n    if params.get('start_year'):\n        years = params['end_year'] - params['start_year'] + 1\n        eemt_time = solar_time * years * 1.5  # Climate data overhead\n    else:\n        eemt_time = solar_time\n\n    return eemt_time\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#troubleshooting-parameters","level":2,"title":"Troubleshooting Parameters","text":"","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#common-parameter-issues","level":3,"title":"Common Parameter Issues","text":"Issue Cause Solution Out of memory Too many threads Reduce <code>num_threads</code> Slow processing Fine time step Increase <code>step</code> to 15-30 Poor results in shadows Coarse time step Decrease <code>step</code> to 3-5 Unrealistic radiation Wrong turbidity Adjust <code>linke_value</code> for conditions Missing diffuse radiation Wrong albedo Set appropriate <code>albedo_value</code> DAYMET download fails Invalid year range Check data availability Large output files No compression Enable <code>output_compression</code>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#parameter-debugging","level":3,"title":"Parameter Debugging","text":"<pre><code># Enable verbose logging\ndebug_params = {\n    'step': 15.0,\n    'num_threads': 1,  # Single thread for debugging\n    'verbose': True,\n    'debug': True,\n    'log_level': 'DEBUG'\n}\n</code></pre>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"api-reference/workflow-parameters/#related-documentation","level":2,"title":"Related Documentation","text":"<ul> <li>Workflow Examples</li> <li>API Reference</li> <li>Scientific Background</li> <li>Web Interface Guide</li> </ul>","path":["API Documentation","Workflow Parameters Reference"],"tags":[]},{"location":"background/","level":1,"title":"Scientific Background","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#what-is-the-critical-zone","level":2,"title":"What is the Critical Zone?","text":"<p>The Critical Zone is Earth's near-surface environment extending from the top of the vegetation canopy down to the groundwater. This zone supports all terrestrial life and controls:</p> <ul> <li>Fresh water availability and quality</li> <li>Soil formation and agricultural productivity  </li> <li>Carbon storage and climate regulation</li> <li>Biodiversity and ecosystem services</li> <li>Natural hazard mitigation</li> </ul> <p> </p> The Critical Zone extends from the vegetation canopy to groundwater, encompassing the zone where rock, soil, water, air, and living organisms interact.","path":["Scientific Background"],"tags":[]},{"location":"background/#energy-and-mass-transfer-principles","level":2,"title":"Energy and Mass Transfer Principles","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#thermodynamic-foundation","level":3,"title":"Thermodynamic Foundation","text":"<p>The Critical Zone operates as an open thermodynamic system where:</p> <ul> <li>Energy and mass flow down gradients (solar, chemical, gravitational)</li> <li>Internal structures develop to optimize energy dissipation</li> <li>System organization emerges from energy flux patterns</li> <li>Steady-state conditions balance inputs and outputs</li> </ul> \\[\\frac{d\\mathcal{U}_{CZ}}{dt} = \\mathcal{K} - T\\sigma\\] <p>Where:</p> <ul> <li> <p>ùí∞<sub>CZ</sub> = Critical Zone energy storage [J m‚Åª¬≤]</p> </li> <li> <p>ùí¶ = Energy flux through the system [W m‚Åª¬≤]  </p> </li> <li> <p>T = System temperature [K]</p> </li> <li> <p>œÉ = Entropy production rate [W m‚Åª¬≤ K‚Åª¬π]</p> </li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#energy-balance-components","level":3,"title":"Energy Balance Components","text":"<p>The total energy flux includes multiple components:</p> \\[E_{Total} = E_{ET} + E_{PPT} + E_{BIO} + E_{ELEV} + E_{GEO} + \\sum E_i\\] Component Description Magnitude Role E<sub>ET</sub> Evapotranspiration ~10‚Åµ MJ m‚Åª¬≤ yr‚Åª¬π Returns to atmosphere E<sub>PPT</sub> Precipitation energy ~10¬≤ MJ m‚Åª¬≤ yr‚Åª¬π Subsurface heat transfer E<sub>BIO</sub> Primary production ~10¬π MJ m‚Åª¬≤ yr‚Åª¬π Biological energy storage E<sub>ELEV</sub> Gravitational potential ~10‚Å∞ MJ m‚Åª¬≤ yr‚Åª¬π Physical denudation E<sub>GEO</sub> Chemical potential ~10‚Åª¬π MJ m‚Åª¬≤ yr‚Åª¬π Chemical weathering","path":["Scientific Background"],"tags":[]},{"location":"background/#eemt-focus","level":3,"title":"EEMT Focus","text":"<p>Effective Energy and Mass Transfer focuses on subsurface energy flux:</p> \\[\\text{EEMT} = E_{BIO} + E_{PPT}\\] <p>This represents the energy effectively transferred to drive:</p> <ul> <li>Soil formation processes</li> <li>Chemical weathering reactions  </li> <li>Biological productivity</li> <li>Carbon sequestration</li> <li>Nutrient cycling</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#physical-processes","level":2,"title":"Physical Processes","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#solar-radiation-and-topography","level":3,"title":"Solar Radiation and Topography","text":"<p>Solar radiation provides the primary energy input to the Critical Zone. Topographic effects modify radiation through:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#slope-and-aspect-effects","level":4,"title":"Slope and Aspect Effects","text":"<ul> <li>Pole-facing (N,S) slopes: Reduced direct radiation, higher soil moisture</li> <li>Equator-facing slopes: Maximum radiation, increased evapotranspiration  </li> <li>Slope angle: Controls radiation intensity and duration</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#shading-and-obstruction","level":4,"title":"Shading and Obstruction","text":"<ul> <li>Horizon effects: Adjacent terrain blocks incoming radiation</li> <li>Vegetation shading: Canopy intercepts and redistributes energy</li> <li>Seasonal variation: Sun angle changes modify topographic effects</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#water-and-energy-coupling","level":3,"title":"Water and Energy Coupling","text":"<p>Water acts as both a mass flux and energy carrier:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#precipitation-energy-eppt","level":4,"title":"Precipitation Energy (E<sub>PPT</sub>)","text":"\\[E_{PPT} = F \\times c_w \\times \\Delta T \\quad \\text{[W m}^{-2}\\text{]}\\] <p>Where:</p> <ul> <li>F = Effective precipitation flux [kg m‚Åª¬≤ s‚Åª¬π]</li> <li>c<sub>w</sub> = Specific heat of water (4,180 J kg‚Åª¬π K‚Åª¬π)</li> <li>ŒîT = Temperature above freezing [K]</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#water-redistribution","level":4,"title":"Water Redistribution","text":"<ul> <li>Infiltration vs. runoff: Controls subsurface energy delivery</li> <li>Topographic convergence: Concentrates water and energy flux</li> <li>Evapotranspiration: Returns energy to atmosphere</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#biological-energy-ebio","level":3,"title":"Biological Energy (E<sub>BIO</sub>)","text":"<p>Primary production stores solar energy in chemical bonds:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#photosynthesis-energy-storage","level":4,"title":"Photosynthesis Energy Storage","text":"\\[E_{BIO} = NPP \\times h_{BIO} \\quad \\text{[W m}^{-2}\\text{]}\\] <p>Where:</p> <ul> <li>NPP = Net Primary Production [kg m‚Åª¬≤ s‚Åª¬π]</li> <li>h<sub>BIO</sub> = Specific biomass enthalpy (22 √ó 10‚Å∂ J kg‚Åª¬π)</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#carbon-energy-coupling","level":4,"title":"Carbon-Energy Coupling","text":"<ul> <li>CO‚ÇÇ fixation: Solar energy ‚Üí chemical energy</li> <li>Decomposition: Chemical energy ‚Üí heat + nutrients</li> <li>Root activity: Drives chemical weathering reactions</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#critical-zone-structure-and-function","level":2,"title":"Critical Zone Structure and Function","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#emergent-organization","level":3,"title":"Emergent Organization","text":"<p>EEMT drives the formation of organized Critical Zone structures:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#soil-horizons","level":4,"title":"Soil Horizons","text":"<ul> <li>O horizon: Organic matter accumulation</li> <li>A horizon: Mineral-organic mixing</li> <li>B horizon: Clay and nutrient accumulation</li> <li>C horizon: Weathered parent material</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#vegetation-patterns","level":4,"title":"Vegetation Patterns","text":"<ul> <li>Productivity gradients: Follow EEMT patterns</li> <li>Species composition: Adapted to local energy/water balance</li> <li>Biomass allocation: Optimizes energy capture and water access</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#landscape-features","level":4,"title":"Landscape Features","text":"<ul> <li>Channel networks: Organize water and sediment transport</li> <li>Slope profiles: Balance weathering and erosion rates</li> <li>Aspect patterns: Reflect energy-controlled processes</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#threshold-behavior","level":3,"title":"Threshold Behavior","text":"<p>EEMT exhibits critical thresholds that control system behavior:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#water-vs-energy-limitation","level":4,"title":"Water vs. Energy Limitation","text":"<p>Threshold: ~70 MJ m‚Åª¬≤ yr‚Åª¬π</p> <ul> <li>Below threshold: Water-limited, E<sub>BIO</sub> dominates</li> <li>Above threshold: Energy-limited, E<sub>PPT</sub> dominates</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#system-transitions","level":4,"title":"System Transitions","text":"<ul> <li>Vegetation shifts: Grassland ‚Üî forest transitions</li> <li>Soil development: Entisol ‚Üî Mollisol ‚Üî Alfisol progression</li> <li>Geomorphic regime: Weathering-limited ‚Üî transport-limited</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#spatial-and-temporal-scales","level":2,"title":"Spatial and Temporal Scales","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#spatial-scale-integration","level":3,"title":"Spatial Scale Integration","text":"<p>EEMT operates across multiple spatial scales:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#local-scale-1-100-m","level":4,"title":"Local Scale (1-100 m)","text":"<ul> <li>Process-level understanding: Individual tree, soil pedon</li> <li>High-resolution data: LiDAR, field measurements</li> <li>Detailed process modeling: Hourly energy/water balance</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#landscape-scale-100-m-10-km","level":4,"title":"Landscape Scale (100 m - 10 km)","text":"<ul> <li>Pattern-process relationships: Topographic controls</li> <li>Moderate-resolution data: Landsat, weather stations</li> <li>Statistical modeling: Spatial correlation analysis</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#regional-scale-10-1000-km","level":4,"title":"Regional Scale (10-1000 km)","text":"<ul> <li>Climate gradient analysis: Elevation, latitude effects</li> <li>Coarse-resolution data: MODIS, climate models</li> <li>Empirical relationships: Broad pattern identification</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#temporal-scale-integration","level":3,"title":"Temporal Scale Integration","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#short-term-processes-days-to-years","level":4,"title":"Short-term Processes (days to years)","text":"<ul> <li>Weather variability: Daily climate fluctuations</li> <li>Seasonal cycles: Vegetation phenology, soil temperature</li> <li>Extreme events: Drought, fire, flooding impacts</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#medium-term-dynamics-years-to-centuries","level":4,"title":"Medium-term Dynamics (years to centuries)","text":"<ul> <li>Climate oscillations: El Ni√±o, Pacific Decadal Oscillation</li> <li>Vegetation succession: Post-disturbance recovery</li> <li>Soil profile development: Horizon differentiation</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#long-term-evolution-centuries-to-millennia","level":4,"title":"Long-term Evolution (centuries to millennia)","text":"<ul> <li>Climate change: Holocene environmental shifts</li> <li>Landscape evolution: Erosion, weathering, soil formation</li> <li>Ecosystem migration: Species range shifts</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#mathematical-framework","level":2,"title":"Mathematical Framework","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#open-system-thermodynamics","level":3,"title":"Open System Thermodynamics","text":"<p>The Critical Zone energy balance follows fundamental thermodynamic principles:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#first-law-energy-conservation","level":4,"title":"First Law (Energy Conservation)","text":"\\[\\frac{dU}{dt} = Q - W\\] <p>Where U = internal energy, Q = heat input, W = work done by system</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#second-law-entropy-increase","level":4,"title":"Second Law (Entropy Increase)","text":"\\[\\frac{dS}{dt} = \\frac{Q}{T} + \\sigma\\] <p>Where S = entropy, œÉ = irreversible entropy production</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#exergy-concept","level":4,"title":"Exergy Concept","text":"\\[\\text{Exergy} = \\text{Energy} - T_0 \\times \\text{Entropy}\\] <p>Exergy represents the maximum useful work extractable from the system.</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#statistical-relationships","level":3,"title":"Statistical Relationships","text":"<p>EEMT exhibits predictable relationships with Critical Zone properties:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#power-law-scaling","level":4,"title":"Power Law Scaling","text":"\\[\\text{Biomass} = \\alpha \\times \\text{EEMT}^{\\beta}\\] <p>Where Œ± = 0.032 kg m¬≤ yr ha‚Åª¬π MJ‚Åª¬π, Œ≤ = 3.22</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#exponential-relationships","level":4,"title":"Exponential Relationships","text":"\\[\\text{Soil\\_Depth} = \\gamma \\times \\exp(\\delta \\times \\text{EEMT})\\] <p>For specific lithologies and climate conditions</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#threshold-functions","level":4,"title":"Threshold Functions","text":"\\[f(\\text{EEMT}) = \\begin{cases} f_1(\\text{EEMT}) &amp; \\text{if EEMT} &lt; 70 \\text{ MJ/m}^2\\text{/yr} \\\\ f_2(\\text{EEMT}) &amp; \\text{if EEMT} \\geq 70 \\text{ MJ/m}^2\\text{/yr} \\end{cases}\\]","path":["Scientific Background"],"tags":[]},{"location":"background/#model-validation","level":2,"title":"Model Validation","text":"","path":["Scientific Background"],"tags":[]},{"location":"background/#field-validation-studies","level":3,"title":"Field Validation Studies","text":"<p>EEMT has been validated against multiple field datasets:</p>","path":["Scientific Background"],"tags":[]},{"location":"background/#soil-properties","level":4,"title":"Soil Properties","text":"<ul> <li>Soil depth: r¬≤ = 0.77 for topographic EEMT</li> <li>Clay content: Significant correlation across climate gradients</li> <li>Organic matter: Strong relationship in temperate systems</li> <li>Chemical weathering: Linear correlation in humid environments</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#vegetation-properties","level":4,"title":"Vegetation Properties","text":"<ul> <li>Aboveground biomass: Power law relationship (r¬≤ = 0.98)</li> <li>Leaf area index: Moderate correlation in forest systems</li> <li>Net primary production: Good agreement with flux tower data</li> <li>Species composition: Predictive of functional groups</li> </ul>","path":["Scientific Background"],"tags":[]},{"location":"background/#geomorphic-properties","level":4,"title":"Geomorphic Properties","text":"<ul> <li>Erosion rates: Inverse relationship in high-EEMT systems</li> <li>Chemical denudation: Linear increase with EEMT</li> <li>Regolith thickness: Exponential relationship</li> <li>Landscape relief: Controls on maximum EEMT values</li> </ul> <p>This framework provides the scientific foundation for understanding how energy drives Critical Zone processes and enables quantitative prediction of landscape evolution and ecosystem function.</p>","path":["Scientific Background"],"tags":[]},{"location":"data-sources/","level":1,"title":"Data Sources for EEMT Calculations","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#overview","level":2,"title":"Overview","text":"<p>EEMT calculations require high-quality elevation and climate data. This guide covers accessing publicly available datasets optimized for Critical Zone analysis.</p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#quick-reference","level":2,"title":"Quick Reference","text":"Data Type Source Resolution Coverage API Access Elevation USGS 3DEP 1m, 10m, 30m United States ‚úÖ OpenTopography Variable Global ‚úÖ FABDEM 30m Global ‚ùå Climate DAYMET 1km daily North America ‚úÖ PRISM 800m monthly United States ‚úÖ GridMET 4km daily United States ‚úÖ Satellite Landsat 30m Global ‚úÖ MODIS 250m-1km Global ‚úÖ","path":["Data Sources"],"tags":[]},{"location":"data-sources/#elevation-data","level":2,"title":"Elevation Data","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#usgs-3dep-3d-elevation-program","level":3,"title":"USGS 3DEP (3D Elevation Program)","text":"<p>Best for: High-resolution analysis in the United States</p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#access-methods","level":4,"title":"Access Methods","text":"<p>1. National Map Downloader <pre><code># Direct download interface\nhttps://apps.nationalmap.gov/downloader/\n\n# Select area of interest, choose elevation products:\n# - 1m DEM (lidar-derived, best quality)\n# - 1/3 arc-second (~10m, good coverage) \n# - 1 arc-second (~30m, complete coverage)\n</code></pre></p> <p>2. USGS API Access <pre><code>import requests\nimport geopandas as gpd\n\ndef download_3dep(bbox, resolution='10m'):\n    \"\"\"\n    Download USGS 3DEP elevation data\n\n    Parameters:\n    bbox: [west, south, east, north] in decimal degrees\n    resolution: '1m', '10m', or '30m'\n    \"\"\"\n\n    base_url = \"https://cloud.sdsc.edu/v1/AUTH_opentopography/Raster/\"\n\n    if resolution == '1m':\n        dataset = \"USGS_LPC\"\n    elif resolution == '10m':  \n        dataset = \"USGS_NED_13\"\n    else:\n        dataset = \"USGS_NED_1\"\n\n    # Construct download URL\n    url = f\"{base_url}{dataset}?west={bbox[0]}&amp;south={bbox[1]}&amp;east={bbox[2]}&amp;north={bbox[3]}&amp;outputFormat=GTiff\"\n\n    return url\n\n# Example usage\narizona_bbox = [-111.5, 32.0, -110.5, 32.5]  \ndem_url = download_3dep(arizona_bbox, '10m')\n</code></pre></p> <p>3. Cloud-Optimized Access <pre><code>import rasterio\nfrom rasterio.session import AWSSession\n\n# Access 3DEP COGs on AWS Open Data\naws_session = AWSSession(profile_name='default')\nwith rasterio.Env(session=aws_session):\n    with rasterio.open('s3://prd-tnm/StagedProducts/Elevation/1m/Projects/...') as src:\n        dem_data = src.read(1)\n</code></pre></p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#opentopography","level":3,"title":"OpenTopography","text":"<p>Best for: Global coverage, lidar access, research applications</p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#web-interface","level":4,"title":"Web Interface","text":"<pre><code># OpenTopography Portal\nhttps://portal.opentopography.org/\n\n# Select dataset:\n# - SRTM GL1 (30m global)\n# - SRTM GL3 (90m global)  \n# - ALOS World 3D (30m global)\n# - Regional lidar datasets\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#api-access","level":4,"title":"API Access","text":"<pre><code>import requests\n\ndef download_opentopo(bbox, dem_type='SRTMGL1'):\n    \"\"\"\n    Download DEM from OpenTopography API\n\n    Parameters:\n    bbox: [west, south, east, north] in decimal degrees\n    dem_type: 'SRTMGL1', 'SRTMGL3', 'ALOS', 'COP30', 'COP90'\n    \"\"\"\n\n    base_url = \"https://cloud.sdsc.edu/v1/AUTH_opentopography/Raster/\"\n\n    params = {\n        'demtype': dem_type,\n        'west': bbox[0],\n        'south': bbox[1], \n        'east': bbox[2],\n        'north': bbox[3],\n        'outputFormat': 'GTiff'\n    }\n\n    response = requests.get(f\"{base_url}/API/globaldem\", params=params)\n\n    if response.status_code == 200:\n        with open(f'{dem_type}_dem.tif', 'wb') as f:\n            f.write(response.content)\n        return f'{dem_type}_dem.tif'\n    else:\n        raise Exception(f\"Download failed: {response.status_code}\")\n\n# Example usage\nbbox = [-111.5, 32.0, -110.5, 32.5]\ndem_file = download_opentopo(bbox, 'SRTMGL1')\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#lidar-point-cloud-access","level":4,"title":"Lidar Point Cloud Access","text":"<pre><code>import pdal\nimport json\n\n# Download lidar point cloud data\npipeline = {\n    \"pipeline\": [\n        {\n            \"type\": \"readers.ept\",\n            \"filename\": \"https://cloud.sdsc.edu/v1/AUTH_opentopography/PC/CA_FullState_2019/ept.json\",\n            \"bounds\": \"([-111.5, -111.0], [32.0, 32.5])\"\n        },\n        {\n            \"type\": \"writers.las\",\n            \"filename\": \"output.las\"\n        }\n    ]\n}\n\npdal_pipeline = pdal.Pipeline(json.dumps(pipeline))\npdal_pipeline.execute()\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#global-dems","level":3,"title":"Global DEMs","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#fabdem-forest-and-buildings-removed-dem","level":4,"title":"FABDEM (Forest And Buildings removed DEM)","text":"<pre><code># Access via Google Earth Engine\nimport ee\n\nee.Initialize()\n\n# Load FABDEM\nfabdem = ee.Image(\"projects/sat-io/open-datasets/FABDEM\")\n\n# Export specific region\ntask = ee.batch.Export.image.toDrive(\n    image=fabdem.select('elevation'),\n    description='FABDEM_export',\n    folder='EarthEngine',\n    scale=30,\n    region=ee.Geometry.Rectangle([-111.5, 32.0, -110.5, 32.5]),\n    crs='EPSG:4326'\n)\ntask.start()\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#copernicus-dem","level":4,"title":"Copernicus DEM","text":"<pre><code># Download via Copernicus Data Space\n# https://dataspace.copernicus.eu/\n\n# Available resolutions:\n# - COP-DEM 30m (global)\n# - COP-DEM 90m (global)\n\n# API access requires registration\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#climate-data","level":2,"title":"Climate Data","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#daymet-daily-surface-weather-data","level":3,"title":"DAYMET (Daily Surface Weather Data)","text":"<p>Best for: High-resolution daily meteorology across North America</p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#variables-available","level":4,"title":"Variables Available","text":"<ul> <li>tmin: Daily minimum temperature (¬∞C)</li> <li>tmax: Daily maximum temperature (¬∞C)  </li> <li>prcp: Daily precipitation (mm/day)</li> <li>srad: Shortwave radiation (W/m¬≤)</li> <li>vp: Water vapor pressure (Pa)</li> <li>swe: Snow water equivalent (kg/m¬≤)</li> <li>dayl: Day length (s/day)</li> </ul>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#api-access_1","level":4,"title":"API Access","text":"<pre><code>import requests\nimport xarray as xr\nfrom datetime import datetime\n\ndef download_daymet(lat, lon, start_year, end_year, variables=['tmin', 'tmax', 'prcp']):\n    \"\"\"\n    Download DAYMET data for point location\n\n    Parameters:\n    lat, lon: coordinates in decimal degrees\n    start_year, end_year: year range\n    variables: list of variable names\n    \"\"\"\n\n    base_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n\n    params = {\n        'lat': lat,\n        'lon': lon,\n        'vars': ','.join(variables),\n        'start': start_year,\n        'end': end_year,\n        'format': 'json'\n    }\n\n    response = requests.get(base_url, params=params)\n    data = response.json()\n\n    return data\n\n# Example usage\ntucson_data = download_daymet(32.25, -110.97, 2015, 2020, \n                            ['tmin', 'tmax', 'prcp', 'vp'])\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#spatial-data-access","level":4,"title":"Spatial Data Access","text":"<pre><code>def download_daymet_spatial(bbox, year, variable='tmin'):\n    \"\"\"\n    Download DAYMET spatial data via THREDDS\n\n    Parameters:\n    bbox: [west, south, east, north]\n    year: data year\n    variable: 'tmin', 'tmax', 'prcp', 'srad', 'vp', 'swe', 'dayl'\n    \"\"\"\n\n    base_url = \"https://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1328\"\n    nc_url = f\"{base_url}/daymet_v4_daily_na_{variable}_{year}.nc\"\n\n    # Open with xarray\n    ds = xr.open_dataset(nc_url)\n\n    # Subset to bounding box\n    subset = ds.sel(\n        x=slice(bbox[0], bbox[2]),\n        y=slice(bbox[1], bbox[3])\n    )\n\n    return subset\n\n# Example usage\nbbox = [-111.5, 32.0, -110.5, 32.5]\ntemp_data = download_daymet_spatial(bbox, 2020, 'tmin')\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#bulk-download-script","level":4,"title":"Bulk Download Script","text":"<pre><code>import os\nfrom concurrent.futures import ThreadPoolExecutor\nimport subprocess\n\ndef download_daymet_bulk(bbox, years, variables, output_dir):\n    \"\"\"\n    Bulk download DAYMET data using wget\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    def download_file(url, filename):\n        cmd = f\"wget -O {output_dir}/{filename} '{url}'\"\n        subprocess.run(cmd, shell=True)\n\n    download_tasks = []\n\n    for year in years:\n        for var in variables:\n            url = f\"https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1328/daymet_v4_daily_na_{var}_{year}.nc\"\n            filename = f\"daymet_{var}_{year}.nc\"\n            download_tasks.append((url, filename))\n\n    # Parallel downloads\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        for url, filename in download_tasks:\n            executor.submit(download_file, url, filename)\n\n# Example usage  \nyears = range(2015, 2021)\nvariables = ['tmin', 'tmax', 'prcp', 'vp']\ndownload_daymet_bulk(bbox, years, variables, 'daymet_data')\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#prism-parameter-elevation-regressions-on-independent-slopes-model","level":3,"title":"PRISM (Parameter-elevation Regressions on Independent Slopes Model)","text":"<p>Best for: High-resolution monthly climate normals for the United States</p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#access-methods_1","level":4,"title":"Access Methods","text":"<pre><code>import requests\nfrom bs4 import BeautifulSoup\n\ndef download_prism(bbox, year, variable='ppt', temporal='monthly'):\n    \"\"\"\n    Download PRISM data\n\n    Parameters:\n    bbox: [west, south, east, north]\n    year: data year or 'normals' for 30-year averages\n    variable: 'ppt', 'tmin', 'tmax', 'tmean', 'tdmean', 'vpdmin', 'vpdmax'\n    temporal: 'monthly', 'daily', 'annual'\n    \"\"\"\n\n    if year == 'normals':\n        base_url = \"https://prism.oregonstate.edu/normals/\"\n    else:\n        base_url = \"https://prism.oregonstate.edu/recent_years/\"\n\n    # PRISM requires spatial subsetting after download\n    # Full datasets available at: ftp://prism.oregonstate.edu/\n\n    print(f\"PRISM data download instructions:\")\n    print(f\"1. Visit: {base_url}\")\n    print(f\"2. Download {variable} data for {year}\")\n    print(f\"3. Use gdal_translate to subset:\")\n    print(f\"   gdal_translate -projwin {bbox[0]} {bbox[3]} {bbox[2]} {bbox[1]} input.bil output.tif\")\n\n# Example usage\ndownload_prism([-111.5, 32.0, -110.5, 32.5], 2020, 'ppt')\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#gridmet","level":3,"title":"GridMET","text":"<p>Best for: Daily meteorological data with broader spatial coverage than DAYMET</p> <pre><code>import xarray as xr\n\ndef download_gridmet(bbox, year, variable='pr'):\n    \"\"\"\n    Download GridMET data\n\n    Parameters:  \n    bbox: [west, south, east, north]\n    year: data year\n    variable: 'pr', 'tmmn', 'tmmx', 'rmax', 'rmin', 'vs', 'th', 'pet', 'erc', 'bi', 'fm100', 'fm1000'\n    \"\"\"\n\n    base_url = \"http://thredds.northwestknowledge.net:8080/thredds/dodsC/MET\"\n    nc_url = f\"{base_url}/{variable}/{variable}_{year}.nc\"\n\n    # Open dataset\n    ds = xr.open_dataset(nc_url)\n\n    # Subset to bounding box\n    subset = ds.sel(\n        lon=slice(bbox[0], bbox[2]),\n        lat=slice(bbox[1], bbox[3])\n    )\n\n    return subset\n\n# Example usage\nbbox = [-111.5, 32.0, -110.5, 32.5]\nprecip_data = download_gridmet(bbox, 2020, 'pr')\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#satellite-data","level":2,"title":"Satellite Data","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#landsat-vegetation-indices","level":3,"title":"Landsat (Vegetation Indices)","text":"<p>Best for: Long-term vegetation monitoring, NDVI calculation</p>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#google-earth-engine-access","level":4,"title":"Google Earth Engine Access","text":"<pre><code>import ee\n\nee.Initialize()\n\ndef get_landsat_collection(bbox, start_date, end_date, cloud_cover=20):\n    \"\"\"\n    Get Landsat collection with cloud masking\n    \"\"\"\n\n    # Define area of interest\n    aoi = ee.Geometry.Rectangle(bbox)\n\n    # Get Landsat 8 collection\n    collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n        .filterBounds(aoi) \\\n        .filterDate(start_date, end_date) \\\n        .filter(ee.Filter.lt('CLOUD_COVER', cloud_cover))\n\n    def mask_clouds(image):\n        # Cloud mask using QA_PIXEL band\n        qa = image.select('QA_PIXEL')\n        cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 3).eq(0)  # Cloud bit\n        return image.updateMask(cloud_mask)\n\n    # Apply cloud masking\n    masked_collection = collection.map(mask_clouds)\n\n    return masked_collection\n\ndef calculate_ndvi(image):\n    \"\"\"Calculate NDVI from Landsat image\"\"\"\n    ndvi = image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\n# Example usage\nbbox = [-111.5, 32.0, -110.5, 32.5]\ncollection = get_landsat_collection(bbox, '2020-01-01', '2020-12-31')\nndvi_collection = collection.map(calculate_ndvi)\n\n# Get median NDVI for year\nmedian_ndvi = ndvi_collection.select('NDVI').median()\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#direct-archive-access","level":4,"title":"Direct Archive Access","text":"<pre><code>import pystac_client\nimport rioxarray\n\n# Access Landsat via STAC\ncatalog = pystac_client.Client.open(\"https://landsatlook.usgs.gov/stac-server\")\n\n# Search for Landsat scenes\nsearch = catalog.search(\n    collections=[\"landsat-c2l2-sr\"],\n    bbox=[-111.5, 32.0, -110.5, 32.5],\n    datetime=\"2020-06-01/2020-08-31\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\n# Download and process\nitems = list(search.get_items())\nfor item in items[:5]:  # First 5 scenes\n    red_asset = item.assets['red']\n    nir_asset = item.assets['nir08']\n\n    # Load bands\n    red = rioxarray.open_rasterio(red_asset.href, chunks=True)\n    nir = rioxarray.open_rasterio(nir_asset.href, chunks=True)\n\n    # Calculate NDVI\n    ndvi = (nir - red) / (nir + red)\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#modis-products","level":3,"title":"MODIS Products","text":"<p>Best for: Global vegetation monitoring, LAI/FPAR products</p> <pre><code>import requests\nfrom pyhdf.SD import SD, SDC\nimport numpy as np\n\ndef download_modis_lai(bbox, year, product='MYD15A3H'):\n    \"\"\"\n    Download MODIS LAI product\n\n    Parameters:\n    bbox: [west, south, east, north] \n    year: data year\n    product: 'MOD15A3H' (Terra) or 'MYD15A3H' (Aqua)\n    \"\"\"\n\n    # MODIS data requires registration at:\n    # https://urs.earthdata.nasa.gov/\n\n    base_url = \"https://e4ftl01.cr.usgs.gov/MOLA/\"\n\n    # Example for automated access (requires authentication)\n    print(f\"MODIS {product} download:\")\n    print(f\"1. Register at: https://urs.earthdata.nasa.gov/\")\n    print(f\"2. Use wget with credentials:\")\n    print(f\"   wget --user=USERNAME --password=PASSWORD {base_url}{product}\")\n    print(f\"3. Process HDF files to extract LAI layer\")\n\n# Process MODIS HDF file\ndef extract_modis_lai(hdf_file):\n    \"\"\"Extract LAI from MODIS HDF file\"\"\"\n\n    # Open HDF file\n    hdf = SD(hdf_file, SDC.READ)\n\n    # Get LAI dataset\n    lai_dataset = hdf.select('Lai_500m')\n    lai_data = lai_dataset.get()\n\n    # Apply scale factor and fill values\n    scale_factor = lai_dataset.attributes()['scale_factor']\n    fill_value = lai_dataset.attributes()['_FillValue']\n\n    lai_data = lai_data.astype(np.float32)\n    lai_data[lai_data == fill_value] = np.nan\n    lai_data = lai_data * scale_factor\n\n    return lai_data\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#data-integration-workflows","level":2,"title":"Data Integration Workflows","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#automated-climate-data-download","level":3,"title":"Automated Climate Data Download","text":"<pre><code>import os\nimport subprocess\nfrom datetime import datetime, timedelta\n\nclass ClimateDataManager:\n    \"\"\"Automated climate data download and processing\"\"\"\n\n    def __init__(self, study_area, output_dir):\n        self.bbox = study_area  # [west, south, east, north]\n        self.output_dir = output_dir\n        os.makedirs(output_dir, exist_ok=True)\n\n    def download_daymet_timeseries(self, start_year, end_year, variables):\n        \"\"\"Download DAYMET time series for study area\"\"\"\n\n        for year in range(start_year, end_year + 1):\n            for var in variables:\n                url = f\"https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1328/daymet_v4_daily_na_{var}_{year}.nc\"\n                output_file = f\"{self.output_dir}/daymet_{var}_{year}.nc\"\n\n                if not os.path.exists(output_file):\n                    print(f\"Downloading {var} for {year}...\")\n                    subprocess.run([\n                        'wget', '-O', output_file, url\n                    ], check=True)\n\n    def process_to_monthly(self, variables):\n        \"\"\"Convert daily DAYMET to monthly averages\"\"\"\n\n        for var in variables:\n            # Use CDO (Climate Data Operators) if available\n            input_pattern = f\"{self.output_dir}/daymet_{var}_*.nc\"\n            output_file = f\"{self.output_dir}/daymet_{var}_monthly.nc\"\n\n            cmd = f\"cdo -monmean -mergetime {input_pattern} {output_file}\"\n            print(f\"Processing {var} to monthly: {cmd}\")\n            # subprocess.run(cmd, shell=True, check=True)\n\n# Example usage\nstudy_area = [-111.5, 32.0, -110.5, 32.5]  # Arizona region\nclimate_manager = ClimateDataManager(study_area, 'climate_data')\n\n# Download 5 years of data\nclimate_manager.download_daymet_timeseries(2015, 2020, ['tmin', 'tmax', 'prcp', 'vp'])\nclimate_manager.process_to_monthly(['tmin', 'tmax', 'prcp', 'vp'])\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#data-quality-control","level":3,"title":"Data Quality Control","text":"<pre><code>import rasterio\nimport numpy as np\nfrom rasterio.mask import mask\nimport geopandas as gpd\n\ndef validate_dataset(filepath, data_type='elevation'):\n    \"\"\"Validate downloaded dataset for quality issues\"\"\"\n\n    with rasterio.open(filepath) as src:\n        data = src.read(1)\n        profile = src.profile\n\n        results = {\n            'file': filepath,\n            'crs': profile['crs'],\n            'shape': data.shape,\n            'dtype': profile['dtype'],\n            'nodata': profile.get('nodata'),\n            'valid_pixels': np.sum(~np.isnan(data)),\n            'coverage_pct': np.sum(~np.isnan(data)) / data.size * 100\n        }\n\n        if data_type == 'elevation':\n            results.update({\n                'elevation_min': np.nanmin(data),\n                'elevation_max': np.nanmax(data),\n                'elevation_mean': np.nanmean(data),\n                'has_negative_elev': np.any(data &lt; -500),  # Flag suspicious values\n            })\n        elif data_type == 'temperature':\n            results.update({\n                'temp_min': np.nanmin(data),\n                'temp_max': np.nanmax(data), \n                'temp_mean': np.nanmean(data),\n                'realistic_range': np.all((data &gt;= -50) &amp; (data &lt;= 60))  # Celsius range\n            })\n        elif data_type == 'precipitation':\n            results.update({\n                'precip_min': np.nanmin(data),\n                'precip_max': np.nanmax(data),\n                'precip_mean': np.nanmean(data),\n                'no_negative': np.all(data &gt;= 0)  # Precipitation can't be negative\n            })\n\n        return results\n\n# Validate all datasets\ndatasets = [\n    ('dem.tif', 'elevation'),\n    ('temperature.tif', 'temperature'), \n    ('precipitation.tif', 'precipitation')\n]\n\nfor filepath, data_type in datasets:\n    if os.path.exists(filepath):\n        validation = validate_dataset(filepath, data_type)\n        print(f\"\\n{filepath} validation:\")\n        for key, value in validation.items():\n            print(f\"  {key}: {value}\")\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#best-practices","level":2,"title":"Best Practices","text":"","path":["Data Sources"],"tags":[]},{"location":"data-sources/#data-selection-guidelines","level":3,"title":"Data Selection Guidelines","text":"<ol> <li>Match spatial resolutions - Use consistent pixel sizes across datasets</li> <li>Align temporal periods - Ensure climate data covers the same time period  </li> <li>Check coordinate systems - Reproject data to common CRS if needed</li> <li>Validate extents - Confirm all datasets cover your study area completely</li> </ol>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#storage-and-organization","level":3,"title":"Storage and Organization","text":"<pre><code>project/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ elevation/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Original downloads\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processed/     # Reprojected, clipped\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata/      # Data provenance\n‚îÇ   ‚îú‚îÄ‚îÄ climate/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ daymet/        # Daily meteorology  \n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prism/         # Monthly normals\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processed/     # Analysis-ready\n‚îÇ   ‚îî‚îÄ‚îÄ satellite/\n‚îÇ       ‚îú‚îÄ‚îÄ landsat/       # Vegetation indices\n‚îÇ       ‚îî‚îÄ‚îÄ modis/         # LAI products\n‚îú‚îÄ‚îÄ scripts/               # Download automation\n‚îî‚îÄ‚îÄ docs/                  # Data documentation\n</code></pre>","path":["Data Sources"],"tags":[]},{"location":"data-sources/#performance-tips","level":3,"title":"Performance Tips","text":"<ul> <li>Use parallel downloads for large datasets</li> <li>Compress intermediate files (LZW compression) </li> <li>Tile large rasters for memory-efficient processing</li> <li>Cache frequently used data locally</li> <li>Document data provenance for reproducibility</li> </ul> <p>Next: GRASS GIS Setup and Configuration</p>","path":["Data Sources"],"tags":[]},{"location":"development/","level":1,"title":"Development Guide","text":"","path":["Development"],"tags":[]},{"location":"development/#contributing-to-eemt","level":2,"title":"Contributing to EEMT","text":"<p>We welcome contributions to improve EEMT calculations, add new features, and extend documentation. This guide covers the development process and technical architecture.</p>","path":["Development"],"tags":[]},{"location":"development/#quick-start-for-developers","level":2,"title":"Quick Start for Developers","text":"","path":["Development"],"tags":[]},{"location":"development/#1-fork-and-clone","level":3,"title":"1. Fork and Clone","text":"<pre><code>git clone https://github.com/your-username/eemt.git\ncd eemt\ngit checkout 2020_update\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#2-set-up-development-environment","level":3,"title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\npython -m venv eemt-dev\nsource eemt-dev/bin/activate\n\n# Install development dependencies\npip install -r requirements.txt\npip install -e .\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#3-run-tests","level":3,"title":"3. Run Tests","text":"<pre><code># Unit tests\npytest tests/\n\n# Integration tests  \nbash tests/integration_tests.sh\n\n# Documentation tests\nmkdocs serve\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#development-workflow","level":2,"title":"Development Workflow","text":"","path":["Development"],"tags":[]},{"location":"development/#branch-strategy","level":3,"title":"Branch Strategy","text":"<ul> <li>master: Stable releases</li> <li>2020_update: Active development branch</li> <li>feature/*: New feature development</li> <li>hotfix/*: Critical bug fixes</li> </ul>","path":["Development"],"tags":[]},{"location":"development/#making-changes","level":3,"title":"Making Changes","text":"<ol> <li> <p>Create feature branch <pre><code>git checkout -b feature/new-climate-data-source\n</code></pre></p> </li> <li> <p>Make changes with tests <pre><code># Add new functionality\ndef new_climate_function():\n    pass\n\n# Add corresponding tests\ndef test_new_climate_function():\n    pass\n</code></pre></p> </li> <li> <p>Update documentation <pre><code>## New Climate Data Source\n\nDescription of new functionality...\n</code></pre></p> </li> <li> <p>Submit pull request</p> </li> <li>Include clear description</li> <li>Reference any related issues</li> <li>Ensure tests pass</li> </ol>","path":["Development"],"tags":[]},{"location":"development/#code-standards","level":2,"title":"Code Standards","text":"","path":["Development"],"tags":[]},{"location":"development/#python-style","level":3,"title":"Python Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use type hints for function signatures</li> <li>Maximum line length: 88 characters</li> <li>Use Black for code formatting</li> </ul> <pre><code>from typing import Optional, Tuple\nimport numpy as np\n\ndef calculate_eemt(\n    temperature: np.ndarray, \n    precipitation: np.ndarray,\n    elevation: Optional[np.ndarray] = None\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate EEMT with proper typing.\"\"\"\n    pass\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#documentation-standards","level":3,"title":"Documentation Standards","text":"<ul> <li>Use Google-style docstrings</li> <li>Include examples in docstrings</li> <li>Update relevant documentation pages</li> <li>Add entries to changelog</li> </ul>","path":["Development"],"tags":[]},{"location":"development/#testing-requirements","level":3,"title":"Testing Requirements","text":"<ul> <li>Unit tests for all new functions</li> <li>Integration tests for workflows</li> <li>Performance benchmarks for algorithms</li> <li>Validation against scientific literature</li> </ul>","path":["Development"],"tags":[]},{"location":"development/#architecture-overview","level":2,"title":"Architecture Overview","text":"","path":["Development"],"tags":[]},{"location":"development/#core-components","level":3,"title":"Core Components","text":"<pre><code>graph TD\n    A[Data Sources] --&gt; B[Preprocessing]\n    B --&gt; C[GRASS GIS Processing]\n    C --&gt; D[EEMT Calculations]\n    D --&gt; E[Output &amp; Validation]\n\n    A1[DAYMET API] --&gt; A\n    A2[USGS 3DEP] --&gt; A\n    A3[OpenTopography] --&gt; A\n    A4[Satellite Data] --&gt; A\n\n    C1[r.sun Solar] --&gt; C\n    C2[r.slope.aspect] --&gt; C\n    C3[r.watershed] --&gt; C\n\n    D1[Traditional EEMT] --&gt; D\n    D2[Topographic EEMT] --&gt; D\n    D3[Vegetation EEMT] --&gt; D</code></pre>","path":["Development"],"tags":[]},{"location":"development/#module-structure","level":3,"title":"Module Structure","text":"<pre><code>eemt/\n‚îú‚îÄ‚îÄ core/\n‚îÇ   ‚îú‚îÄ‚îÄ calculations.py     # Core EEMT functions\n‚îÇ   ‚îú‚îÄ‚îÄ constants.py        # Physical constants\n‚îÇ   ‚îî‚îÄ‚îÄ validation.py       # Result validation\n‚îú‚îÄ‚îÄ data_sources/\n‚îÇ   ‚îú‚îÄ‚îÄ elevation.py        # DEM access functions\n‚îÇ   ‚îú‚îÄ‚îÄ climate.py          # Climate data APIs\n‚îÇ   ‚îî‚îÄ‚îÄ satellite.py        # Remote sensing data\n‚îú‚îÄ‚îÄ grass_interface/\n‚îÇ   ‚îú‚îÄ‚îÄ solar.py           # r.sun wrapper functions\n‚îÇ   ‚îú‚îÄ‚îÄ terrain.py         # Terrain analysis\n‚îÇ   ‚îî‚îÄ‚îÄ parallel.py        # Parallel processing\n‚îú‚îÄ‚îÄ workflows/\n‚îÇ   ‚îú‚îÄ‚îÄ traditional.py     # Traditional EEMT\n‚îÇ   ‚îú‚îÄ‚îÄ topographic.py     # Topographic EEMT\n‚îÇ   ‚îî‚îÄ‚îÄ vegetation.py      # Vegetation EEMT\n‚îî‚îÄ‚îÄ utils/\n    ‚îú‚îÄ‚îÄ io.py              # File I/O utilities\n    ‚îú‚îÄ‚îÄ projections.py     # CRS handling\n    ‚îî‚îÄ‚îÄ performance.py     # Performance monitoring\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#performance-considerations","level":2,"title":"Performance Considerations","text":"","path":["Development"],"tags":[]},{"location":"development/#optimization-guidelines","level":3,"title":"Optimization Guidelines","text":"<ol> <li>Memory Management</li> <li>Process large datasets in chunks</li> <li>Use memory-mapped arrays when possible</li> <li> <p>Clean up temporary files promptly</p> </li> <li> <p>Parallel Processing</p> </li> <li>Maximize CPU utilization with multiprocessing</li> <li>Use thread-safe operations</li> <li> <p>Balance memory vs. speed tradeoffs</p> </li> <li> <p>I/O Optimization</p> </li> <li>Use compressed file formats (LZW, DEFLATE)</li> <li>Implement caching for repeated operations</li> <li>Minimize network requests</li> </ol>","path":["Development"],"tags":[]},{"location":"development/#benchmarking","level":3,"title":"Benchmarking","text":"<pre><code>import time\nimport psutil\nfrom functools import wraps\n\ndef benchmark(func):\n    \"\"\"Decorator for performance benchmarking\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        start_memory = psutil.Process().memory_info().rss\n\n        result = func(*args, **kwargs)\n\n        end_time = time.time()\n        end_memory = psutil.Process().memory_info().rss\n\n        print(f\"{func.__name__} performance:\")\n        print(f\"  Time: {end_time - start_time:.2f} seconds\")\n        print(f\"  Memory: {(end_memory - start_memory) / 1024**2:.1f} MB\")\n\n        return result\n    return wrapper\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#testing-framework","level":2,"title":"Testing Framework","text":"","path":["Development"],"tags":[]},{"location":"development/#test-structure","level":3,"title":"Test Structure","text":"<pre><code>tests/\n‚îú‚îÄ‚îÄ unit/\n‚îÇ   ‚îú‚îÄ‚îÄ test_calculations.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_data_sources.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_grass_interface.py\n‚îú‚îÄ‚îÄ integration/\n‚îÇ   ‚îú‚îÄ‚îÄ test_workflows.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_end_to_end.py\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ sample_dem.tif\n‚îÇ   ‚îî‚îÄ‚îÄ sample_climate.nc\n‚îî‚îÄ‚îÄ conftest.py\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#example-test","level":3,"title":"Example Test","text":"<pre><code>import pytest\nimport numpy as np\nfrom eemt.core.calculations import calculate_eemt_traditional\n\ndef test_eemt_traditional_basic():\n    \"\"\"Test basic traditional EEMT calculation\"\"\"\n\n    # Test data\n    temp = np.array([15.0, 20.0, 25.0])  # ¬∞C\n    precip = np.array([50.0, 40.0, 30.0])  # mm\n\n    # Calculate EEMT\n    eemt, e_bio, e_ppt = calculate_eemt_traditional(temp, precip)\n\n    # Assertions\n    assert isinstance(eemt, np.ndarray)\n    assert np.all(eemt &gt;= 0), \"EEMT should be non-negative\"\n    assert np.all(e_bio &gt;= 0), \"E_BIO should be non-negative\"\n    assert np.all(e_ppt &gt;= 0), \"E_PPT should be non-negative\"\n    assert np.allclose(eemt, e_bio + e_ppt), \"EEMT should equal sum of components\"\n\n@pytest.mark.integration\ndef test_complete_workflow():\n    \"\"\"Test complete EEMT workflow with sample data\"\"\"\n\n    from eemt.workflows import run_complete_eemt_pipeline\n\n    # Use test data\n    result = run_complete_eemt_pipeline(\n        dem_file='tests/data/sample_dem.tif',\n        climate_dir='tests/data/climate/',\n        output_dir='tests/output/',\n        start_year=2015,\n        end_year=2015  # Single year for speed\n    )\n\n    assert 'traditional' in result\n    assert np.any(~np.isnan(result['traditional']))\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#documentation-development","level":2,"title":"Documentation Development","text":"","path":["Development"],"tags":[]},{"location":"development/#mkdocs-setup","level":3,"title":"MkDocs Setup","text":"<pre><code># Serve documentation locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#writing-guidelines","level":3,"title":"Writing Guidelines","text":"<ol> <li>Structure: Use clear headings and subheadings</li> <li>Examples: Include working code examples</li> <li>Math: Use MathJax for equations</li> <li>Images: Optimize for web display</li> <li>Links: Use relative links within documentation</li> </ol>","path":["Development"],"tags":[]},{"location":"development/#documentation-testing","level":3,"title":"Documentation Testing","text":"<pre><code># Check for broken links\npytest docs/test_docs.py\n\n# Validate markdown syntax\nmarkdownlint docs/\n\n# Check MkDocs configuration\nmkdocs build --strict\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#release-process","level":2,"title":"Release Process","text":"","path":["Development"],"tags":[]},{"location":"development/#version-numbering","level":3,"title":"Version Numbering","text":"<ul> <li>Major: Breaking API changes (2.0.0)</li> <li>Minor: New features, backwards compatible (2.1.0)</li> <li>Patch: Bug fixes (2.1.1)</li> </ul>","path":["Development"],"tags":[]},{"location":"development/#release-checklist","level":3,"title":"Release Checklist","text":"<ol> <li> <p>Update version numbers <pre><code># eemt/__init__.py\n__version__ = \"2.1.0\"\n</code></pre></p> </li> <li> <p>Update changelog <pre><code>## [2.1.0] - 2025-01-15\n### Added\n- New climate data source integration\n### Fixed\n- Memory optimization for large datasets\n</code></pre></p> </li> <li> <p>Run full test suite <pre><code>pytest tests/ --cov=eemt\n</code></pre></p> </li> <li> <p>Build and test documentation <pre><code>mkdocs build --strict\n</code></pre></p> </li> <li> <p>Create release tag <pre><code>git tag -a v2.1.0 -m \"Release version 2.1.0\"\ngit push origin v2.1.0\n</code></pre></p> </li> </ol>","path":["Development"],"tags":[]},{"location":"development/#getting-help","level":2,"title":"Getting Help","text":"","path":["Development"],"tags":[]},{"location":"development/#development-support","level":3,"title":"Development Support","text":"<ul> <li>GitHub Issues: Technical questions and bug reports</li> <li>GitHub Discussions: Design discussions and feature requests</li> <li>Email: Direct contact with maintainers</li> </ul>","path":["Development"],"tags":[]},{"location":"development/#code-review-process","level":3,"title":"Code Review Process","text":"<ul> <li>All changes require review by core maintainers</li> <li>Automated testing must pass</li> <li>Documentation must be updated</li> <li>Performance impacts should be evaluated</li> </ul> <p>Ready to contribute? Check the sections above for detailed development instructions.</p>","path":["Development"],"tags":[]},{"location":"development/setup/","level":1,"title":"Development Environment Setup","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#overview","level":2,"title":"Overview","text":"<p>This guide helps you set up a complete development environment for contributing to EEMT. It covers local setup, testing frameworks, debugging tools, and best practices.</p>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#prerequisites","level":2,"title":"Prerequisites","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#required-software","level":3,"title":"Required Software","text":"<ul> <li>Git: Version control (2.30+)</li> <li>Python: 3.12 or later</li> <li>Docker: 20.10+ and Docker Compose 2.0+</li> <li>Make: Build automation</li> <li>VS Code or PyCharm: Recommended IDEs</li> </ul>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#optional-but-recommended","level":3,"title":"Optional but Recommended","text":"<ul> <li>GRASS GIS: 8.4+ for direct testing</li> <li>QGIS: 3.34 LTR for visualization</li> <li>Jupyter: For interactive development</li> <li>Pre-commit: For code quality checks</li> </ul>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#repository-setup","level":2,"title":"Repository Setup","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-fork-and-clone","level":3,"title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub first, then:\ngit clone https://github.com/YOUR_USERNAME/eemt.git\ncd eemt\n\n# Add upstream remote\ngit remote add upstream https://github.com/cyverse-gis/eemt.git\n\n# Check out development branch\ngit checkout 2020_update\ngit pull upstream 2020_update\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-branch-strategy","level":3,"title":"2. Branch Strategy","text":"<pre><code># Create feature branch\ngit checkout -b feature/your-feature-name\n\n# Create bugfix branch\ngit checkout -b bugfix/issue-number-description\n\n# Create documentation branch\ngit checkout -b docs/update-section-name\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#python-development-environment","level":2,"title":"Python Development Environment","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-virtual-environment-setup","level":3,"title":"1. Virtual Environment Setup","text":"<pre><code># Create virtual environment\npython3 -m venv .venv\n\n# Activate environment\nsource .venv/bin/activate  # Linux/macOS\n# .venv\\Scripts\\activate    # Windows\n\n# Upgrade pip\npip install --upgrade pip setuptools wheel\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-install-dependencies","level":3,"title":"2. Install Dependencies","text":"<pre><code># Install runtime dependencies\npip install -r requirements.txt\n\n# Install development dependencies\npip install -r requirements-dev.txt\n\n# Install package in editable mode\npip install -e .\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#3-development-dependencies","level":3,"title":"3. Development Dependencies","text":"<p>Create <code>requirements-dev.txt</code>:</p> <pre><code># Testing\npytest&gt;=7.4.0\npytest-cov&gt;=4.1.0\npytest-asyncio&gt;=0.21.0\npytest-mock&gt;=3.11.0\npytest-timeout&gt;=2.1.0\n\n# Code Quality\nblack&gt;=23.0.0\nflake8&gt;=6.0.0\nmypy&gt;=1.5.0\npylint&gt;=2.17.0\nisort&gt;=5.12.0\npre-commit&gt;=3.3.0\n\n# Documentation\nsphinx&gt;=7.0.0\nsphinx-rtd-theme&gt;=1.3.0\nsphinx-autodoc-typehints&gt;=1.24.0\nmkdocs&gt;=1.5.0\nmkdocs-material&gt;=9.4.0\n\n# Debugging\nipdb&gt;=0.13.0\nipython&gt;=8.14.0\n\n# Performance\nline-profiler&gt;=4.1.0\nmemory-profiler&gt;=0.61.0\npy-spy&gt;=0.3.14\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#docker-development-environment","level":2,"title":"Docker Development Environment","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-build-development-container","level":3,"title":"1. Build Development Container","text":"<pre><code># Build with development features\ndocker build -t eemt-dev -f docker/Dockerfile.dev .\n\n# Or use docker-compose\ndocker-compose -f docker-compose.dev.yml build\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-development-dockerfile","level":3,"title":"2. Development Dockerfile","text":"<p>Create <code>docker/Dockerfile.dev</code>:</p> <pre><code>FROM eemt:ubuntu24.04\n\n# Install development tools\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    vim \\\n    tmux \\\n    htop \\\n    gdb \\\n    valgrind \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python dev dependencies\nCOPY requirements-dev.txt /tmp/\nRUN pip install -r /tmp/requirements-dev.txt\n\n# Enable debugging\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV PYTHONASYNCIODEBUG=1\n\n# Mount source code as volume\nVOLUME /workspace\nWORKDIR /workspace\n\n# Install Jupyter for interactive development\nRUN pip install jupyterlab\n\n# Expose additional ports for debugging\nEXPOSE 5678 8888\n\nCMD [\"/bin/bash\"]\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#3-docker-compose-for-development","level":3,"title":"3. Docker Compose for Development","text":"<p>Create <code>docker-compose.dev.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-dev:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile.dev\n    image: eemt-dev\n    container_name: eemt-dev\n    volumes:\n      - .:/workspace\n      - grass-data:/grassdata\n      - cache:/cache\n    ports:\n      - \"5000:5000\"  # Web interface\n      - \"5678:5678\"  # Debugger\n      - \"8888:8888\"  # Jupyter\n    environment:\n      - PYTHONPATH=/workspace\n      - GRASS_ADDON_PATH=/workspace/grass-addons\n    command: /bin/bash\n    stdin_open: true\n    tty: true\n\nvolumes:\n  grass-data:\n  cache:\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#ide-configuration","level":2,"title":"IDE Configuration","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#vs-code","level":3,"title":"VS Code","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-extensions","level":4,"title":"1. Extensions","text":"<p>Install recommended extensions:</p> <pre><code>// .vscode/extensions.json\n{\n  \"recommendations\": [\n    \"ms-python.python\",\n    \"ms-python.vscode-pylance\",\n    \"ms-python.black-formatter\",\n    \"ms-python.isort\",\n    \"ms-python.flake8\",\n    \"ms-azuretools.vscode-docker\",\n    \"ms-vscode-remote.remote-containers\",\n    \"charliermarsh.ruff\",\n    \"tamasfe.even-better-toml\",\n    \"yzhang.markdown-all-in-one\"\n  ]\n}\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-settings","level":4,"title":"2. Settings","text":"<pre><code>// .vscode/settings.json\n{\n  \"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv/bin/python\",\n  \"python.linting.enabled\": true,\n  \"python.linting.pylintEnabled\": true,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"python.formatting.blackArgs\": [\"--line-length\", \"88\"],\n  \"python.sortImports.args\": [\"--profile\", \"black\"],\n  \"[python]\": {\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n      \"source.organizeImports\": true\n    }\n  },\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.unittestEnabled\": false,\n  \"python.testing.pytestArgs\": [\n    \"tests\",\n    \"-v\",\n    \"--cov=eemt\",\n    \"--cov-report=term-missing\"\n  ]\n}\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#3-launch-configuration","level":4,"title":"3. Launch Configuration","text":"<pre><code>// .vscode/launch.json\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Python: Current File\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"console\": \"integratedTerminal\",\n      \"justMyCode\": false\n    },\n    {\n      \"name\": \"Python: Solar Workflow\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/sol/sol/run-workflow\",\n      \"args\": [\n        \"--step\", \"15\",\n        \"--num_threads\", \"2\",\n        \"examples/mcn_10m.tif\"\n      ],\n      \"console\": \"integratedTerminal\"\n    },\n    {\n      \"name\": \"Python: Debug Tests\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"module\": \"pytest\",\n      \"args\": [\n        \"tests\",\n        \"-v\",\n        \"-s\"\n      ],\n      \"console\": \"integratedTerminal\"\n    },\n    {\n      \"name\": \"Docker: Attach to Container\",\n      \"type\": \"python\",\n      \"request\": \"attach\",\n      \"port\": 5678,\n      \"host\": \"localhost\",\n      \"pathMappings\": [\n        {\n          \"localRoot\": \"${workspaceFolder}\",\n          \"remoteRoot\": \"/workspace\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#pycharm","level":3,"title":"PyCharm","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-project-setup","level":4,"title":"1. Project Setup","text":"<pre><code># Open project\npycharm .\n\n# Configure interpreter\n# File &gt; Settings &gt; Project &gt; Python Interpreter\n# Add &gt; Existing Environment &gt; .venv/bin/python\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-run-configurations","level":4,"title":"2. Run Configurations","text":"<p>Create run configurations for: - Solar workflow - EEMT workflow - Test suite - Docker containers</p>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#testing-framework","level":2,"title":"Testing Framework","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-test-structure","level":3,"title":"1. Test Structure","text":"<pre><code>tests/\n‚îú‚îÄ‚îÄ unit/\n‚îÇ   ‚îú‚îÄ‚îÄ test_calculations.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_solar.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_climate.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îú‚îÄ‚îÄ integration/\n‚îÇ   ‚îú‚îÄ‚îÄ test_workflows.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_api.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_docker.py\n‚îú‚îÄ‚îÄ fixtures/\n‚îÇ   ‚îú‚îÄ‚îÄ sample_dem.tif\n‚îÇ   ‚îú‚îÄ‚îÄ climate_data.nc\n‚îÇ   ‚îî‚îÄ‚îÄ expected_outputs/\n‚îú‚îÄ‚îÄ conftest.py\n‚îî‚îÄ‚îÄ pytest.ini\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-writing-tests","level":3,"title":"2. Writing Tests","text":"<pre><code># tests/unit/test_calculations.py\nimport pytest\nimport numpy as np\nfrom eemt import calculations\n\nclass TestEEMTCalculations:\n    @pytest.fixture\n    def sample_data(self):\n        \"\"\"Provide sample climate data.\"\"\"\n        return {\n            'temperature': np.array([10, 15, 20, 18, 12, 8]),\n            'precipitation': np.array([50, 60, 45, 40, 55, 65])\n        }\n\n    def test_traditional_eemt(self, sample_data):\n        \"\"\"Test traditional EEMT calculation.\"\"\"\n        result = calculations.calculate_eemt_traditional(\n            temperature=sample_data['temperature'],\n            precipitation=sample_data['precipitation']\n        )\n\n        assert 'eemt' in result\n        assert 'e_bio' in result\n        assert 'e_ppt' in result\n        assert result['eemt'] &gt; 0\n        assert np.isfinite(result['eemt']).all()\n\n    @pytest.mark.parametrize(\"step\", [3, 15, 30])\n    def test_solar_time_steps(self, step):\n        \"\"\"Test solar calculations with different time steps.\"\"\"\n        # Test implementation\n        pass\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#3-running-tests","level":3,"title":"3. Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=eemt --cov-report=html\n\n# Run specific test file\npytest tests/unit/test_calculations.py\n\n# Run with verbose output\npytest -v -s\n\n# Run only marked tests\npytest -m \"not slow\"\n\n# Run in parallel\npytest -n auto\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#4-test-configuration","level":3,"title":"4. Test Configuration","text":"<pre><code># pytest.ini\n[tool:pytest]\nminversion = 7.0\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = \n    -ra\n    --strict-markers\n    --cov=eemt\n    --cov-branch\n    --cov-report=term-missing:skip-covered\n    --cov-report=html:htmlcov\n    --cov-report=xml\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: integration tests\n    docker: requires docker\n    gpu: requires GPU\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#code-quality-tools","level":2,"title":"Code Quality Tools","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-pre-commit-configuration","level":3,"title":"1. Pre-commit Configuration","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-json\n      - id: check-toml\n      - id: check-added-large-files\n      - id: check-merge-conflict\n\n  - repo: https://github.com/psf/black\n    rev: 23.7.0\n    hooks:\n      - id: black\n        language_version: python3.12\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 6.1.0\n    hooks:\n      - id: flake8\n        args: [\"--max-line-length=88\", \"--extend-ignore=E203\"]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.5.1\n    hooks:\n      - id: mypy\n        additional_dependencies: [types-requests]\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-code-formatting","level":3,"title":"2. Code Formatting","text":"<pre><code># Format code with Black\nblack eemt/ tests/\n\n# Sort imports\nisort eemt/ tests/\n\n# Check style\nflake8 eemt/ tests/\n\n# Type checking\nmypy eemt/\n\n# All at once with pre-commit\npre-commit run --all-files\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#debugging-techniques","level":2,"title":"Debugging Techniques","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-interactive-debugging","level":3,"title":"1. Interactive Debugging","text":"<pre><code># Using ipdb\nimport ipdb; ipdb.set_trace()\n\n# Using breakpoint() (Python 3.7+)\nbreakpoint()\n\n# Remote debugging with debugpy\nimport debugpy\ndebugpy.listen(5678)\ndebugpy.wait_for_client()\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-logging-configuration","level":3,"title":"2. Logging Configuration","text":"<pre><code># eemt/logging_config.py\nimport logging\nimport logging.config\n\nLOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'standard': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n        },\n        'detailed': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d: %(message)s'\n        }\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'level': 'DEBUG',\n            'formatter': 'standard',\n            'stream': 'ext://sys.stdout'\n        },\n        'file': {\n            'class': 'logging.FileHandler',\n            'level': 'DEBUG',\n            'formatter': 'detailed',\n            'filename': 'eemt_debug.log'\n        }\n    },\n    'loggers': {\n        'eemt': {\n            'level': 'DEBUG',\n            'handlers': ['console', 'file'],\n            'propagate': False\n        }\n    }\n}\n\ndef setup_logging():\n    logging.config.dictConfig(LOGGING_CONFIG)\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#3-performance-profiling","level":3,"title":"3. Performance Profiling","text":"<pre><code># Profile execution time\nimport cProfile\nimport pstats\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# Run your code\nworkflow.run()\n\nprofiler.disable()\nstats = pstats.Stats(profiler).sort_stats('cumulative')\nstats.print_stats(20)\n\n# Memory profiling\nfrom memory_profiler import profile\n\n@profile\ndef memory_intensive_function():\n    # Your code here\n    pass\n\n# Line profiling\nfrom line_profiler import LineProfiler\n\nlp = LineProfiler()\nlp_wrapper = lp(your_function)\nlp_wrapper()\nlp.print_stats()\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#continuous-integration","level":2,"title":"Continuous Integration","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#github-actions-workflow","level":3,"title":"GitHub Actions Workflow","text":"<pre><code># .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [ master, 2020_update ]\n  pull_request:\n    branches: [ master, 2020_update ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: ['3.10', '3.11', '3.12']\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y grass grass-dev gdal-bin\n\n    - name: Install Python dependencies\n      run: |\n        pip install --upgrade pip\n        pip install -r requirements.txt\n        pip install -r requirements-dev.txt\n        pip install -e .\n\n    - name: Run tests\n      run: |\n        pytest --cov=eemt --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#documentation-development","level":2,"title":"Documentation Development","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#1-building-documentation","level":3,"title":"1. Building Documentation","text":"<pre><code># MkDocs (current)\nmkdocs serve  # Live development server\nmkdocs build  # Build static site\n\n# Sphinx (for API docs)\ncd docs/sphinx\nmake html     # Build HTML docs\nmake livehtml # Auto-rebuild on changes\n</code></pre>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#2-writing-documentation","level":3,"title":"2. Writing Documentation","text":"<p>Follow these guidelines: - Use clear, concise language - Include code examples - Add diagrams where helpful - Cross-reference related sections - Keep navigation logical</p>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#troubleshooting-development-issues","level":2,"title":"Troubleshooting Development Issues","text":"","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#common-problems","level":3,"title":"Common Problems","text":"<ol> <li>Import Errors: Ensure PYTHONPATH includes project root</li> <li>GRASS Not Found: Check GISBASE environment variable</li> <li>Docker Build Fails: Clean Docker cache with <code>docker system prune</code></li> <li>Test Failures: Check fixture data and mock configurations</li> </ol>","path":["Development","Development Setup"],"tags":[]},{"location":"development/setup/#getting-help","level":3,"title":"Getting Help","text":"<ul> <li>Check existing issues</li> <li>Ask in discussions</li> <li>Review contribution guidelines</li> </ul> <p>Next: Contributing Guidelines | Testing Guide</p>","path":["Development","Development Setup"],"tags":[]},{"location":"distributed-deployment/","level":1,"title":"Distributed Deployment","text":"<p>The EEMT workflow manager supports distributed execution across multiple virtual machines, bare metal servers, HPC systems, and HTC environments using a master/worker architecture.</p>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#overview","level":2,"title":"Overview","text":"<p>The distributed deployment consists of:</p> <ul> <li>Master/Foreman Node: Coordinates workflow execution, manages job queue, and distributes tasks</li> <li>Worker Nodes: Execute individual workflow tasks using containerized environments</li> <li>Shared Storage: Optional shared filesystem for large datasets</li> </ul> <pre><code>graph TB\n    subgraph \"Master Node\"\n        WM[Workflow Manager]\n        WQ[Work Queue Master]\n        WI[Web Interface]\n        WM --&gt; WQ\n        WM --&gt; WI\n    end\n\n    subgraph \"Worker Nodes\"\n        W1[Worker 1&lt;br/&gt;VM/Server]\n        W2[Worker 2&lt;br/&gt;HPC Node]\n        W3[Worker 3&lt;br/&gt;Container]\n        W4[Worker N&lt;br/&gt;Bare Metal]\n    end\n\n    subgraph \"Storage\"\n        FS[Shared Filesystem&lt;br/&gt;NFS/CIFS/S3]\n        LF[Local Storage]\n    end\n\n    WQ -.-&gt; W1\n    WQ -.-&gt; W2\n    WQ -.-&gt; W3\n    WQ -.-&gt; W4\n\n    W1 --&gt; FS\n    W2 --&gt; LF\n    W3 --&gt; FS\n    W4 --&gt; LF</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#deployment-types","level":2,"title":"Deployment Types","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#1-multi-vm-deployment","level":3,"title":"1. Multi-VM Deployment","text":"<p>Use Case: Research clusters, cloud environments, multi-institutional collaboration</p> <p>Setup: - Master VM with web interface and workflow coordination - Multiple worker VMs with container runtime - Shared storage via NFS, CIFS, or cloud storage</p> <p>Benefits: - Easy scaling by adding VM instances - Isolated execution environments - Cloud-native deployments</p>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#2-hpc-integration","level":3,"title":"2. HPC Integration","text":"<p>Use Case: University HPC clusters, national computing facilities</p> <p>Setup: - Master node on login/head node - Workers on compute nodes via SLURM/PBS job submission - Shared filesystem (Lustre, GPFS, NFS)</p> <p>Benefits: - Leverage existing HPC infrastructure - High-performance interconnects - Large-scale parallel execution</p>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#3-htc-grid-computing","level":3,"title":"3. HTC Grid Computing","text":"<p>Use Case: Open Science Grid, campus grids, volunteer computing</p> <p>Setup: - Master on submit node - Workers on grid compute nodes - Container-based execution for portability</p> <p>Benefits: - Massive scale-out capability - Opportunistic resource usage - Heterogeneous resource pools</p>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#prerequisites","level":2,"title":"Prerequisites","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#master-node-requirements","level":3,"title":"Master Node Requirements","text":"<ul> <li>OS: Linux (Ubuntu 20.04+, CentOS 8+, RHEL 8+)</li> <li>CPU: 4+ cores</li> <li>Memory: 8GB+ RAM</li> <li>Disk: 100GB+ storage</li> <li>Network: Public IP or accessible from worker nodes</li> <li>Software:</li> <li>Docker or Podman</li> <li>CCTools (Makeflow + Work Queue)</li> <li>Python 3.8+</li> </ul>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#worker-node-requirements","level":3,"title":"Worker Node Requirements","text":"<ul> <li>OS: Linux with container runtime OR CCTools installed</li> <li>CPU: 4+ cores (configurable)</li> <li>Memory: 8GB+ RAM (configurable)</li> <li>Disk: 50GB+ temporary storage (configurable)</li> <li>Network: Access to master node</li> <li>Software:</li> <li>Docker/Podman OR Singularity/Apptainer</li> <li>CCTools work_queue_worker</li> <li>Optional: EEMT container image (for offline environments)</li> </ul>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#installation","level":2,"title":"Installation","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#docker-based-deployment-recommended","level":3,"title":"Docker-Based Deployment (Recommended)","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#quick-start-with-docker-compose","level":4,"title":"Quick Start with Docker Compose","text":"<p>The easiest way to deploy a distributed EEMT cluster:</p> <pre><code># Clone repository\ngit clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n\n# Start distributed cluster (master + workers)\ndocker-compose --profile distributed up\n\n# Scale workers as needed\ndocker-compose --profile distributed up --scale eemt-worker-2=3 --scale eemt-worker-3=2\n\n# Access web interface at http://localhost:5000\n# Monitor cluster at http://localhost:5000/monitor\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#distributed-docker-deployment","level":4,"title":"Distributed Docker Deployment","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#1-master-node-container","level":5,"title":"1. Master Node Container","text":"<pre><code># Start master node with web interface\ndocker run -d \\\n  --name eemt-master \\\n  -p 5000:5000 \\\n  -p 9123:9123 \\\n  -v $(pwd)/data:/app/data \\\n  -e EEMT_MODE=master \\\n  -e WORK_QUEUE_PORT=9123 \\\n  -e WORK_QUEUE_PROJECT=EEMT-Cluster \\\n  eemt-web\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#2-worker-node-containers","level":5,"title":"2. Worker Node Containers","text":"<pre><code># Start worker containers on same host\ndocker run -d \\\n  --name eemt-worker-1 \\\n  -e MASTER_HOST=eemt-master \\\n  -e MASTER_PORT=9123 \\\n  -e WORKER_CORES=4 \\\n  -e WORKER_MEMORY=8G \\\n  --network container:eemt-master \\\n  eemt:ubuntu24.04\n\n# Start workers on remote hosts\ndocker run -d \\\n  --name eemt-worker-remote \\\n  -e MASTER_HOST=MASTER_IP_ADDRESS \\\n  -e MASTER_PORT=9123 \\\n  -e WORKER_CORES=8 \\\n  -e WORKER_MEMORY=16G \\\n  eemt:ubuntu24.04\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#3-custom-docker-compose","level":5,"title":"3. Custom Docker Compose","text":"<p>Create <code>docker-compose.production.yml</code>:</p> <pre><code>version: '3.8'\nservices:\n  eemt-master:\n    image: eemt-web:latest\n    ports:\n      - \"5000:5000\"\n      - \"9123:9123\"\n    volumes:\n      - /shared/eemt/data:/app/data\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - EEMT_MODE=master\n      - WORK_QUEUE_PROJECT=EEMT-Production\n      - MAX_WORKERS=100\n    restart: unless-stopped\n\n  eemt-worker:\n    image: eemt:ubuntu24.04\n    environment:\n      - MASTER_HOST=eemt-master\n      - MASTER_PORT=9123\n      - WORKER_CORES=8\n      - WORKER_MEMORY=16G\n      - WORKER_DISK=100G\n    depends_on:\n      - eemt-master\n    restart: unless-stopped\n    deploy:\n      replicas: 5\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#traditional-installation","level":3,"title":"Traditional Installation","text":"<p>For environments requiring manual setup:</p>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#1-install-dependencies","level":4,"title":"1. Install Dependencies","text":"<pre><code># Ubuntu/Debian\nsudo apt update\nsudo apt install -y docker.io docker-compose python3-pip git\n\n# CentOS/RHEL\nsudo dnf install -y docker docker-compose python3-pip git\nsudo systemctl enable --now docker\n\n# Add user to docker group\nsudo usermod -aG docker $USER\n# Log out and back in for group changes to take effect\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#2-clone-and-build","level":4,"title":"2. Clone and Build","text":"<pre><code># Clone repository\ngit clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n\n# Build containers\ndocker build -t eemt:ubuntu24.04 -f docker/ubuntu/24.04/Dockerfile .\ndocker build -t eemt-web -f docker/web-interface/Dockerfile .\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#3-start-master-node","level":4,"title":"3. Start Master Node","text":"<pre><code># Using Docker\ndocker run -d \\\n  --name eemt-master \\\n  -p 5000:5000 \\\n  -p 9123:9123 \\\n  -v $(pwd)/data:/app/data \\\n  eemt-web\n\n# Using Python scripts (requires manual CCTools installation)\npython scripts/start-master.py \\\n    --work-dir /data/eemt-master \\\n    --port 9123 \\\n    --project EEMT-Production \\\n    --web-interface \\\n    --web-port 5000\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#worker-node-setup","level":3,"title":"Worker Node Setup","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#docker-worker-deployment-recommended","level":4,"title":"Docker Worker Deployment (Recommended)","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#1-single-worker-container","level":5,"title":"1. Single Worker Container","text":"<pre><code># Connect to remote master\ndocker run -d \\\n  --name eemt-worker \\\n  -e MASTER_HOST=MASTER_IP_ADDRESS \\\n  -e MASTER_PORT=9123 \\\n  -e WORKER_CORES=8 \\\n  -e WORKER_MEMORY=16G \\\n  -e WORKER_DISK=100G \\\n  eemt:ubuntu24.04 \\\n  python /scripts/start-worker.py \\\n    --master-host $MASTER_HOST \\\n    --master-port $MASTER_PORT \\\n    --cores $WORKER_CORES \\\n    --memory $WORKER_MEMORY \\\n    --disk $WORKER_DISK\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#2-multiple-workers-docker-swarm","level":5,"title":"2. Multiple Workers (Docker Swarm)","text":"<pre><code># Initialize Docker Swarm (on manager node)\ndocker swarm init --advertise-addr MANAGER_IP\n\n# Create worker service\ndocker service create \\\n  --name eemt-workers \\\n  --replicas 10 \\\n  --env MASTER_HOST=MASTER_IP \\\n  --env MASTER_PORT=9123 \\\n  --env WORKER_CORES=4 \\\n  --env WORKER_MEMORY=8G \\\n  eemt:ubuntu24.04\n\n# Join worker nodes to swarm\ndocker swarm join --token WORKER_TOKEN MANAGER_IP:2377\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#3-kubernetes-deployment","level":5,"title":"3. Kubernetes Deployment","text":"<p>Create <code>k8s-eemt-workers.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eemt-workers\nspec:\n  replicas: 20\n  selector:\n    matchLabels:\n      app: eemt-worker\n  template:\n    metadata:\n      labels:\n        app: eemt-worker\n    spec:\n      containers:\n      - name: eemt-worker\n        image: eemt:ubuntu24.04\n        env:\n        - name: MASTER_HOST\n          value: \"eemt-master-service\"\n        - name: MASTER_PORT\n          value: \"9123\"\n        - name: WORKER_CORES\n          value: \"4\"\n        - name: WORKER_MEMORY\n          value: \"8G\"\n        resources:\n          requests:\n            memory: \"8Gi\"\n            cpu: \"4\"\n          limits:\n            memory: \"16Gi\"\n            cpu: \"8\"\n        command: [\"python\", \"/scripts/start-worker.py\"]\n        args: [\"--master-host\", \"$(MASTER_HOST)\", \"--master-port\", \"$(MASTER_PORT)\"]\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#traditional-worker-setup","level":4,"title":"Traditional Worker Setup","text":"<p>For environments without container orchestration:</p>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#1-install-dependencies_1","level":5,"title":"1. Install Dependencies","text":"<pre><code># Ubuntu/Debian\nsudo apt update\nsudo apt install -y docker.io python3-pip\n\n# CentOS/RHEL  \nsudo dnf install -y docker python3-pip\nsudo systemctl enable --now docker\n\n# For HPC environments without root access\nwget https://github.com/cooperative-computing-lab/cctools/releases/download/release%2F7.8.2/cctools-7.8.2-x86_64-linux.tar.gz\ntar xzf cctools-7.8.2-x86_64-linux.tar.gz\nexport PATH=$PWD/cctools-7.8.2-x86_64-linux/bin:$PATH\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#2-pull-container-image","level":5,"title":"2. Pull Container Image","text":"<pre><code># Option A: Pull from registry\ndocker pull eemt:ubuntu24.04\n\n# Option B: Transfer from master node\ndocker save eemt:ubuntu24.04 | gzip &gt; eemt-container.tar.gz\nscp eemt-container.tar.gz worker-node:\n# On worker node:\ngunzip -c eemt-container.tar.gz | docker load\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#3-start-worker-node","level":5,"title":"3. Start Worker Node","text":"<pre><code># Containerized worker\ndocker run -d \\\n  --name eemt-worker \\\n  -v /tmp:/tmp \\\n  -e MASTER_HOST=MASTER_IP \\\n  -e MASTER_PORT=9123 \\\n  eemt:ubuntu24.04 \\\n  python /scripts/start-worker.py \\\n    --master-host $MASTER_HOST \\\n    --master-port $MASTER_PORT \\\n    --cores 8 --memory 16G\n\n# Native worker (requires CCTools installation)\npython scripts/start-worker.py \\\n    --master-host MASTER_IP \\\n    --master-port 9123 \\\n    --cores 8 --memory 16G\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#hpc-integration-examples","level":2,"title":"HPC Integration Examples","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#slurm-docker-integration","level":3,"title":"SLURM Docker Integration","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#option-1-containerized-workers","level":4,"title":"Option 1: Containerized Workers","text":"<p>Create <code>eemt-docker-worker.sbatch</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=eemt-docker-worker\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32GB\n#SBATCH --time=4:00:00\n#SBATCH --partition=compute\n\n# Load Docker module\nmodule load docker/20.10\n\n# Set master information\nexport MASTER_HOST=login-node.hpc.example.edu\nexport MASTER_PORT=9123\n\n# Start containerized worker\ndocker run --rm \\\n  --name eemt-worker-$SLURM_JOB_ID \\\n  -e MASTER_HOST=$MASTER_HOST \\\n  -e MASTER_PORT=$MASTER_PORT \\\n  -e WORKER_CORES=$SLURM_CPUS_PER_TASK \\\n  -e WORKER_MEMORY=${SLURM_MEM_PER_NODE}M \\\n  -v $TMPDIR:/tmp \\\n  eemt:ubuntu24.04 \\\n  python /scripts/start-worker.py \\\n    --master-host $MASTER_HOST \\\n    --master-port $MASTER_PORT \\\n    --cores $SLURM_CPUS_PER_TASK \\\n    --memory ${SLURM_MEM_PER_NODE}M \\\n    --work-dir /tmp/eemt-worker-$SLURM_JOB_ID\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#option-2-singularity-workers-for-hpc-without-docker","level":4,"title":"Option 2: Singularity Workers (for HPC without Docker)","text":"<p>Create <code>eemt-singularity-worker.sbatch</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=eemt-singularity-worker\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32GB\n#SBATCH --time=4:00:00\n\n# Load Singularity\nmodule load singularity/3.8\n\n# Convert Docker image to Singularity (one-time)\n# singularity pull eemt-ubuntu24.04.sif docker://eemt:ubuntu24.04\n\n# Start worker\nsingularity exec \\\n  --bind $TMPDIR:/tmp \\\n  --env MASTER_HOST=$MASTER_HOST \\\n  --env MASTER_PORT=$MASTER_PORT \\\n  eemt-ubuntu24.04.sif \\\n  python /scripts/start-worker.py \\\n    --master-host $MASTER_HOST \\\n    --master-port $MASTER_PORT \\\n    --cores $SLURM_CPUS_PER_TASK \\\n    --memory ${SLURM_MEM_PER_NODE}M\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#submit-multiple-workers","level":4,"title":"Submit Multiple Workers","text":"<pre><code># Submit 20 Docker workers\nfor i in {1..20}; do\n    sbatch eemt-docker-worker.sbatch\ndone\n\n# Submit with array jobs\nsbatch --array=1-20 eemt-docker-worker.sbatch\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#pbstorque-docker-example","level":3,"title":"PBS/Torque Docker Example","text":"<p>Create <code>eemt-docker-worker.pbs</code>:</p> <pre><code>#!/bin/bash\n#PBS -N eemt-docker-worker\n#PBS -l nodes=1:ppn=8\n#PBS -l mem=32gb\n#PBS -l walltime=04:00:00\n#PBS -q compute\n\ncd $PBS_O_WORKDIR\n\n# Load Docker module\nmodule load docker/20.10\n\n# Start containerized worker\ndocker run --rm \\\n  --name eemt-worker-$PBS_JOBID \\\n  -e MASTER_HOST=$MASTER_HOST \\\n  -e MASTER_PORT=9123 \\\n  -e WORKER_CORES=8 \\\n  -e WORKER_MEMORY=32G \\\n  -v $TMPDIR:/tmp \\\n  eemt:ubuntu24.04 \\\n  python /scripts/start-worker.py \\\n    --master-host $MASTER_HOST \\\n    --master-port 9123 \\\n    --cores 8 \\\n    --memory 32G \\\n    --work-dir /tmp/eemt-worker-$PBS_JOBID\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#lsf-docker-example","level":3,"title":"LSF Docker Example","text":"<p>Create <code>eemt-docker-worker.lsf</code>:</p> <pre><code>#!/bin/bash\n#BSUB -J eemt-docker-worker\n#BSUB -n 8\n#BSUB -R \"rusage[mem=4096]\"\n#BSUB -W 4:00\n#BSUB -o eemt-worker.%J.out\n#BSUB -e eemt-worker.%J.err\n\n# Load Docker\nmodule load docker\n\n# Start containerized worker\ndocker run --rm \\\n  --name eemt-worker-$LSB_JOBID \\\n  -e MASTER_HOST=$MASTER_HOST \\\n  -e MASTER_PORT=9123 \\\n  -e WORKER_CORES=$LSB_MAX_NUM_PROCESSORS \\\n  -e WORKER_MEMORY=${LSB_MAX_MEM_POLICY}M \\\n  -v $TMPDIR:/tmp \\\n  eemt:ubuntu24.04 \\\n  python /scripts/start-worker.py \\\n    --master-host $MASTER_HOST \\\n    --master-port 9123 \\\n    --cores $LSB_MAX_NUM_PROCESSORS \\\n    --memory ${LSB_MAX_MEM_POLICY}M\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#monitoring-and-management","level":2,"title":"Monitoring and Management","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#docker-based-monitoring","level":3,"title":"Docker-Based Monitoring","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#master-node-monitoring","level":4,"title":"Master Node Monitoring","text":"<pre><code># Monitor master container\ndocker logs -f eemt-master\n\n# Check Work Queue status\ndocker exec eemt-master work_queue_status -M EEMT\n\n# Web interface monitoring\nfirefox http://master-node:5000/monitor\n\n# Container resource usage\ndocker stats eemt-master\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#worker-monitoring","level":4,"title":"Worker Monitoring","text":"<pre><code># List all worker containers\ndocker ps --filter name=eemt-worker\n\n# Monitor worker logs\ndocker logs -f eemt-worker-1\n\n# Check worker connectivity (container)\ndocker run --rm eemt:ubuntu24.04 \\\n  python /scripts/start-worker.py \\\n  --master-host MASTER_IP --dry-run\n\n# Resource monitoring across containers\ndocker stats $(docker ps --filter name=eemt-worker --format \"{{.Names}}\")\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#cluster-status-commands","level":4,"title":"Cluster Status Commands","text":"<pre><code># Show detailed cluster status\ndocker exec eemt-master work_queue_status -M EEMT --verbose\n\n# Show worker locations and resources\ndocker exec eemt-master work_queue_status -M EEMT --workers --resources\n\n# Historical statistics\ndocker exec eemt-master work_queue_status -M EEMT --stats\n\n# Container orchestration status\ndocker service ls  # Docker Swarm\nkubectl get pods   # Kubernetes\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#docker-compose-monitoring","level":3,"title":"Docker Compose Monitoring","text":"<pre><code># View all services\ndocker-compose ps\n\n# Follow logs for specific service\ndocker-compose logs -f eemt-master\ndocker-compose logs -f eemt-worker\n\n# Scale workers dynamically\ndocker-compose up --scale eemt-worker=10\n\n# View resource usage\ndocker-compose top\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#configuration-files","level":2,"title":"Configuration Files","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#master-configuration","level":3,"title":"Master Configuration","text":"<p>Create <code>/etc/eemt/master.conf</code>:</p> <pre><code>[master]\nport = 9123\nproject = EEMT-Production\nmax_workers = 100\nworker_timeout = 300\nheartbeat_interval = 30\n\n[web_interface]\nenabled = true\nport = 5000\nhost = 0.0.0.0\n\n[security]\npassword_file = /etc/eemt/cluster.password\nssl_cert = /etc/eemt/ssl/cert.pem\nssl_key = /etc/eemt/ssl/key.pem\n\n[storage]\nshared_dir = /shared/eemt\nresults_dir = /data/eemt/results\nuploads_dir = /data/eemt/uploads\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#worker-configuration","level":3,"title":"Worker Configuration","text":"<p>Create <code>/etc/eemt/worker.conf</code>:</p> <pre><code>[worker]\nmaster_host = cluster-master.example.com\nmaster_port = 9123\ncores = auto\nmemory = auto\ndisk = auto\n\n[runtime]\nwork_dir = /tmp/eemt-worker\ncontainer_runtime = docker\nreconnect_attempts = 5\nreconnect_delay = 10\n\n[logging]\nlevel = INFO\nfile = /var/log/eemt/worker.log\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#security-considerations","level":2,"title":"Security Considerations","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#network-security","level":3,"title":"Network Security","text":"<ul> <li>Firewall Rules: Open Work Queue port (9123) between master and workers</li> <li>VPN/Private Networks: Use private networks for sensitive data</li> <li>SSH Tunneling: Secure connections through SSH tunnels if needed</li> </ul> <pre><code># SSH tunnel example for secure connection\nssh -L 9123:master-internal:9123 gateway.example.com\npython scripts/start-worker.py --master-host localhost --master-port 9123\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#authentication","level":3,"title":"Authentication","text":"<ul> <li>Password Protection: Use strong Work Queue passwords</li> <li>Certificate-based: Implement SSL/TLS for web interface</li> <li>Network Isolation: Restrict access to trusted networks</li> </ul> <pre><code># Generate secure password\nopenssl rand -base64 32 &gt; ~/.eemt-makeflow-password\nchmod 600 ~/.eemt-makeflow-password\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#common-issues","level":3,"title":"Common Issues","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#master-not-starting","level":4,"title":"Master Not Starting","text":"<pre><code># Check port availability\nnetstat -an | grep 9123\n\n# Check CCTools installation\nwork_queue_worker --version\nmakeflow --version\n\n# Check Docker availability\ndocker info\n\n# View detailed logs\npython scripts/start-master.py --log-level DEBUG\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#workers-not-connecting","level":4,"title":"Workers Not Connecting","text":"<pre><code># Test connectivity\ntelnet master-host 9123\nping master-host\n\n# Check firewall\nsudo ufw status\nsudo iptables -L\n\n# Verify CCTools\nwhich work_queue_worker\nwork_queue_worker --help\n\n# Test with verbose logging\npython scripts/start-worker.py --master-host MASTER --log-level DEBUG\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#performance-issues","level":4,"title":"Performance Issues","text":"<pre><code># Monitor system resources\nhtop\niostat -x 1\ndf -h\n\n# Check network bandwidth\niperf3 -c master-host\n\n# Monitor container resource usage\ndocker stats\n\n# Work Queue statistics\nwork_queue_status -M EEMT --resources --verbose\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#container-issues","level":4,"title":"Container Issues","text":"<pre><code># Check Docker daemon\nsudo systemctl status docker\n\n# Pull latest container\ndocker pull eemt:ubuntu24.04\n\n# Test container manually\ndocker run -it --rm eemt:ubuntu24.04 /bin/bash\n\n# Check container logs\ndocker logs CONTAINER_ID\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#best-practices","level":2,"title":"Best Practices","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#resource-management","level":3,"title":"Resource Management","text":"<ul> <li>CPU Allocation: Leave 1-2 cores for system processes</li> <li>Memory Limits: Reserve 2GB for OS and Docker</li> <li>Disk Space: Ensure adequate temporary space for workflows</li> <li>Network: Use high-bandwidth connections for large datasets</li> </ul>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#deployment-strategy","level":3,"title":"Deployment Strategy","text":"<ul> <li>Start Small: Begin with 2-3 workers, scale gradually</li> <li>Monitor Performance: Track task completion rates and resource usage</li> <li>Load Balancing: Distribute workers across different hardware types</li> <li>Fault Tolerance: Plan for worker failures and network interruptions</li> </ul>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#maintenance","level":3,"title":"Maintenance","text":"<ul> <li>Regular Updates: Keep container images and CCTools updated</li> <li>Log Rotation: Implement log rotation to manage disk space</li> <li>Monitoring: Set up alerts for master/worker failures</li> <li>Backup: Regular backup of configuration and critical data</li> </ul>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#advanced-configurations","level":2,"title":"Advanced Configurations","text":"","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#multi-master-setup","level":3,"title":"Multi-Master Setup","text":"<p>For high availability, deploy multiple master nodes:</p> <pre><code># Master 1\npython scripts/start-master.py --port 9123 --project EEMT-Primary\n\n# Master 2 (backup)\npython scripts/start-master.py --port 9124 --project EEMT-Backup\n\n# Workers connect to both\npython scripts/start-worker.py --master-host master1 --master-port 9123 &amp;\npython scripts/start-worker.py --master-host master2 --master-port 9124 &amp;\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#heterogeneous-workers","level":3,"title":"Heterogeneous Workers","text":"<p>Deploy different worker types for different tasks:</p> <pre><code># CPU-intensive workers\npython scripts/start-worker.py --master-host MASTER --cores 16 --memory 64G\n\n# Memory-intensive workers  \npython scripts/start-worker.py --master-host MASTER --cores 8 --memory 128G\n\n# GPU workers (future)\npython scripts/start-worker.py --master-host MASTER --cores 8 --memory 32G --gpu 1\n</code></pre>","path":["Distributed Deployment"],"tags":[]},{"location":"distributed-deployment/#container-orchestration","level":3,"title":"Container Orchestration","text":"<p>Integration with Kubernetes, Docker Swarm, or other orchestrators:</p> <pre><code># Kubernetes example\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eemt-workers\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: eemt-worker\n  template:\n    metadata:\n      labels:\n        app: eemt-worker\n    spec:\n      containers:\n      - name: eemt-worker\n        image: eemt:ubuntu24.04\n        command: [\"python\", \"/scripts/start-worker.py\"]\n        args: [\"--master-host\", \"eemt-master-service\", \"--master-port\", \"9123\"]\n        resources:\n          requests:\n            memory: \"8Gi\"\n            cpu: \"4\"\n          limits:\n            memory: \"16Gi\"\n            cpu: \"8\"\n</code></pre> <p>This distributed deployment guide provides comprehensive coverage for deploying EEMT across various computing environments while maintaining security, performance, and reliability.</p>","path":["Distributed Deployment"],"tags":[]},{"location":"docker/cleanup-integration/","level":1,"title":"Docker Cleanup Integration Guide","text":"<p>This guide explains how to integrate the EEMT job data cleanup system with Docker and container orchestration platforms.</p>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#overview","level":2,"title":"Overview","text":"<p>The EEMT cleanup system is designed to work seamlessly in containerized environments, providing multiple integration options:</p> <ul> <li>Built-in container cleanup: Cleanup runs within the web interface container</li> <li>Dedicated cleanup container: Separate container for cleanup operations  </li> <li>Orchestration integration: Works with Docker Compose, Kubernetes, and Swarm</li> <li>Volume management: Handles Docker volumes and bind mounts correctly</li> <li>Multi-container coordination: Supports distributed deployments</li> </ul>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#docker-compose-integration","level":2,"title":"Docker Compose Integration","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#basic-configuration","level":3,"title":"Basic Configuration","text":"<p>Add cleanup configuration to your <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-web:\n    image: eemt-web:latest\n    container_name: eemt-web\n    ports:\n      - \"5000:5000\"\n    environment:\n      # Cleanup configuration\n      - EEMT_SUCCESS_RETENTION_DAYS=7\n      - EEMT_FAILED_RETENTION_HOURS=12\n      - EEMT_ENABLE_AUTO_CLEANUP=true\n      - EEMT_CLEANUP_SCHEDULE=0 2 * * *  # Daily at 2 AM\n    volumes:\n      - ./data/uploads:/app/uploads\n      - ./data/results:/app/results\n      - ./data/jobs.db:/app/jobs.db\n      - ./data/logs:/app/logs\n    restart: unless-stopped\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#with-dedicated-cleanup-service","level":3,"title":"With Dedicated Cleanup Service","text":"<p>For better separation of concerns, use a dedicated cleanup service:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-web:\n    image: eemt-web:latest\n    container_name: eemt-web\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - eemt-uploads:/app/uploads\n      - eemt-results:/app/results\n      - eemt-db:/app/db\n    restart: unless-stopped\n\n  eemt-cleanup:\n    image: eemt-web:latest\n    container_name: eemt-cleanup\n    environment:\n      - EEMT_SUCCESS_RETENTION_DAYS=7\n      - EEMT_FAILED_RETENTION_HOURS=12\n    volumes:\n      - eemt-uploads:/app/uploads:rw\n      - eemt-results:/app/results:rw\n      - eemt-db:/app/db:rw\n    entrypoint: [\"/bin/sh\", \"-c\"]\n    command:\n      - |\n        while true; do\n          echo \"Running cleanup at $$(date)\"\n          python /app/cleanup_jobs.py\n          echo \"Cleanup complete. Sleeping for 24 hours...\"\n          sleep 86400\n        done\n    restart: unless-stopped\n    depends_on:\n      - eemt-web\n\nvolumes:\n  eemt-uploads:\n  eemt-results:\n  eemt-db:\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#production-configuration","level":3,"title":"Production Configuration","text":"<p>Full production setup with monitoring and backups:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-web:\n    image: eemt-web:${VERSION:-latest}\n    container_name: eemt-web\n    ports:\n      - \"5000:5000\"\n    environment:\n      - EEMT_ENV=production\n      - EEMT_LOG_LEVEL=INFO\n    volumes:\n      - eemt-uploads:/app/uploads\n      - eemt-results:/app/results\n      - eemt-db:/app/db\n      - eemt-logs:/app/logs\n    restart: always\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  eemt-cleanup:\n    image: eemt-web:${VERSION:-latest}\n    container_name: eemt-cleanup\n    environment:\n      - EEMT_SUCCESS_RETENTION_DAYS=${SUCCESS_RETENTION:-7}\n      - EEMT_FAILED_RETENTION_HOURS=${FAILED_RETENTION:-12}\n      - EEMT_CLEANUP_LOG_LEVEL=INFO\n      - TZ=America/New_York  # Set timezone for scheduling\n    volumes:\n      - eemt-uploads:/app/uploads:rw\n      - eemt-results:/app/results:rw\n      - eemt-db:/app/db:rw\n      - eemt-logs:/app/logs:rw\n      - eemt-cleanup-logs:/app/cleanup-logs:rw\n    entrypoint: [\"/usr/local/bin/cleanup-scheduler.sh\"]\n    restart: always\n    depends_on:\n      eemt-web:\n        condition: service_healthy\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n\n  # Optional: Backup service\n  eemt-backup:\n    image: alpine:latest\n    container_name: eemt-backup\n    volumes:\n      - eemt-results:/data/results:ro\n      - eemt-db:/data/db:ro\n      - ./backups:/backups:rw\n    entrypoint: [\"/bin/sh\", \"-c\"]\n    command:\n      - |\n        while true; do\n          echo \"Creating backup at $$(date)\"\n          tar -czf /backups/backup_$$(date +%Y%m%d_%H%M%S).tar.gz \\\n            /data/results /data/db\n          # Keep only last 7 backups\n          ls -t /backups/backup_*.tar.gz | tail -n +8 | xargs -r rm\n          sleep 86400\n        done\n    restart: unless-stopped\n\nvolumes:\n  eemt-uploads:\n    driver: local\n  eemt-results:\n    driver: local\n  eemt-db:\n    driver: local\n  eemt-logs:\n    driver: local\n  eemt-cleanup-logs:\n    driver: local\n\nnetworks:\n  default:\n    name: eemt-network\n    driver: bridge\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#container-execution-methods","level":2,"title":"Container Execution Methods","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#method-1-execute-in-running-container","level":3,"title":"Method 1: Execute in Running Container","text":"<p>Run cleanup in an existing web interface container:</p> <pre><code># One-time cleanup\ndocker exec eemt-web python /app/cleanup_jobs.py\n\n# Dry run to preview\ndocker exec eemt-web python /app/cleanup_jobs.py --dry-run\n\n# With custom retention\ndocker exec eemt-web python /app/cleanup_jobs.py \\\n  --success-retention-days 3 \\\n  --failed-retention-hours 6\n\n# View cleanup logs\ndocker exec eemt-web tail -f /app/logs/cleanup_jobs.log\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#method-2-dedicated-cleanup-container","level":3,"title":"Method 2: Dedicated Cleanup Container","text":"<p>Run cleanup in a separate container:</p> <pre><code># Create cleanup container\ndocker run -d \\\n  --name eemt-cleanup \\\n  --network eemt-network \\\n  -v eemt-uploads:/app/uploads \\\n  -v eemt-results:/app/results \\\n  -v eemt-db:/app/db \\\n  -e EEMT_SUCCESS_RETENTION_DAYS=7 \\\n  -e EEMT_FAILED_RETENTION_HOURS=12 \\\n  eemt-web:latest \\\n  sh -c 'while true; do python /app/cleanup_jobs.py; sleep 86400; done'\n\n# Check cleanup status\ndocker logs eemt-cleanup --tail 50\n\n# Stop cleanup container\ndocker stop eemt-cleanup\ndocker rm eemt-cleanup\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#method-3-one-off-cleanup-container","level":3,"title":"Method 3: One-off Cleanup Container","text":"<p>Run cleanup as a one-time operation:</p> <pre><code># Run and remove container after cleanup\ndocker run --rm \\\n  --network eemt-network \\\n  -v $(pwd)/data/uploads:/app/uploads \\\n  -v $(pwd)/data/results:/app/results \\\n  -v $(pwd)/data/jobs.db:/app/jobs.db \\\n  eemt-web:latest \\\n  python /app/cleanup_jobs.py --verbose\n\n# Dry run with bind mounts\ndocker run --rm \\\n  -v $(pwd)/uploads:/app/uploads:ro \\\n  -v $(pwd)/results:/app/results:ro \\\n  -v $(pwd)/jobs.db:/app/jobs.db:ro \\\n  eemt-web:latest \\\n  python /app/cleanup_jobs.py --dry-run\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#dockerfile-integration","level":2,"title":"Dockerfile Integration","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#adding-cleanup-to-web-interface-image","level":3,"title":"Adding Cleanup to Web Interface Image","text":"<p>Modify your Dockerfile to include cleanup capabilities:</p> <pre><code>FROM eemt:ubuntu24.04\n\n# Install cleanup dependencies\nRUN pip install --no-cache-dir \\\n    schedule \\\n    python-crontab\n\n# Copy cleanup scripts\nCOPY web-interface/cleanup_jobs.py /app/\nCOPY scripts/cleanup-scheduler.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/cleanup-scheduler.sh\n\n# Create cleanup log directory\nRUN mkdir -p /app/logs/cleanup\n\n# Set cleanup environment defaults\nENV EEMT_SUCCESS_RETENTION_DAYS=7 \\\n    EEMT_FAILED_RETENTION_HOURS=12 \\\n    EEMT_ENABLE_AUTO_CLEANUP=false\n\n# Entry point that optionally starts cleanup\nCOPY docker-entrypoint.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/docker-entrypoint.sh\n\nENTRYPOINT [\"/usr/local/bin/docker-entrypoint.sh\"]\nCMD [\"python\", \"app.py\"]\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#docker-entry-point-script","level":3,"title":"Docker Entry Point Script","text":"<p>Create <code>docker-entrypoint.sh</code> to handle cleanup scheduling:</p> <pre><code>#!/bin/sh\nset -e\n\n# Start cleanup scheduler if enabled\nif [ \"$EEMT_ENABLE_AUTO_CLEANUP\" = \"true\" ]; then\n    echo \"Starting cleanup scheduler...\"\n\n    # Create cron job\n    SCHEDULE=\"${EEMT_CLEANUP_SCHEDULE:-0 2 * * *}\"\n    echo \"$SCHEDULE cd /app &amp;&amp; python cleanup_jobs.py &gt;&gt; /app/logs/cleanup/cleanup.log 2&gt;&amp;1\" | crontab -\n\n    # Start cron daemon\n    crond || cron\n\n    echo \"Cleanup scheduler started with schedule: $SCHEDULE\"\nfi\n\n# Execute main command\nexec \"$@\"\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#cleanup-scheduler-script","level":3,"title":"Cleanup Scheduler Script","text":"<p>Create <code>cleanup-scheduler.sh</code> for dedicated cleanup containers:</p> <pre><code>#!/bin/sh\n#\n# Cleanup scheduler for Docker containers\n#\n\nset -e\n\n# Configuration from environment\nSUCCESS_RETENTION=${EEMT_SUCCESS_RETENTION_DAYS:-7}\nFAILED_RETENTION=${EEMT_FAILED_RETENTION_HOURS:-12}\nCLEANUP_INTERVAL=${EEMT_CLEANUP_INTERVAL:-86400}  # Default 24 hours\nDRY_RUN=${EEMT_DRY_RUN:-false}\n\necho \"EEMT Cleanup Scheduler Started\"\necho \"================================\"\necho \"Success retention: $SUCCESS_RETENTION days\"\necho \"Failed retention: $FAILED_RETENTION hours\"\necho \"Cleanup interval: $CLEANUP_INTERVAL seconds\"\necho \"Dry run mode: $DRY_RUN\"\necho \"\"\n\n# Function to run cleanup\nrun_cleanup() {\n    echo \"[$(date)] Starting cleanup run...\"\n\n    CLEANUP_ARGS=\"--success-retention-days $SUCCESS_RETENTION\"\n    CLEANUP_ARGS=\"$CLEANUP_ARGS --failed-retention-hours $FAILED_RETENTION\"\n\n    if [ \"$DRY_RUN\" = \"true\" ]; then\n        CLEANUP_ARGS=\"$CLEANUP_ARGS --dry-run\"\n    fi\n\n    if python /app/cleanup_jobs.py $CLEANUP_ARGS; then\n        echo \"[$(date)] Cleanup completed successfully\"\n    else\n        echo \"[$(date)] Cleanup failed with exit code $?\"\n    fi\n}\n\n# Signal handlers\ntrap 'echo \"Received SIGTERM, shutting down...\"; exit 0' TERM\ntrap 'echo \"Received SIGINT, shutting down...\"; exit 0' INT\n\n# Main loop\nwhile true; do\n    run_cleanup\n\n    echo \"[$(date)] Sleeping for $CLEANUP_INTERVAL seconds...\"\n    sleep $CLEANUP_INTERVAL &amp;\n    wait $!\ndone\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#kubernetes-integration","level":2,"title":"Kubernetes Integration","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#kubernetes-cronjob","level":3,"title":"Kubernetes CronJob","text":"<p>Deploy cleanup as a Kubernetes CronJob:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: eemt-cleanup\n  namespace: eemt\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM UTC\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: eemt-cleanup\n        spec:\n          containers:\n          - name: cleanup\n            image: eemt-web:latest\n            imagePullPolicy: Always\n            command:\n              - python\n              - /app/cleanup_jobs.py\n            env:\n            - name: EEMT_SUCCESS_RETENTION_DAYS\n              value: \"7\"\n            - name: EEMT_FAILED_RETENTION_HOURS\n              value: \"12\"\n            - name: EEMT_DRY_RUN\n              value: \"false\"\n            volumeMounts:\n            - name: uploads\n              mountPath: /app/uploads\n            - name: results\n              mountPath: /app/results\n            - name: database\n              mountPath: /app/db\n            resources:\n              requests:\n                memory: \"256Mi\"\n                cpu: \"100m\"\n              limits:\n                memory: \"512Mi\"\n                cpu: \"500m\"\n          restartPolicy: OnFailure\n          volumes:\n          - name: uploads\n            persistentVolumeClaim:\n              claimName: eemt-uploads-pvc\n          - name: results\n            persistentVolumeClaim:\n              claimName: eemt-results-pvc\n          - name: database\n            persistentVolumeClaim:\n              claimName: eemt-database-pvc\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#kubernetes-configmap","level":3,"title":"Kubernetes ConfigMap","text":"<p>Store cleanup configuration in a ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: eemt-cleanup-config\n  namespace: eemt\ndata:\n  cleanup_config.yaml: |\n    retention:\n      successful_jobs:\n        days: 7\n        keep_metadata: true\n      failed_jobs:\n        hours: 12\n        keep_error_logs: true\n\n    performance:\n      batch_size: 100\n      parallel_delete: false\n\n    logging:\n      level: INFO\n      format: json\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#helm-chart-integration","level":3,"title":"Helm Chart Integration","text":"<p>Add cleanup to your Helm chart:</p> <pre><code># values.yaml\ncleanup:\n  enabled: true\n  schedule: \"0 2 * * *\"\n  retention:\n    successDays: 7\n    failedHours: 12\n  resources:\n    requests:\n      memory: 256Mi\n      cpu: 100m\n    limits:\n      memory: 512Mi\n      cpu: 500m\n\n# templates/cleanup-cronjob.yaml\n{{- if .Values.cleanup.enabled }}\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: {{ include \"eemt.fullname\" . }}-cleanup\nspec:\n  schedule: {{ .Values.cleanup.schedule | quote }}\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: {{ .Values.image.repository }}:{{ .Values.image.tag }}\n            command: [\"python\", \"/app/cleanup_jobs.py\"]\n            env:\n            - name: EEMT_SUCCESS_RETENTION_DAYS\n              value: {{ .Values.cleanup.retention.successDays | quote }}\n            - name: EEMT_FAILED_RETENTION_HOURS\n              value: {{ .Values.cleanup.retention.failedHours | quote }}\n            resources:\n              {{- toYaml .Values.cleanup.resources | nindent 14 }}\n{{- end }}\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#docker-swarm-integration","level":2,"title":"Docker Swarm Integration","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#swarm-service-configuration","level":3,"title":"Swarm Service Configuration","text":"<p>Deploy cleanup as a Swarm service:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-cleanup:\n    image: eemt-web:latest\n    environment:\n      - EEMT_SUCCESS_RETENTION_DAYS=7\n      - EEMT_FAILED_RETENTION_HOURS=12\n    volumes:\n      - eemt-uploads:/app/uploads\n      - eemt-results:/app/results\n      - eemt-db:/app/db\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      restart_policy:\n        condition: any\n        delay: 5s\n        max_attempts: 3\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.1'\n          memory: 128M\n    networks:\n      - eemt-network\n    command: |\n      sh -c '\n        while true; do\n          python /app/cleanup_jobs.py\n          sleep 86400\n        done\n      '\n\nvolumes:\n  eemt-uploads:\n    driver: local\n    driver_opts:\n      type: nfs\n      o: addr=nfs-server.example.com,rw\n      device: \":/data/eemt/uploads\"\n  eemt-results:\n    driver: local\n    driver_opts:\n      type: nfs\n      o: addr=nfs-server.example.com,rw\n      device: \":/data/eemt/results\"\n  eemt-db:\n    driver: local\n    driver_opts:\n      type: nfs\n      o: addr=nfs-server.example.com,rw\n      device: \":/data/eemt/db\"\n\nnetworks:\n  eemt-network:\n    driver: overlay\n    attachable: true\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#volume-management","level":2,"title":"Volume Management","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#docker-volume-best-practices","level":3,"title":"Docker Volume Best Practices","text":"<ol> <li> <p>Use Named Volumes: Prefer named volumes over bind mounts for production:    <pre><code>volumes:\n  eemt-data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /data/eemt\n</code></pre></p> </li> <li> <p>Volume Backup: Before cleanup, backup important data:    <pre><code># Backup volume to tar\ndocker run --rm \\\n  -v eemt-results:/data \\\n  -v $(pwd)/backups:/backup \\\n  alpine \\\n  tar czf /backup/results_$(date +%Y%m%d).tar.gz /data\n</code></pre></p> </li> <li> <p>Volume Inspection: Check volume usage:    <pre><code># List volumes\ndocker volume ls | grep eemt\n\n# Inspect volume\ndocker volume inspect eemt-results\n\n# Check volume size\ndocker run --rm -v eemt-results:/data alpine du -sh /data\n</code></pre></p> </li> </ol>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#shared-volume-considerations","level":3,"title":"Shared Volume Considerations","text":"<p>When multiple containers share volumes:</p> <pre><code>services:\n  eemt-web:\n    volumes:\n      - shared-data:/app/data:rw\n\n  eemt-worker-1:\n    volumes:\n      - shared-data:/app/data:ro  # Read-only for workers\n\n  eemt-cleanup:\n    volumes:\n      - shared-data:/app/data:rw  # Read-write for cleanup\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#monitoring-in-docker","level":2,"title":"Monitoring in Docker","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#container-logs","level":3,"title":"Container Logs","text":"<p>Monitor cleanup operations:</p> <pre><code># View cleanup logs\ndocker logs eemt-cleanup --follow\n\n# View last 100 lines\ndocker logs eemt-cleanup --tail 100\n\n# View logs since specific time\ndocker logs eemt-cleanup --since 2024-01-20T10:00:00\n\n# Save logs to file\ndocker logs eemt-cleanup &gt; cleanup_logs.txt 2&gt;&amp;1\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#docker-stats","level":3,"title":"Docker Stats","text":"<p>Monitor resource usage:</p> <pre><code># Real-time stats\ndocker stats eemt-cleanup\n\n# One-time snapshot\ndocker stats --no-stream eemt-cleanup\n\n# Format output\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#health-checks","level":3,"title":"Health Checks","text":"<p>Add health checks to monitor cleanup service:</p> <pre><code>HEALTHCHECK --interval=1h --timeout=5m --start-period=30s --retries=3 \\\n  CMD python /app/cleanup_jobs.py --dry-run || exit 1\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#troubleshooting-docker-integration","level":2,"title":"Troubleshooting Docker Integration","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#common-issues","level":3,"title":"Common Issues","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#container-permissions","level":4,"title":"Container Permissions","text":"<pre><code># Check container user\ndocker exec eemt-cleanup whoami\n\n# Fix volume permissions\ndocker exec eemt-cleanup chown -R $(id -u):$(id -g) /app/data\n\n# Run with specific user\ndocker run --user $(id -u):$(id -g) eemt-web cleanup_jobs.py\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#volume-mount-issues","level":4,"title":"Volume Mount Issues","text":"<pre><code># Verify volume mounts\ndocker inspect eemt-cleanup | jq '.[0].Mounts'\n\n# Test volume access\ndocker exec eemt-cleanup ls -la /app/uploads /app/results\n\n# Check volume driver\ndocker volume inspect eemt-results | jq '.[0].Driver'\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#container-networking","level":4,"title":"Container Networking","text":"<pre><code># Check container network\ndocker inspect eemt-cleanup | jq '.[0].NetworkSettings.Networks'\n\n# Test connectivity\ndocker exec eemt-cleanup ping -c 3 eemt-web\n\n# List network details\ndocker network inspect eemt-network\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#debug-mode","level":3,"title":"Debug Mode","text":"<p>Enable detailed debugging in containers:</p> <pre><code># Run with debug environment\ndocker run --rm \\\n  -e EEMT_CLEANUP_LOG_LEVEL=DEBUG \\\n  -e PYTHONUNBUFFERED=1 \\\n  -v eemt-data:/app/data \\\n  eemt-web:latest \\\n  python /app/cleanup_jobs.py --verbose --dry-run\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#best-practices","level":2,"title":"Best Practices","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#1-container-lifecycle-management","level":3,"title":"1. Container Lifecycle Management","text":"<pre><code># Ensure cleanup runs after main services\ndepends_on:\n  eemt-web:\n    condition: service_healthy\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#2-resource-limits","level":3,"title":"2. Resource Limits","text":"<pre><code># Set appropriate limits for cleanup\ndeploy:\n  resources:\n    limits:\n      cpus: '0.5'\n      memory: 512M\n    reservations:\n      cpus: '0.1'\n      memory: 128M\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#3-logging-strategy","level":3,"title":"3. Logging Strategy","text":"<pre><code># Configure logging driver\nlogging:\n  driver: json-file\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#4-security","level":3,"title":"4. Security","text":"<pre><code># Run as non-root user\nuser: \"1000:1000\"\nread_only: true\nsecurity_opt:\n  - no-new-privileges:true\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#5-monitoring-integration","level":3,"title":"5. Monitoring Integration","text":"<pre><code># Prometheus metrics\nports:\n  - \"9090:9090\"\nenvironment:\n  - ENABLE_METRICS=true\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#example-deployment-scripts","level":2,"title":"Example Deployment Scripts","text":"","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#deploy-with-cleanup","level":3,"title":"Deploy with Cleanup","text":"<pre><code>#!/bin/bash\n# deploy-with-cleanup.sh\n\nset -e\n\necho \"Deploying EEMT with cleanup...\"\n\n# Build images\ndocker-compose build\n\n# Start services\ndocker-compose up -d eemt-web\n\n# Wait for web service to be healthy\necho \"Waiting for web service...\"\nuntil docker-compose exec eemt-web curl -f http://localhost:5000/health; do\n  sleep 5\ndone\n\n# Start cleanup service\ndocker-compose up -d eemt-cleanup\n\necho \"Deployment complete!\"\necho \"Web interface: http://localhost:5000\"\necho \"Cleanup service: running (check logs with: docker logs eemt-cleanup)\"\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#manual-cleanup-trigger","level":3,"title":"Manual Cleanup Trigger","text":"<pre><code>#!/bin/bash\n# trigger-cleanup.sh\n\nset -e\n\necho \"Triggering manual cleanup...\"\n\n# Check if dry run\nDRY_RUN=${1:-false}\n\nif [ \"$DRY_RUN\" = \"true\" ]; then\n  echo \"Running in dry-run mode...\"\n  docker exec eemt-web python /app/cleanup_jobs.py --dry-run\nelse\n  echo \"Running actual cleanup...\"\n  docker exec eemt-web python /app/cleanup_jobs.py\nfi\n\necho \"Cleanup complete. Check logs:\"\necho \"  docker logs eemt-web --tail 50\"\n</code></pre>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"docker/cleanup-integration/#summary","level":2,"title":"Summary","text":"<p>Docker integration provides flexible deployment options for the EEMT cleanup system:</p> <ul> <li>Simple integration with existing Docker Compose setups</li> <li>Multiple execution methods from built-in to dedicated containers</li> <li>Orchestration support for Kubernetes, Swarm, and other platforms</li> <li>Volume management with proper permissions and backup strategies</li> <li>Comprehensive monitoring through container logs and metrics</li> </ul> <p>Choose the integration method that best fits your deployment architecture and operational requirements.</p>","path":["Docker","Docker Cleanup Integration Guide"],"tags":[]},{"location":"examples/","level":1,"title":"EEMT Calculation Examples","text":"","path":["Examples"],"tags":[]},{"location":"examples/#overview","level":2,"title":"Overview","text":"<p>This section provides complete, working examples for EEMT calculations across different study areas and use cases, integrating public datasets with the GRASS GIS parallel processing framework.</p>","path":["Examples"],"tags":[]},{"location":"examples/#example-1-arizona-sky-islands-analysis","level":2,"title":"Example 1: Arizona Sky Islands Analysis","text":"","path":["Examples"],"tags":[]},{"location":"examples/#study-area-santa-catalina-mountains","level":3,"title":"Study Area: Santa Catalina Mountains","text":"<p>Based on the validation case study from Rasmussen et al. (2014).</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nArizona Sky Islands EEMT Analysis\nReplicating Rasmussen et al. (2014) study with modern tools\n\"\"\"\n\nimport numpy as np\nimport rasterio\nimport xarray as xr\nfrom pathlib import Path\n\n# Study area definition\nSTUDY_AREA = {\n    'name': 'Santa Catalina Mountains',\n    'bbox': [-110.95, 32.35, -110.70, 32.45],  # [west, south, east, north]\n    'elevation_range': [800, 2800],  # meters\n    'climate_zones': ['desert_scrub', 'oak_woodland', 'pine_forest', 'mixed_conifer']\n}\n\ndef arizona_eemt_example():\n    \"\"\"Complete Arizona EEMT analysis example\"\"\"\n\n    project_dir = Path('arizona_eemt_example')\n    project_dir.mkdir(exist_ok=True)\n\n    print(\"=== Arizona Sky Islands EEMT Analysis ===\")\n    print(f\"Study area: {STUDY_AREA['name']}\")\n    print(f\"Bounding box: {STUDY_AREA['bbox']}\")\n\n    # Step 1: Download data\n    print(\"\\\\n1. Downloading elevation data...\")\n\n    # Use OpenTopography for this region\n    from docs.data_sources import download_opentopo\n    dem_file = download_opentopo(STUDY_AREA['bbox'], 'SRTMGL1')\n\n    # Download higher resolution USGS data if available\n    try:\n        from docs.data_sources import download_3dep\n        dem_high_res = download_3dep(STUDY_AREA['bbox'], '10m')\n        if dem_high_res:\n            dem_file = dem_high_res\n            print(\"‚úì Using 10m USGS elevation data\")\n    except:\n        print(\"‚úì Using 30m SRTM elevation data\")\n\n    # Step 2: Download climate data\n    print(\"\\\\n2. Downloading DAYMET climate data...\")\n\n    climate_dir = project_dir / 'climate'\n    climate_dir.mkdir(exist_ok=True)\n\n    # Download 5 years of DAYMET data\n    for year in range(2015, 2020):\n        for variable in ['tmin', 'tmax', 'prcp', 'vp']:\n            url = f\"https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1328/daymet_v4_daily_na_{variable}_{year}.nc\"\n            output_file = climate_dir / f\"daymet_{variable}_{year}.nc\"\n\n            if not output_file.exists():\n                print(f\"  Downloading {variable} {year}...\")\n                # Download implementation here\n\n    # Step 3: Download vegetation data\n    print(\"\\\\n3. Downloading Landsat NDVI data...\")\n\n    # Use Google Earth Engine or USGS API\n    # Implementation for Landsat NDVI download\n\n    # Step 4: Run EEMT calculations\n    print(\"\\\\n4. Running EEMT calculations...\")\n\n    # Traditional EEMT\n    eemt_trad = calculate_traditional_eemt(\n        dem_file, climate_dir, project_dir / 'eemt_traditional.tif'\n    )\n\n    # Topographic EEMT  \n    eemt_topo = calculate_topographic_eemt(\n        dem_file, climate_dir, project_dir / 'eemt_topographic.tif'\n    )\n\n    # Vegetation EEMT\n    eemt_veg = calculate_vegetation_eemt(\n        dem_file, climate_dir, project_dir / 'eemt_vegetation.tif',\n        ndvi_file=project_dir / 'landsat_ndvi.tif'\n    )\n\n    # Step 5: Analysis and validation\n    print(\"\\\\n5. Analyzing results...\")\n\n    # Load results for comparison\n    with rasterio.open(project_dir / 'eemt_traditional.tif') as src:\n        eemt_trad_data = src.read(1)\n    with rasterio.open(project_dir / 'eemt_topographic.tif') as src:\n        eemt_topo_data = src.read(1)\n    with rasterio.open(project_dir / 'eemt_vegetation.tif') as src:\n        eemt_veg_data = src.read(1)\n\n    # Generate summary statistics\n    methods = {\n        'Traditional': eemt_trad_data,\n        'Topographic': eemt_topo_data, \n        'Vegetation': eemt_veg_data\n    }\n\n    print(\"\\\\nEEMT Summary by Method:\")\n    print(\"-\" * 50)\n    for method, data in methods.items():\n        print(f\"{method:12} | Mean: {np.nanmean(data):6.1f} | Std: {np.nanstd(data):5.1f} | Range: {np.nanmin(data):5.1f}-{np.nanmax(data):5.1f} MJ/m¬≤/yr\")\n\n    # Aspect analysis (north vs south slopes)\n    with rasterio.open(dem_file) as src:\n        elevation = src.read(1)\n\n    # Calculate aspect using GDAL\n    import subprocess\n    aspect_file = project_dir / 'aspect.tif'\n    subprocess.run([\n        'gdaldem', 'aspect', str(dem_file), str(aspect_file)\n    ], check=True)\n\n    with rasterio.open(aspect_file) as src:\n        aspect = src.read(1)\n\n    # Define north vs south facing slopes\n    north_mask = (aspect &gt;= 315) | (aspect &lt;= 45)  # North-facing ¬±45¬∞\n    south_mask = (aspect &gt;= 135) &amp; (aspect &lt;= 225)  # South-facing ¬±45¬∞\n\n    print(\"\\\\nAspect Analysis:\")\n    print(\"-\" * 30)\n    for method, data in methods.items():\n        north_mean = np.nanmean(data[north_mask])\n        south_mean = np.nanmean(data[south_mask])\n        difference = north_mean - south_mean\n        print(f\"{method:12} | North: {north_mean:5.1f} | South: {south_mean:5.1f} | Diff: {difference:+5.1f} MJ/m¬≤/yr\")\n\n    # Elevation gradient analysis\n    elevation_bins = np.arange(800, 2800, 200)\n\n    print(\"\\\\nElevation Gradient Analysis:\")\n    print(\"-\" * 40)\n    print(\"Elevation   | Traditional | Topographic | Vegetation\")\n    print(\"-\" * 40)\n\n    for i in range(len(elevation_bins)-1):\n        elev_mask = (elevation &gt;= elevation_bins[i]) &amp; (elevation &lt; elevation_bins[i+1])\n\n        if np.sum(elev_mask) &gt; 100:  # Sufficient pixels\n            trad_mean = np.nanmean(eemt_trad_data[elev_mask])\n            topo_mean = np.nanmean(eemt_topo_data[elev_mask])\n            veg_mean = np.nanmean(eemt_veg_data[elev_mask])\n\n            print(f\"{elevation_bins[i]:4.0f}-{elevation_bins[i+1]:4.0f}m | {trad_mean:10.1f} | {topo_mean:10.1f} | {veg_mean:9.1f}\")\n\n    print(f\"\\\\n‚úì Arizona Sky Islands analysis completed\")\n    print(f\"Results saved to: {project_dir}\")\n\nif __name__ == '__main__':\n    arizona_eemt_example()\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#example-2-large-watershed-analysis","level":2,"title":"Example 2: Large Watershed Analysis","text":"","path":["Examples"],"tags":[]},{"location":"examples/#multi-scale-eemt-calculation","level":3,"title":"Multi-Scale EEMT Calculation","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nLarge Watershed EEMT Analysis\nDemonstrates tiled processing for continental-scale applications\n\"\"\"\n\nimport numpy as np\nfrom pathlib import Path\nimport multiprocessing as mp\n\ndef large_watershed_example():\n    \"\"\"\n    Large watershed EEMT calculation example\n    Demonstrates handling of large spatial extents\n    \"\"\"\n\n    # Colorado River Basin example\n    STUDY_AREA = {\n        'name': 'Colorado River Basin',\n        'bbox': [-114.0, 32.0, -106.0, 42.0],  # Large region\n        'tile_size': 1.0,  # 1 degree tiles\n        'years': [2010, 2020]\n    }\n\n    project_dir = Path('colorado_river_eemt')\n    project_dir.mkdir(exist_ok=True)\n\n    print(\"=== Large Watershed EEMT Analysis ===\")\n    print(f\"Study area: {STUDY_AREA['name']}\")\n    print(f\"Spatial extent: {STUDY_AREA['bbox']}\")\n\n    # Step 1: Create processing tiles\n    print(\"\\\\n1. Creating processing tiles...\")\n\n    west, south, east, north = STUDY_AREA['bbox']\n    tile_size = STUDY_AREA['tile_size']\n\n    tiles = []\n    tile_id = 0\n\n    for lat in np.arange(south, north, tile_size):\n        for lon in np.arange(west, east, tile_size):\n\n            tile_bbox = [lon, lat, lon + tile_size, lat + tile_size]\n            tiles.append({\n                'id': tile_id,\n                'bbox': tile_bbox,\n                'center': [lon + tile_size/2, lat + tile_size/2]\n            })\n            tile_id += 1\n\n    print(f\"Created {len(tiles)} processing tiles\")\n\n    # Step 2: Process tiles in parallel\n    print(\"\\\\n2. Processing tiles in parallel...\")\n\n    def process_tile(tile):\n        \"\"\"Process single tile\"\"\"\n\n        tile_dir = project_dir / f\"tile_{tile['id']:03d}\"\n        tile_dir.mkdir(exist_ok=True)\n\n        try:\n            # Download data for tile\n            dem_file = download_3dep(tile['bbox'], '30m')\n\n            # Download DAYMET data for tile\n            climate_files = download_daymet_spatial(\n                tile['bbox'], \n                range(STUDY_AREA['years'][0], STUDY_AREA['years'][1]+1),\n                ['tmin', 'tmax', 'prcp']\n            )\n\n            # Calculate EEMT for tile\n            from workflows import run_complete_eemt_pipeline\n\n            results = run_complete_eemt_pipeline(\n                dem_file,\n                tile_dir / 'results',\n                tile_dir / 'climate',\n                STUDY_AREA['years'][0], \n                STUDY_AREA['years'][1]\n            )\n\n            return tile['id'], True, tile_dir / 'results' / 'eemt_topographic.tif'\n\n        except Exception as e:\n            print(f\"Tile {tile['id']} failed: {e}\")\n            return tile['id'], False, None\n\n    # Process tiles in parallel\n    max_workers = min(mp.cpu_count(), 8)  # Limit concurrent downloads\n\n    with mp.Pool(max_workers) as pool:\n        tile_results = pool.map(process_tile, tiles)\n\n    # Step 3: Merge tile results\n    print(\"\\\\n3. Merging tile results...\")\n\n    successful_tiles = [result for result in tile_results if result[1]]\n    failed_tiles = [result for result in tile_results if not result[1]]\n\n    print(f\"Successful tiles: {len(successful_tiles)}\")\n    print(f\"Failed tiles: {len(failed_tiles)}\")\n\n    if successful_tiles:\n        # Merge using GDAL\n        tile_files = [str(result[2]) for result in successful_tiles]\n        merged_file = project_dir / 'colorado_river_eemt_merged.tif'\n\n        import subprocess\n        subprocess.run([\n            'gdal_merge.py', '-o', str(merged_file), '-co', 'COMPRESS=LZW'\n        ] + tile_files, check=True)\n\n        print(f\"‚úì Merged results saved to: {merged_file}\")\n\n    return len(successful_tiles) &gt; 0\n\nif __name__ == '__main__':\n    large_watershed_example()\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#example-3-time-series-analysis","level":2,"title":"Example 3: Time Series Analysis","text":"","path":["Examples"],"tags":[]},{"location":"examples/#multi-decade-eemt-trends","level":3,"title":"Multi-Decade EEMT Trends","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nTime Series EEMT Analysis\nCalculate long-term trends and climate change effects\n\"\"\"\n\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport pandas as pd\n\ndef time_series_eemt_example():\n    \"\"\"\n    Multi-decade EEMT time series analysis\n    Demonstrates climate change impact assessment\n    \"\"\"\n\n    # Example: Western US mountain region\n    STUDY_AREA = {\n        'name': 'Rocky Mountain National Park',\n        'bbox': [-105.8, 40.1, -105.5, 40.5],\n        'years': [1980, 2023],  # Full DAYMET record\n        'elevation_bands': [2000, 2500, 3000, 3500]  # Analyze by elevation zone\n    }\n\n    project_dir = Path('rocky_mountain_time_series')\n    project_dir.mkdir(exist_ok=True)\n\n    print(\"=== Multi-Decade EEMT Time Series Analysis ===\")\n    print(f\"Study area: {STUDY_AREA['name']}\")\n    print(f\"Time period: {STUDY_AREA['years'][0]}-{STUDY_AREA['years'][1]} ({STUDY_AREA['years'][1] - STUDY_AREA['years'][0] + 1} years)\")\n\n    # Step 1: Download multi-decade climate data\n    print(\"\\\\n1. Downloading multi-decade climate data...\")\n\n    climate_data = {}\n    years = range(STUDY_AREA['years'][0], STUDY_AREA['years'][1] + 1)\n\n    for variable in ['tmin', 'tmax', 'prcp']:\n\n        # Download and concatenate all years\n        annual_files = []\n        for year in years:\n            url = f\"https://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1328/daymet_v4_daily_na_{variable}_{year}.nc\"\n\n            # Subset to study area\n            ds = xr.open_dataset(url)\n            bbox = STUDY_AREA['bbox']\n            subset = ds.sel(x=slice(bbox[0], bbox[2]), y=slice(bbox[1], bbox[3]))\n            annual_files.append(subset)\n\n        # Concatenate all years\n        climate_data[variable] = xr.concat(annual_files, dim='time')\n        print(f\"‚úì {variable}: {len(years)} years loaded\")\n\n    # Step 2: Calculate annual EEMT time series\n    print(\"\\\\n2. Calculating annual EEMT time series...\")\n\n    # Resample to annual means\n    annual_climate = {}\n    for var, data in climate_data.items():\n        annual_climate[var] = data.resample(time='1Y').mean()\n\n    # Calculate EEMT for each year\n    eemt_time_series = []\n\n    for year_idx in range(len(years)):\n\n        year = years[year_idx]\n        print(f\"  Processing year {year}...\")\n\n        # Extract climate for this year\n        temp_mean = (annual_climate['tmin'].isel(time=year_idx) + \n                    annual_climate['tmax'].isel(time=year_idx)) / 2\n        precip = annual_climate['prcp'].isel(time=year_idx)\n\n        # Simple EEMT calculation (traditional method)\n        # Calculate PET (simplified Hamon)\n        temp_celsius = temp_mean.values - 273.15\n        pet = 0.0023 * (temp_celsius + 17.8) * np.sqrt(np.maximum(0, temp_celsius)) * 58.93\n\n        # Effective precipitation\n        effective_precip = np.maximum(0, precip.values - pet)\n\n        # NPP (Lieth method)\n        npp = np.where(temp_celsius &gt; 0, \n                      3000 * (1 - np.exp(1.315 - 0.119 * temp_celsius))**(-1),\n                      0)\n\n        # EEMT components\n        h_bio = 22e6  # J/kg\n        c_w = 4180   # J/kg/K\n\n        e_bio = (npp / 1000) * h_bio / (365 * 24 * 3600)  # W/m¬≤\n        e_ppt = (effective_precip / 1000) * c_w * np.maximum(0, temp_celsius) / (365 * 24 * 3600)\n\n        eemt = (e_bio + e_ppt) * 365 * 24 * 3600 / 1e6  # MJ/m¬≤/yr\n\n        # Store results\n        eemt_time_series.append({\n            'year': year,\n            'eemt_mean': np.nanmean(eemt),\n            'eemt_std': np.nanstd(eemt),\n            'temp_mean': np.nanmean(temp_celsius),\n            'precip_mean': np.nanmean(precip.values),\n            'npp_mean': np.nanmean(npp)\n        })\n\n    # Step 3: Trend analysis\n    print(\"\\\\n3. Analyzing trends...\")\n\n    # Convert to DataFrame\n    df = pd.DataFrame(eemt_time_series)\n\n    # Calculate trends\n    trends = {}\n    for variable in ['eemt_mean', 'temp_mean', 'precip_mean', 'npp_mean']:\n        slope, intercept, r_value, p_value, std_err = stats.linregress(df['year'], df[variable])\n\n        trends[variable] = {\n            'slope': slope,\n            'r_squared': r_value**2,\n            'p_value': p_value,\n            'trend_per_decade': slope * 10\n        }\n\n    # Print trend analysis\n    print(\"\\\\nTrend Analysis (1980-2023):\")\n    print(\"-\" * 60)\n    print(f\"{'Variable':&lt;15} | {'Trend/Decade':&lt;12} | {'R¬≤':&lt;6} | {'p-value':&lt;8}\")\n    print(\"-\" * 60)\n\n    trend_labels = {\n        'eemt_mean': 'EEMT',\n        'temp_mean': 'Temperature', \n        'precip_mean': 'Precipitation',\n        'npp_mean': 'NPP'\n    }\n\n    units = {\n        'eemt_mean': 'MJ/m¬≤/yr',\n        'temp_mean': '¬∞C',\n        'precip_mean': 'mm/yr', \n        'npp_mean': 'g/m¬≤/yr'\n    }\n\n    for var, trend in trends.items():\n        label = trend_labels[var]\n        unit = units[var]\n        print(f\"{label:&lt;15} | {trend['trend_per_decade']:+8.3f} {unit:&lt;3} | {trend['r_squared']:&lt;6.3f} | {trend['p_value']:&lt;8.4f}\")\n\n    # Step 4: Generate visualizations\n    print(\"\\\\n4. Generating time series plots...\")\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    fig.suptitle(f\"{STUDY_AREA['name']} EEMT Time Series (1980-2023)\")\n\n    # EEMT time series\n    axes[0,0].plot(df['year'], df['eemt_mean'], 'b-', linewidth=2)\n    axes[0,0].fill_between(df['year'], \n                          df['eemt_mean'] - df['eemt_std'],\n                          df['eemt_mean'] + df['eemt_std'], \n                          alpha=0.3)\n    axes[0,0].set_title('EEMT Time Series')\n    axes[0,0].set_ylabel('EEMT (MJ/m¬≤/yr)')\n\n    # Temperature trend\n    axes[0,1].plot(df['year'], df['temp_mean'], 'r-', linewidth=2)\n    axes[0,1].set_title('Temperature Trend')\n    axes[0,1].set_ylabel('Temperature (¬∞C)')\n\n    # Precipitation trend  \n    axes[1,0].plot(df['year'], df['precip_mean'], 'g-', linewidth=2)\n    axes[1,0].set_title('Precipitation Trend')\n    axes[1,0].set_ylabel('Precipitation (mm/yr)')\n\n    # NPP trend\n    axes[1,1].plot(df['year'], df['npp_mean'], 'orange', linewidth=2)\n    axes[1,1].set_title('NPP Trend')\n    axes[1,1].set_ylabel('NPP (g/m¬≤/yr)')\n\n    for ax in axes.flat:\n        ax.grid(True, alpha=0.3)\n        ax.set_xlabel('Year')\n\n    plt.tight_layout()\n    plt.savefig(project_dir / 'eemt_time_series.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    # Step 5: Climate sensitivity analysis\n    print(\"\\\\n5. Analyzing climate sensitivity...\")\n\n    # Calculate correlations between EEMT and climate drivers\n    correlations = {}\n    for climate_var in ['temp_mean', 'precip_mean']:\n        r, p = stats.pearsonr(df['eemt_mean'], df[climate_var])\n        correlations[climate_var] = {'correlation': r, 'p_value': p}\n\n    print(\"\\\\nEEMT-Climate Correlations:\")\n    print(\"-\" * 35)\n    for var, corr in correlations.items():\n        var_name = 'Temperature' if 'temp' in var else 'Precipitation'\n        print(f\"{var_name:&lt;13} | r={corr['correlation']:+6.3f} | p={corr['p_value']:&lt;6.4f}\")\n\n    # Save time series data\n    df.to_csv(project_dir / 'eemt_time_series.csv', index=False)\n\n    print(f\"\\\\n‚úì Time series analysis completed\")\n    print(f\"Data saved to: {project_dir / 'eemt_time_series.csv'}\")\n    print(f\"Plots saved to: {project_dir / 'eemt_time_series.png'}\")\n\nif __name__ == '__main__':\n    time_series_eemt_example()\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#example-3-urban-heat-island-analysis","level":2,"title":"Example 3: Urban Heat Island Analysis","text":"","path":["Examples"],"tags":[]},{"location":"examples/#eemt-in-urban-environments","level":3,"title":"EEMT in Urban Environments","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nUrban Heat Island EEMT Analysis\nDemonstrates EEMT application to urban environments\n\"\"\"\n\ndef urban_heat_island_example():\n    \"\"\"\n    Urban EEMT calculation example\n    Phoenix, Arizona metropolitan area\n    \"\"\"\n\n    STUDY_AREA = {\n        'name': 'Phoenix Metropolitan Area',\n        'bbox': [-112.5, 33.2, -111.5, 33.8],\n        'urban_center': [-112.0, 33.5],\n        'land_cover_classes': ['urban', 'desert', 'agriculture', 'mountain']\n    }\n\n    project_dir = Path('phoenix_urban_eemt')\n    project_dir.mkdir(exist_ok=True)\n\n    print(\"=== Urban Heat Island EEMT Analysis ===\")\n    print(f\"Study area: {STUDY_AREA['name']}\")\n\n    # Step 1: Download high-resolution data\n    print(\"\\\\n1. Downloading urban-scale data...\")\n\n    # High-resolution DEM for urban analysis\n    dem_file = download_3dep(STUDY_AREA['bbox'], '1m')  # 1m lidar if available\n\n    # Landsat thermal and optical data\n    landsat_data = download_landsat_collection(\n        STUDY_AREA['bbox'], \n        '2020-06-01', '2020-08-31',  # Summer period\n        ['red', 'nir', 'thermal']\n    )\n\n    # Urban land cover data\n    # Use NLCD (National Land Cover Database) for US\n    nlcd_url = \"https://www.mrlc.gov/geoserver/mrlc_download/NLCD_2019_Land_Cover_L48/wcs\"\n\n    # Step 2: Urban-specific EEMT modifications\n    print(\"\\\\n2. Calculating urban-modified EEMT...\")\n\n    # Urban heat island temperature adjustment\n    def calculate_urban_temperature_effect(land_cover, base_temperature):\n        \"\"\"Apply urban heat island temperature corrections\"\"\"\n\n        # Urban heat island intensity by land cover class\n        uhi_correction = {\n            'developed_high': +4.0,      # ¬∞C increase in dense urban\n            'developed_medium': +2.5,    # ¬∞C increase in suburban  \n            'developed_low': +1.0,       # ¬∞C increase in low density\n            'developed_open': +0.5,      # ¬∞C increase in parks/open space\n            'natural': 0.0               # No UHI effect\n        }\n\n        # Apply corrections based on land cover\n        temp_adjusted = base_temperature.copy()\n        for class_name, correction in uhi_correction.items():\n            mask = land_cover == class_name\n            temp_adjusted[mask] += correction\n\n        return temp_adjusted\n\n    # Urban albedo effects\n    def calculate_urban_albedo(land_cover):\n        \"\"\"Calculate albedo by urban land cover type\"\"\"\n\n        albedo_values = {\n            'developed_high': 0.15,     # Dark urban surfaces\n            'developed_medium': 0.18,   # Mixed urban/suburban\n            'developed_low': 0.22,      # Suburban with vegetation\n            'developed_open': 0.25,     # Parks and open space\n            'natural': 0.20             # Natural vegetation\n        }\n\n        albedo = np.zeros_like(land_cover, dtype=np.float32)\n        for class_name, albedo_val in albedo_values.items():\n            mask = land_cover == class_name\n            albedo[mask] = albedo_val\n\n        return albedo\n\n    # Urban NPP modifications\n    def calculate_urban_npp(land_cover, base_npp):\n        \"\"\"Modify NPP for urban land cover effects\"\"\"\n\n        npp_factors = {\n            'developed_high': 0.1,      # Very low NPP in dense urban\n            'developed_medium': 0.3,    # Reduced NPP in suburban\n            'developed_low': 0.6,       # Moderate NPP with some vegetation\n            'developed_open': 0.8,      # Near-natural NPP in parks\n            'natural': 1.0              # Unmodified NPP\n        }\n\n        npp_urban = base_npp.copy()\n        for class_name, factor in npp_factors.items():\n            mask = land_cover == class_name\n            npp_urban[mask] *= factor\n\n        return npp_urban\n\n    # Step 3: Run urban EEMT calculation\n    print(\"\\\\n3. Running urban EEMT calculation...\")\n\n    # Load base data\n    with rasterio.open(dem_file) as src:\n        elevation = src.read(1)\n        profile = src.profile\n\n    # Load land cover (placeholder - implement actual NLCD loading)\n    land_cover = np.random.choice(['developed_high', 'developed_medium', 'natural'], \n                                 size=elevation.shape)\n\n    # Load climate data\n    climate_data = load_daymet_annual_means(STUDY_AREA['bbox'], 2020)\n\n    # Apply urban modifications\n    temp_urban = calculate_urban_temperature_effect(land_cover, climate_data['temperature'])\n    albedo_urban = calculate_urban_albedo(land_cover) \n\n    # Calculate solar radiation with urban albedo\n    # (This would integrate with r.sun calculations)\n\n    # Calculate urban NPP\n    base_npp = calculate_npp_lieth(temp_urban, climate_data['precipitation'])\n    npp_urban = calculate_urban_npp(land_cover, base_npp)\n\n    # Calculate urban EEMT\n    h_bio = 22e6\n    c_w = 4180\n\n    e_bio = (npp_urban / 1000) * h_bio / (365 * 24 * 3600)\n    e_ppt = (climate_data['effective_precipitation'] / 1000) * c_w * np.maximum(0, temp_urban - 273.15) / (365 * 24 * 3600)\n\n    eemt_urban = (e_bio + e_ppt) * 365 * 24 * 3600 / 1e6\n\n    # Step 4: Urban vs natural comparison\n    print(\"\\\\n4. Comparing urban vs natural EEMT...\")\n\n    # Calculate natural EEMT (without urban effects)\n    temp_natural = climate_data['temperature']\n    npp_natural = calculate_npp_lieth(temp_natural, climate_data['precipitation'])\n\n    e_bio_natural = (npp_natural / 1000) * h_bio / (365 * 24 * 3600)\n    e_ppt_natural = (climate_data['effective_precipitation'] / 1000) * c_w * np.maximum(0, temp_natural - 273.15) / (365 * 24 * 3600)\n    eemt_natural = (e_bio_natural + e_ppt_natural) * 365 * 24 * 3600 / 1e6\n\n    # Calculate urban effect\n    urban_effect = eemt_urban - eemt_natural\n\n    # Analyze by land cover class\n    print(\"\\\\nUrban EEMT Effects by Land Cover:\")\n    print(\"-\" * 50)\n    print(f\"{'Land Cover':&lt;18} | {'Mean EEMT':&lt;10} | {'Urban Effect':&lt;12}\")\n    print(\"-\" * 50)\n\n    for class_name in ['developed_high', 'developed_medium', 'developed_low', 'natural']:\n        mask = land_cover == class_name\n        if np.sum(mask) &gt; 100:  # Sufficient pixels\n            mean_eemt = np.nanmean(eemt_urban[mask])\n            mean_effect = np.nanmean(urban_effect[mask])\n            print(f\"{class_name:&lt;18} | {mean_eemt:8.1f} | {mean_effect:+10.1f}\")\n\n    # Step 5: Save results\n    print(\"\\\\n5. Saving urban analysis results...\")\n\n    outputs = {\n        'eemt_urban.tif': eemt_urban,\n        'eemt_natural.tif': eemt_natural,\n        'urban_effect.tif': urban_effect,\n        'temperature_urban.tif': temp_urban,\n        'npp_urban.tif': npp_urban,\n        'land_cover.tif': land_cover.astype(np.int16)\n    }\n\n    for filename, data in outputs.items():\n        with rasterio.open(project_dir / filename, 'w', **profile) as dst:\n            dst.write(data.astype(np.float32), 1)\n\n    print(f\"‚úì Urban heat island analysis completed\")\n    print(f\"Results saved to: {project_dir}\")\n\nif __name__ == '__main__':\n    urban_heat_island_example()\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#example-4-climate-change-scenarios","level":2,"title":"Example 4: Climate Change Scenarios","text":"","path":["Examples"],"tags":[]},{"location":"examples/#future-eemt-projections","level":3,"title":"Future EEMT Projections","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nClimate Change EEMT Projections\nCalculate EEMT under future climate scenarios\n\"\"\"\n\ndef climate_change_scenarios_example():\n    \"\"\"\n    EEMT calculation for climate change scenarios\n    Using bias-corrected climate model projections\n    \"\"\"\n\n    SCENARIOS = {\n        'baseline': {\n            'period': [1980, 2010],\n            'description': 'Historical baseline'\n        },\n        'near_future': {\n            'period': [2020, 2050], \n            'temp_change': +2.0,  # ¬∞C warming\n            'precip_change': -10,  # % precipitation change\n            'description': 'Near-term projections'\n        },\n        'far_future': {\n            'period': [2070, 2100],\n            'temp_change': +4.0,  # ¬∞C warming  \n            'precip_change': -20,  # % precipitation change\n            'description': 'End-of-century projections'\n        }\n    }\n\n    project_dir = Path('climate_scenarios_eemt')\n    project_dir.mkdir(exist_ok=True)\n\n    print(\"=== Climate Change EEMT Scenarios ===\")\n\n    # Calculate EEMT for each scenario\n    scenario_results = {}\n\n    for scenario_name, scenario in SCENARIOS.items():\n\n        print(f\"\\\\nCalculating {scenario_name} scenario...\")\n        print(f\"  Description: {scenario['description']}\")\n\n        if scenario_name == 'baseline':\n            # Use historical data\n            eemt_result = calculate_historical_eemt(project_dir / scenario_name)\n        else:\n            # Apply climate change modifications\n            eemt_result = calculate_future_eemt(\n                project_dir / scenario_name,\n                temp_change=scenario['temp_change'],\n                precip_change=scenario['precip_change']\n            )\n\n        scenario_results[scenario_name] = eemt_result\n        print(f\"‚úì {scenario_name} completed\")\n\n    # Compare scenarios\n    print(\"\\\\nClimate Change Impact Analysis:\")\n    print(\"-\" * 60)\n    print(f\"{'Scenario':&lt;15} | {'Mean EEMT':&lt;10} | {'Change from Baseline':&lt;18}\")\n    print(\"-\" * 60)\n\n    baseline_mean = np.nanmean(scenario_results['baseline'])\n\n    for scenario_name, result in scenario_results.items():\n        mean_eemt = np.nanmean(result)\n\n        if scenario_name == 'baseline':\n            change = 0.0\n            change_pct = 0.0\n        else:\n            change = mean_eemt - baseline_mean\n            change_pct = (change / baseline_mean) * 100\n\n        print(f\"{scenario_name:&lt;15} | {mean_eemt:8.1f} | {change:+8.1f} ({change_pct:+5.1f}%)\")\n\n    # Spatial analysis of changes\n    print(\"\\\\nSpatial Analysis of Climate Impacts:\")\n\n    for scenario_name in ['near_future', 'far_future']:\n        change_map = scenario_results[scenario_name] - scenario_results['baseline']\n\n        print(f\"\\\\n{scenario_name.replace('_', ' ').title()} Changes:\")\n        print(f\"  Mean change: {np.nanmean(change_map):+6.2f} MJ/m¬≤/yr\")\n        print(f\"  Max increase: {np.nanmax(change_map):+6.2f} MJ/m¬≤/yr\")\n        print(f\"  Max decrease: {np.nanmin(change_map):+6.2f} MJ/m¬≤/yr\")\n\n        # Save change maps\n        change_file = project_dir / f'eemt_change_{scenario_name}.tif'\n        with rasterio.open(change_file, 'w', **profile) as dst:\n            dst.write(change_map.astype(np.float32), 1)\n\n    print(f\"\\\\n‚úì Climate change analysis completed\")\n    print(f\"Results saved to: {project_dir}\")\n\nif __name__ == '__main__':\n    climate_change_scenarios_example()\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#running-the-examples","level":2,"title":"Running the Examples","text":"","path":["Examples"],"tags":[]},{"location":"examples/#prerequisites","level":3,"title":"Prerequisites","text":"<pre><code># Ensure all dependencies are installed\npip install -r requirements.txt\n\n# Verify GRASS GIS installation\ngrass --version\n\n# Check data access\npython -c \"import requests; print('‚úì Internet connection OK')\"\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#example-execution","level":3,"title":"Example Execution","text":"<pre><code># Run Arizona Sky Islands example\ncd docs/examples/\npython arizona_sky_islands.py\n\n# Run large watershed example  \npython large_watershed.py --max-workers 4\n\n# Run urban heat island example\npython urban_heat_island.py --resolution 1m\n\n# Run climate scenarios example\npython climate_scenarios.py --scenarios all\n</code></pre>","path":["Examples"],"tags":[]},{"location":"examples/#expected-results","level":3,"title":"Expected Results","text":"","path":["Examples"],"tags":[]},{"location":"examples/#arizona-sky-islands","level":4,"title":"Arizona Sky Islands","text":"<ul> <li>EEMT Range: 5-45 MJ/m¬≤/yr across elevation gradient</li> <li>Aspect Effect: ~5 MJ/m¬≤/yr higher on north-facing slopes</li> <li>Elevation Effect: 300m elevation ‚âà north-facing aspect</li> </ul>","path":["Examples"],"tags":[]},{"location":"examples/#large-watershed","level":4,"title":"Large Watershed","text":"<ul> <li>Processing Time: ~2-6 hours for Colorado River Basin</li> <li>Tile Success Rate: &gt;95% with robust error handling</li> <li>Spatial Patterns: Clear elevation and latitude gradients</li> </ul>","path":["Examples"],"tags":[]},{"location":"examples/#urban-heat-island","level":4,"title":"Urban Heat Island","text":"<ul> <li>Urban Effect: +2-8 MJ/m¬≤/yr in developed areas</li> <li>Land Cover Sensitivity: Strong correlation with development density</li> <li>Seasonal Variation: Greatest effects during summer months</li> </ul>","path":["Examples"],"tags":[]},{"location":"examples/#climate-scenarios","level":4,"title":"Climate Scenarios","text":"<ul> <li>Temperature Sensitivity: ~2-4 MJ/m¬≤/yr per ¬∞C warming</li> <li>Precipitation Sensitivity: Varies by baseline aridity</li> <li>Spatial Heterogeneity: Mountain regions most sensitive</li> </ul> <p>These examples demonstrate the flexibility and power of the EEMT framework for diverse earth system applications, from local ecosystem studies to continental-scale climate impact assessments.</p>","path":["Examples"],"tags":[]},{"location":"getting-started/","level":1,"title":"Getting Started with EEMT","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#overview","level":2,"title":"Overview","text":"<p>This guide will help you set up the EEMT calculation environment and run your first analysis. EEMT calculations require topographic data, climate data, and specialized GIS software.</p>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#system-requirements","level":2,"title":"System Requirements","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#minimum-hardware","level":3,"title":"Minimum Hardware","text":"<ul> <li>CPU: 4 cores (8+ recommended for parallel processing)</li> <li>RAM: 8 GB (16+ GB recommended for large datasets)</li> <li>Storage: 50 GB free space (more for large study areas)</li> <li>GPU: Optional but recommended for r.sun calculations</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#software-dependencies","level":3,"title":"Software Dependencies","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#core-gis-stack","level":4,"title":"Core GIS Stack","text":"<pre><code># Ubuntu/Debian installation\nsudo apt update\nsudo apt install grass grass-dev gdal-bin python3-gdal python3-rasterio\n\n# macOS (via Homebrew)\nbrew install grass gdal python\n\n# Windows: Use OSGeo4W installer or Conda\nconda install -c conda-forge grass gdal rasterio\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#python-environment","level":4,"title":"Python Environment","text":"<pre><code># Create virtual environment\npython3 -m venv eemt-env\nsource eemt-env/bin/activate  # Linux/macOS\n# eemt-env\\Scripts\\activate   # Windows\n\n# Install required packages\npip install -r requirements.txt\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#required-python-packages","level":4,"title":"Required Python Packages","text":"<pre><code>numpy&gt;=1.24\npandas&gt;=2.0\nxarray&gt;=2024.1\nrasterio&gt;=1.3\ngeopandas&gt;=0.14\nmatplotlib&gt;=3.7\nscipy&gt;=1.11\nrequests&gt;=2.28\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#core-concepts","level":2,"title":"Core Concepts","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#eemt-components","level":3,"title":"EEMT Components","text":"<p>Effective Energy and Mass Transfer quantifies energy flux to the Critical Zone:</p> <pre><code>EEMT = E_BIO + E_PPT [MJ m‚Åª¬≤ yr‚Åª¬π]\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#biological-energy-e_bio","level":4,"title":"Biological Energy (E_BIO)","text":"<ul> <li>Energy from photosynthesis and primary production</li> <li>Calculated from Net Primary Production (NPP)</li> <li>Formula: <code>E_BIO = NPP √ó 22 MJ/kg</code></li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#precipitation-energy-e_ppt","level":4,"title":"Precipitation Energy (E_PPT)","text":"<ul> <li>Thermal energy from effective precipitation</li> <li>Water available for subsurface processes</li> <li>Formula: <code>E_PPT = F √ó c_w √ó ŒîT</code></li> <li>F = effective precipitation flux [kg m‚Åª¬≤ s‚Åª¬π]</li> <li>c_w = specific heat of water [4.18 √ó 10¬≥ J kg‚Åª¬π K‚Åª¬π]</li> <li>ŒîT = temperature difference from 273.15K</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#calculation-approaches","level":3,"title":"Calculation Approaches","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#1-traditional-eemt-eemt_trad","level":4,"title":"1. Traditional EEMT (EEMT_TRAD)","text":"<ul> <li>Uses simple climate averages</li> <li>No topographic or vegetation effects</li> <li>Good for regional comparisons</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#2-topographic-eemt-eemt_topo","level":4,"title":"2. Topographic EEMT (EEMT_TOPO)","text":"<ul> <li>Incorporates slope, aspect, and solar radiation</li> <li>Mass-conservative water redistribution</li> <li>Accounts for local microclimates</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#3-vegetation-eemt-eemt_topo-veg","level":4,"title":"3. Vegetation EEMT (EEMT_TOPO-VEG)","text":"<ul> <li>Adds vegetation structure effects</li> <li>Uses Leaf Area Index (LAI) and canopy height</li> <li>Most accurate for site-specific analyses</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#basic-workflow","level":2,"title":"Basic Workflow","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#step-1-prepare-input-data","level":3,"title":"Step 1: Prepare Input Data","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#digital-elevation-model-dem","level":4,"title":"Digital Elevation Model (DEM)","text":"<pre><code># Download USGS 3DEP data (example for Arizona)\nwget \"https://cloud.sdsc.edu/v1/AUTH_opentopography/Raster/SRTMGL1/SRTMGL1_srtm.zip\"\n\n# Or use OpenTopography API\ncurl -X GET \"https://portal.opentopography.org/API/globaldem\" \\\n  -G -d \"demtype=SRTMGL1\" \\\n  -d \"south=32.0\" -d \"north=32.5\" \\\n  -d \"west=-111.0\" -d \"east=-110.5\" \\\n  -d \"outputFormat=GTiff\"\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#climate-data","level":4,"title":"Climate Data","text":"<pre><code># DAYMET data (automated download in workflow)\n# Or manual download from ORNL DAAC\n# https://daymet.ornl.gov/\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#step-2-set-up-grass-gis-environment","level":3,"title":"Step 2: Set Up GRASS GIS Environment","text":"<pre><code># Create new GRASS location from DEM\ngrass -c your_dem.tif ~/grassdata/eemt_project/PERMANENT\n\n# Verify projection\ng.proj -p\n\n# Set computational region  \ng.region raster=your_dem -p\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#step-3-run-solar-radiation-analysis","level":3,"title":"Step 3: Run Solar Radiation Analysis","text":"<pre><code># Basic solar radiation for single day\nr.sun elevation=dem day=180 glob_rad=solar_jun29\n\n# Multi-day parallel processing\npython sol/run-workflow --step 15 --num_threads 4 your_dem.tif\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#step-4-calculate-eemt","level":3,"title":"Step 4: Calculate EEMT","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#simple-eemt-calculation","level":4,"title":"Simple EEMT Calculation","text":"<pre><code>import numpy as np\nimport rasterio\n\n# Load required data\nwith rasterio.open('solar_annual.tif') as src:\n    solar_radiation = src.read(1)\n\nwith rasterio.open('precipitation.tif') as src:\n    precipitation = src.read(1)\n\nwith rasterio.open('temperature.tif') as src:\n    temperature = src.read(1)\n\n# Calculate NPP (simplified)\nnpp = 3000 * (1 - np.exp(1.315 - 0.119 * temperature))  # kg/m¬≤/yr\nnpp[precipitation &lt;= 0] = 0  # No production without water\n\n# Calculate energy components\ne_bio = npp * 22e6 / (365 * 24 * 3600)  # Convert to W/m¬≤\ne_ppt = precipitation * 4180 * (temperature - 273.15) / (365 * 24 * 3600)\ne_ppt[e_ppt &lt; 0] = 0  # No energy below freezing\n\n# Calculate EEMT\neemt = e_bio + e_ppt  # W/m¬≤\neemt_annual = eemt * 365 * 24 * 3600 / 1e6  # MJ/m¬≤/yr\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#using-the-eemt-workflow","level":4,"title":"Using the EEMT Workflow","text":"<pre><code># Full EEMT calculation with topography\ncd eemt/\npython run-workflow --start-year 2015 --end-year 2020 \\\n  --step 15 --num_threads 8 \\\n  --output eemt_results/ \\\n  your_dem.tif\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#validation-and-quality-control","level":2,"title":"Validation and Quality Control","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#check-input-data-quality","level":3,"title":"Check Input Data Quality","text":"<pre><code># Verify DEM characteristics\nimport rasterio\nwith rasterio.open('dem.tif') as src:\n    print(f\"Projection: {src.crs}\")\n    print(f\"Resolution: {src.res}\")\n    print(f\"Bounds: {src.bounds}\")\n    print(f\"Data type: {src.dtypes[0]}\")\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#validate-eemt-results","level":3,"title":"Validate EEMT Results","text":"<pre><code># Check EEMT value ranges\neemt_stats = {\n    'min': np.nanmin(eemt_annual),\n    'max': np.nanmax(eemt_annual), \n    'mean': np.nanmean(eemt_annual),\n    'std': np.nanstd(eemt_annual)\n}\n\nprint(f\"EEMT range: {eemt_stats['min']:.1f} - {eemt_stats['max']:.1f} MJ/m¬≤/yr\")\n\n# Expected ranges by climate zone:\n# Arid: 5-15 MJ/m¬≤/yr\n# Semiarid: 15-25 MJ/m¬≤/yr  \n# Humid: 25-50 MJ/m¬≤/yr\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#common-issues-and-solutions","level":2,"title":"Common Issues and Solutions","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#memory-issues","level":3,"title":"Memory Issues","text":"<pre><code># Process large DEMs in tiles\ngdal_retile.py -ps 1000 1000 -targetDir tiles/ large_dem.tif\n\n# Or use chunked processing with Dask\nimport dask.array as da\ndem_chunked = da.from_array(dem, chunks=(1000, 1000))\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#projection-problems","level":3,"title":"Projection Problems","text":"<pre><code># Reproject DEM to match climate data\ngdalwarp -t_srs EPSG:4326 input_dem.tif output_dem.tif\n\n# Check coordinate reference systems\ngdalinfo dem.tif | grep -i \"coordinate system\"\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#missing-climate-data","level":3,"title":"Missing Climate Data","text":"<pre><code># Download DAYMET data programmatically\npython scripts/download_daymet.py --bbox -111.0,32.0,-110.5,32.5 \\\n  --years 2015-2020 --variables tmin,tmax,prcp\n</code></pre>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#next-steps","level":2,"title":"Next Steps","text":"<p>Once you have basic EEMT calculations working:</p> <ol> <li>Data Sources Guide - Access higher resolution data</li> <li>GRASS GIS Tutorials - Advanced terrain analysis</li> <li>Workflow Examples - Real-world case studies</li> <li>API Reference - Function documentation</li> </ol>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["Getting Started"],"tags":[]},{"location":"getting-started/#parallel-processing","level":3,"title":"Parallel Processing","text":"<ul> <li>Use <code>--num_threads</code> parameter for CPU cores</li> <li>Enable GPU acceleration for r.sun when available  </li> <li>Process multiple years simultaneously</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#memory-management","level":3,"title":"Memory Management","text":"<ul> <li>Tile large datasets with <code>gdal_retile.py</code></li> <li>Use compressed GeoTIFF outputs (<code>-co COMPRESS=LZW</code>)</li> <li>Monitor memory usage with <code>htop</code> or Task Manager</li> </ul>","path":["Getting Started"],"tags":[]},{"location":"getting-started/#storage-optimization","level":3,"title":"Storage Optimization","text":"<ul> <li>Use Cloud-Optimized GeoTIFF (COG) format</li> <li>Compress intermediate files</li> <li>Clean up temporary GRASS locations</li> </ul> <p>For detailed technical information, see the scientific background and API documentation.</p>","path":["Getting Started"],"tags":[]},{"location":"getting-started/cleanup-scripts/","level":1,"title":"Cleanup Scripts User Guide","text":"<p>This guide provides step-by-step instructions for using the EEMT job data cleanup scripts to manage disk space and maintain system performance.</p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#quick-start","level":2,"title":"Quick Start","text":"<p>The fastest way to get started with automated cleanup:</p> <pre><code># Navigate to web interface directory\ncd web-interface/\n\n# Set up automated daily cleanup (user-level)\n./setup_cleanup_cron.sh --user --method cron\n\n# Test cleanup in dry-run mode\npython cleanup_jobs.py --dry-run\n</code></pre> <p>Default Configuration</p> <p>The default settings work well for most users: - Successful job data kept for 7 days - Failed job data removed after 12 hours - Runs daily at 2:00 AM local time - Job configurations always preserved</p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#using-the-cleanup-script","level":2,"title":"Using the Cleanup Script","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#basic-commands","level":3,"title":"Basic Commands","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#manual-cleanup","level":4,"title":"Manual Cleanup","text":"<p>Run cleanup immediately with default settings:</p> <pre><code>python cleanup_jobs.py\n</code></pre> <p>Expected output: <pre><code>2024-01-20 14:30:15 - cleanup_jobs - INFO - Starting EEMT job data cleanup process\n2024-01-20 14:30:15 - cleanup_jobs - INFO - Found 3 successful jobs for data cleanup\n2024-01-20 14:30:15 - cleanup_jobs - INFO - Found 1 failed jobs for complete deletion\n2024-01-20 14:30:16 - cleanup_jobs - INFO - Deleted results directory: results/job-20240113-095423 (8234.5 MB)\n2024-01-20 14:30:17 - cleanup_jobs - INFO - === CLEANUP SUMMARY ===\n2024-01-20 14:30:17 - cleanup_jobs - INFO - Successful jobs processed: 3\n2024-01-20 14:30:17 - cleanup_jobs - INFO - Failed jobs processed: 1\n2024-01-20 14:30:17 - cleanup_jobs - INFO - Total disk space freed: 15678.9 MB\n</code></pre></p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#preview-mode-dry-run","level":4,"title":"Preview Mode (Dry Run)","text":"<p>See what would be deleted without making changes:</p> <pre><code>python cleanup_jobs.py --dry-run\n</code></pre> <p>Sample output: <pre><code>2024-01-20 14:35:00 - cleanup_jobs - INFO - === DRY RUN MODE - NO CHANGES WILL BE MADE ===\n2024-01-20 14:35:00 - cleanup_jobs - INFO - [DRY RUN] Would delete results directory: results/job-20240113-095423 (8234.5 MB)\n2024-01-20 14:35:00 - cleanup_jobs - INFO - [DRY RUN] Would delete uploaded DEM: uploads/job-20240113-095423_dem.tif (125.3 MB)\n2024-01-20 14:35:00 - cleanup_jobs - INFO - [DRY RUN] Summary: Would clean 3 jobs, freeing ~15.3 GB\n</code></pre></p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#custom-retention-periods","level":4,"title":"Custom Retention Periods","text":"<p>Override default retention settings:</p> <pre><code># Keep successful jobs for 14 days instead of 7\npython cleanup_jobs.py --success-retention-days 14\n\n# Remove failed jobs after 24 hours instead of 12\npython cleanup_jobs.py --failed-retention-hours 24\n\n# Combine both\npython cleanup_jobs.py --success-retention-days 14 --failed-retention-hours 24\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#advanced-options","level":3,"title":"Advanced Options","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#verbose-output","level":4,"title":"Verbose Output","text":"<p>Get detailed information about each operation:</p> <pre><code>python cleanup_jobs.py --verbose\n</code></pre> <p>Verbose output includes: - Individual file deletions - Directory sizes - Database operations - Timing information</p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#custom-paths","level":4,"title":"Custom Paths","text":"<p>If your installation uses non-standard paths:</p> <pre><code>python cleanup_jobs.py \\\n    --base-dir /custom/eemt/web-interface \\\n    --verbose\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#combining-options","level":4,"title":"Combining Options","text":"<p>Use multiple options together:</p> <pre><code>python cleanup_jobs.py \\\n    --dry-run \\\n    --verbose \\\n    --success-retention-days 3 \\\n    --failed-retention-hours 6\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#understanding-output","level":3,"title":"Understanding Output","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#cleanup-summary","level":4,"title":"Cleanup Summary","text":"<p>After each run, you'll see a summary:</p> <pre><code>=== CLEANUP SUMMARY ===\nSuccessful jobs processed: 5      # Jobs older than 7 days with data removed\nFailed jobs processed: 2          # Failed jobs older than 12 hours fully removed\nTotal disk space freed: 25678.9 MB   # Actual disk space recovered\nJob configs preserved: 5          # Job records kept in database\nJob configs deleted: 2            # Failed job records removed\nErrors encountered: 0             # Any errors during cleanup\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#log-files","level":4,"title":"Log Files","text":"<p>Cleanup operations are logged to: - Console output (when run manually) - <code>cleanup_jobs.log</code> (in the script directory) - System logs (when run via cron/systemd)</p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#json-summary-files","level":4,"title":"JSON Summary Files","text":"<p>Each cleanup creates a detailed JSON summary: <pre><code># View latest cleanup summary\nls -lt cleanup_summary_*.json | head -1\n\n# Pretty-print the summary\npython -m json.tool cleanup_summary_20240120_143017.json\n</code></pre></p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#setting-up-automated-cleanup","level":2,"title":"Setting Up Automated Cleanup","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#using-the-setup-script","level":3,"title":"Using the Setup Script","text":"<p>The <code>setup_cleanup_cron.sh</code> script automates the scheduling configuration:</p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#install-with-cron-recommended","level":4,"title":"Install with Cron (Recommended)","text":"<pre><code># User-level installation (recommended)\n./setup_cleanup_cron.sh --user --method cron\n\n# System-wide installation (requires sudo)\nsudo ./setup_cleanup_cron.sh --system --method cron\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#install-with-systemd","level":4,"title":"Install with Systemd","text":"<p>For systems using systemd:</p> <pre><code># User-level systemd timer\n./setup_cleanup_cron.sh --user --method systemd\n\n# System-wide systemd timer (requires sudo)\nsudo ./setup_cleanup_cron.sh --system --method systemd\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#verify-installation","level":4,"title":"Verify Installation","text":"<p>After installation, verify the schedule:</p> <pre><code># For cron\ncrontab -l | grep cleanup_jobs\n\n# For systemd\nsystemctl --user status eemt-cleanup.timer\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#manual-cron-configuration","level":3,"title":"Manual Cron Configuration","text":"<p>If you prefer manual configuration:</p> <ol> <li> <p>Open crontab editor:    <pre><code>crontab -e\n</code></pre></p> </li> <li> <p>Add cleanup schedule (choose one):    <pre><code># Daily at 2:00 AM\n0 2 * * * cd /path/to/web-interface &amp;&amp; python3 cleanup_jobs.py &gt;&gt; cleanup_jobs.log 2&gt;&amp;1\n\n# Every 6 hours\n0 */6 * * * cd /path/to/web-interface &amp;&amp; python3 cleanup_jobs.py\n\n# Weekly on Sunday at 3:00 AM\n0 3 * * 0 cd /path/to/web-interface &amp;&amp; python3 cleanup_jobs.py\n\n# Twice daily at 2:00 AM and 2:00 PM\n0 2,14 * * * cd /path/to/web-interface &amp;&amp; python3 cleanup_jobs.py\n</code></pre></p> </li> <li> <p>Save and exit (usually Ctrl+X in nano)</p> </li> </ol>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#manual-systemd-configuration","level":3,"title":"Manual Systemd Configuration","text":"<p>For systemd timer setup:</p> <ol> <li> <p>Create service file (<code>~/.config/systemd/user/eemt-cleanup.service</code>):    <pre><code>[Unit]\nDescription=EEMT Job Data Cleanup Service\nAfter=network.target\n\n[Service]\nType=oneshot\nWorkingDirectory=/path/to/web-interface\nExecStart=/usr/bin/python3 /path/to/web-interface/cleanup_jobs.py\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=default.target\n</code></pre></p> </li> <li> <p>Create timer file (<code>~/.config/systemd/user/eemt-cleanup.timer</code>):    <pre><code>[Unit]\nDescription=Daily EEMT Job Cleanup Timer\n\n[Timer]\nOnCalendar=daily\nOnCalendar=*-*-* 02:00:00\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre></p> </li> <li> <p>Enable and start timer:    <pre><code>systemctl --user daemon-reload\nsystemctl --user enable eemt-cleanup.timer\nsystemctl --user start eemt-cleanup.timer\n</code></pre></p> </li> </ol>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#configuring-retention-periods","level":2,"title":"Configuring Retention Periods","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#environment-variables","level":3,"title":"Environment Variables","text":"<p>Set system-wide retention periods:</p> <pre><code># Add to ~/.bashrc or /etc/environment\nexport EEMT_SUCCESS_RETENTION_DAYS=14    # Keep successful jobs for 2 weeks\nexport EEMT_FAILED_RETENTION_HOURS=24    # Keep failed jobs for 1 day\n\n# Apply changes\nsource ~/.bashrc\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#per-execution-override","level":3,"title":"Per-Execution Override","text":"<p>Override settings for a single run:</p> <pre><code># Quick cleanup - 3 day retention\npython cleanup_jobs.py --success-retention-days 3\n\n# Conservative cleanup - 30 day retention\npython cleanup_jobs.py --success-retention-days 30\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#configuration-file","level":3,"title":"Configuration File","text":"<p>For complex setups, create <code>cleanup_config.yaml</code>:</p> <pre><code># Retention settings\nretention:\n  successful_jobs:\n    days: 7\n    keep_logs: true\n  failed_jobs:\n    hours: 12\n    keep_error_logs: true\n\n# Performance settings\nperformance:\n  batch_size: 50\n  verbose: true\n\n# Notification settings (optional)\nnotifications:\n  email_on_completion: admin@example.com\n  slack_webhook: https://hooks.slack.com/services/XXX\n</code></pre> <p>Load configuration: <pre><code># In a custom script\nimport yaml\n\nwith open('cleanup_config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nretention_days = config['retention']['successful_jobs']['days']\n</code></pre></p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#monitoring-cleanup-operations","level":2,"title":"Monitoring Cleanup Operations","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#viewing-logs","level":3,"title":"Viewing Logs","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#real-time-monitoring","level":4,"title":"Real-time Monitoring","text":"<p>Watch cleanup progress in real-time:</p> <pre><code># Follow log file\ntail -f cleanup_jobs.log\n\n# Watch with highlighting\ntail -f cleanup_jobs.log | grep --color=auto -E 'ERROR|WARNING|$'\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#historical-logs","level":4,"title":"Historical Logs","text":"<p>Review past cleanup operations:</p> <pre><code># View last 50 lines\ntail -n 50 cleanup_jobs.log\n\n# Search for specific job\ngrep \"job-20240115-123456\" cleanup_jobs.log\n\n# Count freed space over time\ngrep \"Total disk space freed\" cleanup_jobs.log | awk '{sum+=$7} END {print sum/1024 \" GB total\"}'\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#systemd-journal","level":3,"title":"Systemd Journal","text":"<p>If using systemd timers:</p> <pre><code># View recent cleanup runs\njournalctl --user -u eemt-cleanup.service -n 50\n\n# Follow live\njournalctl --user -u eemt-cleanup.service -f\n\n# View since yesterday\njournalctl --user -u eemt-cleanup.service --since yesterday\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#cleanup-metrics","level":3,"title":"Cleanup Metrics","text":"<p>Monitor cleanup effectiveness:</p> <pre><code># Check disk usage trend\ndf -h /path/to/results | grep -v Filesystem\n\n# Count jobs by status\nsqlite3 jobs.db \"SELECT status, COUNT(*) FROM jobs GROUP BY status;\"\n\n# View jobs pending cleanup\nsqlite3 jobs.db \"\nSELECT id, status, datetime(completed_at) as completed, \n       CAST((julianday('now') - julianday(completed_at)) AS INTEGER) as days_old\nFROM jobs \nWHERE status IN ('completed', 'failed') \n  AND completed_at IS NOT NULL\n  AND data_cleaned_at IS NULL\nORDER BY completed_at;\"\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#common-issues-and-solutions","level":3,"title":"Common Issues and Solutions","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#issue-cleanup-not-running-automatically","level":4,"title":"Issue: Cleanup Not Running Automatically","text":"<p>Check cron job: <pre><code># List current cron jobs\ncrontab -l\n\n# Check cron service\nsystemctl status cron  # or 'crond' on some systems\n\n# View cron logs\ngrep CRON /var/log/syslog | tail -20\n</code></pre></p> <p>Check systemd timer: <pre><code># Timer status\nsystemctl --user status eemt-cleanup.timer\n\n# List timers\nsystemctl --user list-timers\n\n# Check next run time\nsystemctl --user list-timers eemt-cleanup.timer\n</code></pre></p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#issue-permission-denied-errors","level":4,"title":"Issue: Permission Denied Errors","text":"<pre><code># Check ownership\nls -la uploads/ results/ jobs.db\n\n# Fix ownership (replace 'username' with your user)\nsudo chown -R username:username uploads/ results/ jobs.db\n\n# Fix permissions\nchmod 755 uploads/ results/\nchmod 644 jobs.db\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#issue-database-locked","level":4,"title":"Issue: Database Locked","text":"<pre><code># Find process using database\nfuser jobs.db\n\n# Or using lsof\nlsof jobs.db\n\n# Wait and retry, or if safe, kill the process\nkill &lt;PID&gt;\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#issue-disk-space-not-freed","level":4,"title":"Issue: Disk Space Not Freed","text":"<pre><code># Check actual disk usage\ndu -sh results/* | sort -h\n\n# Check for deleted but open files\nlsof +L1\n\n# Force filesystem to release space\nsync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches  # Requires root\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#issue-cleanup-takes-too-long","level":4,"title":"Issue: Cleanup Takes Too Long","text":"<p>Optimize performance:</p> <pre><code># Run with smaller batch size\npython cleanup_jobs.py --batch-size 10\n\n# Skip large directories temporarily\npython cleanup_jobs.py --skip-large\n\n# Increase logging to identify bottleneck\npython cleanup_jobs.py --verbose --dry-run\n</code></pre>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#getting-help","level":3,"title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li> <p>Check logs for error messages:    <pre><code>tail -100 cleanup_jobs.log | grep -i error\n</code></pre></p> </li> <li> <p>Run in debug mode:    <pre><code>python cleanup_jobs.py --verbose --dry-run\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>python -c \"import cleanup_jobs; print('Script OK')\"\n</code></pre></p> </li> <li> <p>Test database connection:    <pre><code>sqlite3 jobs.db \".tables\"\n</code></pre></p> </li> </ol>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#best-practices","level":2,"title":"Best Practices","text":"","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#regular-maintenance","level":3,"title":"Regular Maintenance","text":"<ol> <li> <p>Weekly Review: Check cleanup summaries weekly    <pre><code># Review weekly summaries\nls -lt cleanup_summary_*.json | head -7\n</code></pre></p> </li> <li> <p>Monthly Analysis: Analyze trends monthly    <pre><code># Monthly space freed\ngrep \"Total disk space freed\" cleanup_jobs.log | \\\n  awk '{print substr($1,1,7), $7}' | \\\n  awk '{a[$1]+=$2} END {for(i in a) print i, a[i]/1024 \" GB\"}'\n</code></pre></p> </li> <li> <p>Quarterly Tuning: Adjust retention based on usage    <pre><code># Average job age at cleanup\nsqlite3 jobs.db \"\nSELECT AVG(julianday(data_cleaned_at) - julianday(completed_at)) as avg_days\nFROM jobs WHERE data_cleaned_at IS NOT NULL;\"\n</code></pre></p> </li> </ol>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#safety-measures","level":3,"title":"Safety Measures","text":"<ol> <li> <p>Always test with dry-run before changing retention:    <pre><code>python cleanup_jobs.py --dry-run --success-retention-days 3\n</code></pre></p> </li> <li> <p>Backup important results before they expire:    <pre><code># Archive jobs older than 5 days\ntar -czf backup_$(date +%Y%m%d).tar.gz \\\n  $(find results -maxdepth 1 -type d -mtime +5)\n</code></pre></p> </li> <li> <p>Monitor disk usage proactively:    <pre><code># Set up disk usage alert\nUSAGE=$(df /path/to/results | awk 'NR==2 {print $5}' | sed 's/%//')\nif [ $USAGE -gt 80 ]; then\n  echo \"Warning: Disk usage at ${USAGE}%\" | mail -s \"EEMT Disk Alert\" admin@example.com\nfi\n</code></pre></p> </li> </ol>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#performance-tips","level":3,"title":"Performance Tips","text":"<ol> <li> <p>Schedule during low-usage periods: Run cleanup when system is idle</p> </li> <li> <p>Adjust batch size for your system:    <pre><code># Smaller batches for limited resources\nexport EEMT_CLEANUP_BATCH_SIZE=25\n\n# Larger batches for powerful systems\nexport EEMT_CLEANUP_BATCH_SIZE=200\n</code></pre></p> </li> <li> <p>Use nice for background cleanup:    <pre><code>nice -n 10 python cleanup_jobs.py\n</code></pre></p> </li> <li> <p>Implement staged cleanup for very large deployments:    <pre><code># Clean failed jobs first (usually smaller)\npython cleanup_jobs.py --success-retention-days 999 --failed-retention-hours 12\n\n# Then clean successful jobs\npython cleanup_jobs.py --success-retention-days 7 --failed-retention-hours 999\n</code></pre></p> </li> </ol>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/cleanup-scripts/#summary","level":2,"title":"Summary","text":"<p>The EEMT cleanup scripts provide flexible, automated management of job data to maintain optimal system performance. Key points:</p> <ul> <li>Simple setup: One command to enable automated cleanup</li> <li>Configurable retention: Adjust to your needs</li> <li>Safe operation: Dry-run mode and job config preservation</li> <li>Multiple scheduling options: Cron, systemd, or manual</li> <li>Comprehensive logging: Full visibility into cleanup operations</li> </ul> <p>Regular use of these cleanup scripts ensures your EEMT deployment remains efficient and responsive even with high job volumes.</p>","path":["Getting Started","Cleanup Scripts User Guide"],"tags":[]},{"location":"getting-started/docker-deployment/","level":1,"title":"Docker Deployment Guide","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#overview","level":2,"title":"Overview","text":"<p>This guide provides comprehensive instructions for deploying EEMT using Docker containers. The containerized deployment offers the most reliable and reproducible way to run EEMT workflows, eliminating complex dependency management and ensuring consistent execution across different systems.</p>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#prerequisites","level":2,"title":"Prerequisites","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#system-requirements","level":3,"title":"System Requirements","text":"<p>Minimum Hardware: - CPU: 4 cores (8+ recommended) - RAM: 8 GB (16+ GB recommended) - Storage: 50 GB free space - Network: Stable internet connection for climate data downloads</p> <p>Software Requirements: - Docker Engine 20.10+ or Docker Desktop - Docker Compose v2.0+ - Git (for cloning repository) - Modern web browser (Chrome, Firefox, Safari, Edge)</p>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#docker-installation","level":3,"title":"Docker Installation","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#linux-ubuntudebian","level":4,"title":"Linux (Ubuntu/Debian)","text":"<pre><code># Update package index\nsudo apt update\n\n# Install prerequisites\nsudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n\n# Add Docker GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add Docker repository\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker\nsudo apt update\nsudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n\n# Add user to docker group (log out and back in after)\nsudo usermod -aG docker $USER\n\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#macos","level":4,"title":"macOS","text":"<pre><code># Install Docker Desktop\n# Download from: https://www.docker.com/products/docker-desktop\n\n# Or use Homebrew\nbrew install --cask docker\n\n# Start Docker Desktop from Applications\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#windows","level":4,"title":"Windows","text":"<pre><code># Install Docker Desktop\n# Download from: https://www.docker.com/products/docker-desktop\n\n# Enable WSL2 backend (recommended)\nwsl --install\n\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#quick-start","level":2,"title":"Quick Start","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#1-clone-repository","level":3,"title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#2-build-containers","level":3,"title":"2. Build Containers","text":"<pre><code># Build all required containers\ndocker compose build\n\n# Or build specific service\ndocker compose build eemt-web\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#3-start-services","level":3,"title":"3. Start Services","text":"<pre><code># Start in foreground (see logs)\ndocker compose up\n\n# Start in background\ndocker compose up -d\n\n# Access web interface\nopen http://localhost:5000  # macOS\nxdg-open http://localhost:5000  # Linux\nstart http://localhost:5000  # Windows\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#4-submit-a-job","level":3,"title":"4. Submit a Job","text":"<ol> <li>Navigate to http://localhost:5000</li> <li>Select workflow type (Solar or EEMT)</li> <li>Upload your DEM file</li> <li>Configure parameters</li> <li>Click \"Submit Job\"</li> <li>Monitor progress at http://localhost:5000/monitor</li> </ol>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#deployment-modes","level":2,"title":"Deployment Modes","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#local-mode-default","level":3,"title":"Local Mode (Default)","text":"<p>Single-container deployment for development and small-scale processing:</p> <pre><code># docker-compose.yml (simplified)\nservices:\n  eemt-web:\n    build:\n      context: .\n      dockerfile: docker/web-interface/Dockerfile\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./data:/app/data\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - EEMT_MODE=local\n</code></pre> <p>Start Command: <pre><code>docker compose up eemt-web\n</code></pre></p> <p>Use Cases: - Development and testing - Small to medium DEM processing - Single-user environments - Educational purposes</p>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#distributed-mode","level":3,"title":"Distributed Mode","text":"<p>Multi-container deployment with master-worker architecture:</p> <pre><code># docker-compose.yml (distributed profile)\nservices:\n  eemt-master:\n    profiles: [distributed]\n    ports:\n      - \"5000:5000\"  # Web interface\n      - \"9123:9123\"  # Work Queue port\n    environment:\n      - EEMT_MODE=master\n      - MAX_WORKERS=10\n\n  eemt-worker:\n    profiles: [distributed]\n    environment:\n      - MASTER_HOST=eemt-master\n      - WORKER_CORES=4\n    deploy:\n      replicas: 5\n</code></pre> <p>Start Command: <pre><code># Start distributed cluster\ndocker compose --profile distributed up\n\n# Scale workers dynamically\ndocker compose --profile distributed up --scale eemt-worker=10\n</code></pre></p> <p>Use Cases: - Large-scale processing - Multi-user environments - Production deployments - HPC integration</p>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#documentation-mode","level":3,"title":"Documentation Mode","text":"<p>Serve documentation alongside the application:</p> <pre><code># Start with documentation\ndocker compose --profile docs up\n\n# Access documentation at http://localhost:8000\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#configuration","level":2,"title":"Configuration","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#environment-variables","level":3,"title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Application Configuration\nEEMT_HOST=0.0.0.0\nEEMT_PORT=5000\nEEMT_MODE=local\n\n# Resource Limits\nCONTAINER_CPU_LIMIT=4\nCONTAINER_MEMORY_LIMIT=8G\nCONTAINER_DISK_LIMIT=50G\n\n# Directory Configuration\nEEMT_UPLOAD_DIR=./data/uploads\nEEMT_RESULTS_DIR=./data/results\nEEMT_TEMP_DIR=./data/temp\nEEMT_CACHE_DIR=./data/cache\n\n# Distributed Mode (optional)\nWORK_QUEUE_PORT=9123\nWORK_QUEUE_PROJECT=EEMT-Production\nMAX_WORKERS=20\n\n# Worker Configuration (optional)\nMASTER_HOST=eemt-master\nMASTER_PORT=9123\nWORKER_CORES=8\nWORKER_MEMORY=16G\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#docker-compose-override","level":3,"title":"Docker Compose Override","text":"<p>Create <code>docker-compose.override.yml</code> for local customizations:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-web:\n    environment:\n      - DEBUG=true\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./custom-data:/app/custom-data\n    ports:\n      - \"8080:5000\"  # Use different port\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#volume-configuration","level":3,"title":"Volume Configuration","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#persistent-data-volumes","level":4,"title":"Persistent Data Volumes","text":"<pre><code>volumes:\n  # Named volumes for persistence\n  eemt-uploads:\n    driver: local\n  eemt-results:\n    driver: local\n  eemt-cache:\n    driver: local\n\nservices:\n  eemt-web:\n    volumes:\n      - eemt-uploads:/app/uploads\n      - eemt-results:/app/results\n      - eemt-cache:/app/cache\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#bind-mounts-for-development","level":4,"title":"Bind Mounts for Development","text":"<pre><code>services:\n  eemt-web:\n    volumes:\n      # Mount source code for live updates\n      - ./web-interface:/app/web-interface\n      - ./sol:/opt/eemt/sol\n      - ./eemt:/opt/eemt/eemt\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#advanced-configuration","level":2,"title":"Advanced Configuration","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#resource-management","level":3,"title":"Resource Management","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#cpu-and-memory-limits","level":4,"title":"CPU and Memory Limits","text":"<pre><code>services:\n  eemt-worker:\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 8G\n        reservations:\n          cpus: '2.0'\n          memory: 4G\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#gpu-support-future","level":4,"title":"GPU Support (Future)","text":"<pre><code>services:\n  eemt-gpu-worker:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#network-configuration","level":3,"title":"Network Configuration","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#custom-network","level":4,"title":"Custom Network","text":"<pre><code>networks:\n  eemt-network:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.20.0.0/16\n          gateway: 172.20.0.1\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#external-network-access","level":4,"title":"External Network Access","text":"<pre><code>services:\n  eemt-web:\n    networks:\n      - eemt-network\n      - external-network\n\nnetworks:\n  external-network:\n    external: true\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#security-configuration","level":3,"title":"Security Configuration","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#read-only-root-filesystem","level":4,"title":"Read-Only Root Filesystem","text":"<pre><code>services:\n  eemt-worker:\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /run\n    volumes:\n      - ./data:/data:ro\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#secrets-management","level":4,"title":"Secrets Management","text":"<pre><code>secrets:\n  db_password:\n    file: ./secrets/db_password.txt\n\nservices:\n  eemt-web:\n    secrets:\n      - db_password\n    environment:\n      - DB_PASSWORD_FILE=/run/secrets/db_password\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#monitoring-and-logging","level":2,"title":"Monitoring and Logging","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#view-logs","level":3,"title":"View Logs","text":"<pre><code># All services\ndocker compose logs\n\n# Specific service\ndocker compose logs eemt-web\n\n# Follow logs in real-time\ndocker compose logs -f\n\n# Last 100 lines\ndocker compose logs --tail=100\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#container-statistics","level":3,"title":"Container Statistics","text":"<pre><code># Resource usage\ndocker stats\n\n# Specific containers\ndocker stats eemt-web eemt-worker\n\n# Format output\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#health-checks","level":3,"title":"Health Checks","text":"<pre><code>services:\n  eemt-web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#backup-and-recovery","level":2,"title":"Backup and Recovery","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#backup-data-volumes","level":3,"title":"Backup Data Volumes","text":"<pre><code># Stop containers\ndocker compose down\n\n# Backup uploads directory\ndocker run --rm -v eemt_data-uploads:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar czf /backup/uploads-backup.tar.gz /data\n\n# Backup results directory  \ndocker run --rm -v eemt_data-results:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar czf /backup/results-backup.tar.gz /data\n\n# Backup database\ndocker run --rm -v eemt_data:/data -v $(pwd):/backup \\\n  ubuntu:24.04 cp /data/jobs.db /backup/jobs-backup.db\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#restore-data-volumes","level":3,"title":"Restore Data Volumes","text":"<pre><code># Restore uploads\ndocker run --rm -v eemt_data-uploads:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar xzf /backup/uploads-backup.tar.gz -C /\n\n# Restore results\ndocker run --rm -v eemt_data-results:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar xzf /backup/results-backup.tar.gz -C /\n\n# Restore database\ndocker run --rm -v eemt_data:/data -v $(pwd):/backup \\\n  ubuntu:24.04 cp /backup/jobs-backup.db /data/jobs.db\n\n# Restart services\ndocker compose up -d\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#common-issues","level":3,"title":"Common Issues","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#port-already-in-use","level":4,"title":"Port Already in Use","text":"<pre><code># Check what's using port 5000\nlsof -i :5000  # macOS/Linux\nnetstat -ano | findstr :5000  # Windows\n\n# Use different port\ndocker compose up -e EEMT_PORT=8080\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#docker-daemon-not-running","level":4,"title":"Docker Daemon Not Running","text":"<pre><code># Linux\nsudo systemctl start docker\n\n# macOS/Windows\n# Start Docker Desktop application\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#container-wont-start","level":4,"title":"Container Won't Start","text":"<pre><code># Check logs\ndocker compose logs eemt-web\n\n# Inspect container\ndocker inspect eemt-web\n\n# Debug interactively\ndocker compose run --entrypoint /bin/bash eemt-web\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#permission-denied-errors","level":4,"title":"Permission Denied Errors","text":"<pre><code># Fix Docker socket permissions (Linux)\nsudo chmod 666 /var/run/docker.sock\n\n# Fix volume permissions\ndocker compose exec eemt-web chown -R eemt:eemt /app/data\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#out-of-disk-space","level":4,"title":"Out of Disk Space","text":"<pre><code># Check disk usage\ndf -h\n\n# Clean up Docker\ndocker system prune -a\ndocker volume prune\ndocker image prune\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#debugging","level":3,"title":"Debugging","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#interactive-shell-access","level":4,"title":"Interactive Shell Access","text":"<pre><code># Access running container\ndocker compose exec eemt-web /bin/bash\n\n# Start new container with shell\ndocker compose run --rm eemt-web /bin/bash\n\n# Override entrypoint\ndocker compose run --entrypoint /bin/bash eemt-web\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#network-debugging","level":4,"title":"Network Debugging","text":"<pre><code># Test connectivity between containers\ndocker compose exec eemt-worker ping eemt-master\n\n# Inspect network\ndocker network inspect eemt_eemt-network\n\n# Use network debugging container\ndocker run --rm -it --network eemt_eemt-network nicolaka/netshoot\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#build-optimization","level":3,"title":"Build Optimization","text":"<pre><code># Use BuildKit for faster builds\nDOCKER_BUILDKIT=1 docker compose build\n\n# Parallel builds\ndocker compose build --parallel\n\n# Use cache\ndocker compose build --no-cache=false\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#runtime-optimization","level":3,"title":"Runtime Optimization","text":"<pre><code>services:\n  eemt-web:\n    # Enable shared memory\n    shm_size: '2gb'\n\n    # Optimize logging\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#storage-optimization","level":3,"title":"Storage Optimization","text":"<pre><code># Use tmpfs for temporary data\ndocker compose run --tmpfs /tmp:size=2G eemt-worker\n\n# Enable compression\ndocker save eemt:ubuntu24.04 | gzip &gt; eemt.tar.gz\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#production-deployment","level":2,"title":"Production Deployment","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#ssltls-configuration","level":3,"title":"SSL/TLS Configuration","text":"<pre><code>services:\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./certs:/etc/nginx/certs\n    depends_on:\n      - eemt-web\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#docker-swarm-deployment","level":3,"title":"Docker Swarm Deployment","text":"<pre><code># Initialize swarm\ndocker swarm init\n\n# Deploy stack\ndocker stack deploy -c docker-compose.yml eemt\n\n# Scale service\ndocker service scale eemt_eemt-worker=10\n\n# Monitor services\ndocker service ls\ndocker service ps eemt_eemt-web\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#kubernetes-deployment","level":3,"title":"Kubernetes Deployment","text":"<pre><code># Convert docker-compose to Kubernetes\nkompose convert\n\n# Deploy to Kubernetes\nkubectl apply -f eemt-deployment.yaml\nkubectl apply -f eemt-service.yaml\n\n# Check deployment\nkubectl get pods\nkubectl get services\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#maintenance","level":2,"title":"Maintenance","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#update-containers","level":3,"title":"Update Containers","text":"<pre><code># Pull latest images\ndocker compose pull\n\n# Rebuild containers\ndocker compose build --pull\n\n# Restart with new images\ndocker compose up -d\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#clean-up","level":3,"title":"Clean Up","text":"<pre><code># Stop and remove containers\ndocker compose down\n\n# Remove volumes (WARNING: deletes data)\ndocker compose down -v\n\n# Remove everything including images\ndocker compose down --rmi all -v\n\n# System-wide cleanup\ndocker system prune -a --volumes\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#log-rotation","level":3,"title":"Log Rotation","text":"<pre><code>services:\n  eemt-web:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"100m\"\n        max-file: \"10\"\n        compress: \"true\"\n</code></pre>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#best-practices","level":2,"title":"Best Practices","text":"","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#development","level":3,"title":"Development","text":"<ol> <li>Use <code>.env</code> files for configuration</li> <li>Mount source code as volumes for hot-reloading</li> <li>Use override files for local settings</li> <li>Keep images small with multi-stage builds</li> </ol>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#production","level":3,"title":"Production","text":"<ol> <li>Use specific image tags (not <code>latest</code>)</li> <li>Implement health checks</li> <li>Set resource limits</li> <li>Use secrets for sensitive data</li> <li>Enable log rotation</li> <li>Regular backups</li> <li>Monitor resource usage</li> </ol>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#security","level":3,"title":"Security","text":"<ol> <li>Run containers as non-root user</li> <li>Use read-only filesystems where possible</li> <li>Limit network exposure</li> <li>Scan images for vulnerabilities</li> <li>Keep base images updated</li> </ol>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"getting-started/docker-deployment/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Web Interface Guide - Learn to use the web interface</li> <li>API Reference - Integrate with the REST API</li> <li>Distributed Deployment - Scale across multiple nodes</li> <li>Container Architecture - Understand the container design</li> </ul>","path":["Getting Started","Docker Deployment Guide"],"tags":[]},{"location":"grass-gis/","level":1,"title":"GRASS GIS for EEMT Calculations","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#overview","level":2,"title":"Overview","text":"<p>GRASS GIS provides the core geospatial analysis capabilities for EEMT calculations, particularly the r.sun module for solar radiation modeling. This guide covers installation, configuration, and parallel processing techniques.</p>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#contents","level":2,"title":"Contents","text":"<ol> <li>Installation and Setup</li> <li>r.sun Solar Radiation Modeling</li> <li>Parallel Processing with r.sun (nprocs)</li> <li>Terrain Analysis</li> <li>Batch Processing Workflows</li> <li>Performance Optimization</li> </ol>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#installation-and-setup","level":2,"title":"Installation and Setup","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#grass-gis-installation","level":3,"title":"GRASS GIS Installation","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#ubuntudebian","level":4,"title":"Ubuntu/Debian","text":"<pre><code># Install GRASS GIS 8.x\nsudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt update\nsudo apt install grass grass-dev grass-doc\n\n# Verify installation\ngrass --version\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#macos","level":4,"title":"macOS","text":"<pre><code># Via Homebrew\nbrew install grass\n\n# Or download from: https://grass.osgeo.org/download/mac/\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#windows","level":4,"title":"Windows","text":"<pre><code># Download OSGeo4W installer\n# https://trac.osgeo.org/osgeo4w/\n\n# Or use conda\nconda install -c conda-forge grass\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#creating-a-grass-location","level":3,"title":"Creating a GRASS Location","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#from-dem-file","level":4,"title":"From DEM File","text":"<pre><code># Create location from DEM projection\ngrass -c /path/to/your_dem.tif ~/grassdata/eemt_project/PERMANENT\n\n# Alternative: create location interactively  \ngrass ~/grassdata/eemt_project/PERMANENT\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#verify-setup","level":4,"title":"Verify Setup","text":"<pre><code># Check projection information\ng.proj -p\n\n# Set computational region to match DEM\ng.region raster=your_dem -p\n\n# Display basic info\nr.info your_dem\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#rsun-solar-radiation-modeling","level":2,"title":"r.sun Solar Radiation Modeling","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#basic-rsun-usage","level":3,"title":"Basic r.sun Usage","text":"<pre><code># Import DEM\nr.in.gdal input=dem.tif output=elevation\n\n# Calculate slope and aspect  \nr.slope.aspect elevation=elevation slope=slope_deg aspect=aspect_deg\n\n# Basic solar radiation calculation\nr.sun elevation=elevation aspect=aspect_deg slope=slope_deg \\\n      day=180 glob_rad=solar_june29 insol_time=hours_june29\n\n# View results\nd.rast solar_june29\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#advanced-rsun-parameters","level":3,"title":"Advanced r.sun Parameters","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#temporal-settings","level":4,"title":"Temporal Settings","text":"<pre><code># Single day calculation\nr.sun elevation=dem day=180 step=0.25 \\\n      glob_rad=global_rad insol_time=sunshine_hours\n\n# Multi-day calculation\nr.sun elevation=dem start_day=170 end_day=190 day_step=1 \\\n      step=0.25 glob_rad=summer_radiation\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#atmospheric-parameters","level":4,"title":"Atmospheric Parameters","text":"<pre><code># Atmospheric conditions\nr.sun elevation=dem day=180 \\\n      linke_value=3.0 \\        # Atmospheric turbidity (1.0-8.0)\n      albedo_value=0.2 \\       # Surface albedo (0.0-1.0)  \n      slope_value=0.1 \\        # Solar constant correction\n      aspect_value=180.0 \\     # Default south-facing slope\n      glob_rad=solar_output insol_time=sun_hours\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#horizon-and-shading","level":4,"title":"Horizon and Shading","text":"<pre><code># Include horizon effects  \nr.sun elevation=dem day=180 \\\n      horizonstep=30 \\         # Horizon calculation step (degrees)\n      horizon=horizon_angles \\  # Output horizon file\n      glob_rad=solar_with_horizon\n\n# Cast shadows from features\nr.sun elevation=dem day=180 \\\n      cast_shadow=shadow_map \\  # Shadow raster output\n      glob_rad=solar_with_shadows\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#parallel-processing-with-rsun-nprocs","level":2,"title":"Parallel Processing with r.sun (nprocs)","text":"<p>r.sun merged into core r.sun</p> <p>As of GRASS GIS 7.4 (2018), the r.sun addon was merged into the core <code>r.sun</code> module. Use the <code>nprocs</code> parameter for multi-threaded processing instead of the deprecated <code>threads</code> parameter.</p>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#openmp-multi-core-processing","level":3,"title":"OpenMP Multi-core Processing","text":"<pre><code># Set number of threads\nexport OMP_NUM_THREADS=8\n\n# Run r.sun with multiple threads using nprocs parameter\nr.sun elevation=dem aspect=aspect_deg slope=slope_deg \\\n      day=180 step=0.25 nprocs=8 \\\n      glob_rad=solar_parallel insol_time=hours_parallel\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#python-wrapper-for-parallel-processing","level":3,"title":"Python Wrapper for Parallel Processing","text":"<p>Based on the analysis of <code>/sol/rsun.sh</code>, here's the enhanced parallel processing approach:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nEnhanced r.sun parallel processing for EEMT calculations\nBased on sol/run-workflow with modern improvements\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport subprocess\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nclass GrassSolarCalculator:\n    \"\"\"GRASS GIS solar radiation calculator with parallel processing\"\"\"\n\n    def __init__(self, dem_path, output_dir, num_nprocs=None):\n        self.dem_path = Path(dem_path)\n        self.output_dir = Path(output_dir)\n        self.num_threads = num_threads or mp.cpu_count()\n\n        # Create output directories\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        (self.output_dir / 'global' / 'daily').mkdir(parents=True, exist_ok=True)\n        (self.output_dir / 'insol' / 'daily').mkdir(parents=True, exist_ok=True)\n\n    def setup_grass_environment(self, day):\n        \"\"\"Create temporary GRASS environment for single day calculation\"\"\"\n\n        # Create temporary directory\n        temp_dir = tempfile.mkdtemp(prefix=f'grass_day_{day}_')\n        location_dir = Path(temp_dir) / 'grassdata' / f'solar_day_{day}' / 'PERMANENT'\n        location_dir.mkdir(parents=True, exist_ok=True)\n\n        # GRASS environment variables\n        grass_env = os.environ.copy()\n        grass_env.update({\n            'GISDBASE': str(location_dir.parent.parent),\n            'LOCATION_NAME': f'solar_day_{day}',\n            'MAPSET': 'PERMANENT',\n            'GRASS_GUI': 'text',\n            'GRASS_VERBOSE': '0'\n        })\n\n        return temp_dir, grass_env\n\n    def calculate_daily_solar(self, day, step=0.25, linke_value=3.0, albedo_value=0.2):\n        \"\"\"Calculate solar radiation for a single day\"\"\"\n\n        temp_dir, grass_env = self.setup_grass_environment(day)\n\n        try:\n            # Start GRASS session\n            grass_cmd = [\n                'grass', '--text',\n                f\"{grass_env['GISDBASE']}/{grass_env['LOCATION_NAME']}/{grass_env['MAPSET']}\"\n            ]\n\n            # GRASS commands\n            commands = f\"\"\"\n# Create location from DEM\ng.proj -c georef={self.dem_path}\n\n# Import DEM\nr.in.gdal input={self.dem_path} output=dem\n\n# Set region\ng.region raster=dem\n\n# Calculate slope and aspect  \nr.slope.aspect elevation=dem slope=slope_deg aspect=aspect_deg\n\n# Run r.sun with optimal threading\nr.sun elevation=dem aspect=aspect_deg slope=slope_deg \\\\\n         day={day} step={step} \\\\\n         linke_value={linke_value} albedo_value={albedo_value} \\\\\n         nprocs={min(self.num_threads, 4)} \\\\\n         glob_rad=solar_global insol_time=solar_hours\n\n# Export results\nr.out.gdal input=solar_global output={self.output_dir}/global/daily/total_sun_day_{day}.tif \\\\\n           createopt=\"COMPRESS=LZW,TILED=YES\"\n\nr.out.gdal input=solar_hours output={self.output_dir}/insol/daily/hours_sun_day_{day}.tif \\\\\n           createopt=\"COMPRESS=LZW,TILED=YES\"\n\"\"\"\n\n            # Execute GRASS commands\n            process = subprocess.Popen(\n                grass_cmd,\n                stdin=subprocess.PIPE,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                env=grass_env\n            )\n\n            stdout, stderr = process.communicate(input=commands)\n\n            if process.returncode != 0:\n                raise RuntimeError(f\"GRASS error for day {day}: {stderr}\")\n\n            print(f\"Completed day {day}\")\n            return day\n\n        finally:\n            # Cleanup temporary directory\n            shutil.rmtree(temp_dir, ignore_errors=True)\n\n    def calculate_annual_solar(self, year_days=None, step=0.25, linke_value=3.0, albedo_value=0.2):\n        \"\"\"Calculate solar radiation for multiple days in parallel\"\"\"\n\n        if year_days is None:\n            year_days = range(1, 366)  # Full year\n\n        print(f\"Calculating solar radiation for {len(year_days)} days using {self.num_threads} threads\")\n\n        # Process days in parallel\n        with ProcessPoolExecutor(max_workers=self.num_threads) as executor:\n\n            # Submit all tasks\n            future_to_day = {\n                executor.submit(\n                    self.calculate_daily_solar, \n                    day, step, linke_value, albedo_value\n                ): day for day in year_days\n            }\n\n            # Collect results\n            completed_days = []\n            for future in as_completed(future_to_day):\n                day = future_to_day[future]\n                try:\n                    result = future.result()\n                    completed_days.append(result)\n                    print(f\"‚úì Day {day} completed ({len(completed_days)}/{len(year_days)})\")\n                except Exception as e:\n                    print(f\"‚úó Day {day} failed: {e}\")\n\n        return completed_days\n\n    def calculate_monthly_summaries(self):\n        \"\"\"Calculate monthly summaries from daily outputs\"\"\"\n\n        months = {\n            'jan': range(1, 32),   'feb': range(32, 60),   'mar': range(60, 91),\n            'apr': range(91, 121), 'may': range(121, 152), 'jun': range(152, 182),\n            'jul': range(182, 213), 'aug': range(213, 244), 'sep': range(244, 274),\n            'oct': range(274, 305), 'nov': range(305, 335), 'dec': range(335, 366)\n        }\n\n        for month, days in months.items():\n\n            # Build list of daily files\n            global_files = [f\"{self.output_dir}/global/daily/total_sun_day_{day}.tif\" \n                          for day in days]\n            insol_files = [f\"{self.output_dir}/insol/daily/hours_sun_day_{day}.tif\" \n                         for day in days]\n\n            # Check if all daily files exist\n            missing_files = [f for f in global_files + insol_files if not os.path.exists(f)]\n            if missing_files:\n                print(f\"Warning: Missing files for {month}: {len(missing_files)} files\")\n                continue\n\n            # Calculate monthly sums using GDAL\n            global_output = f\"{self.output_dir}/global/monthly/total_sun_{month}_sum.tif\"\n            insol_output = f\"{self.output_dir}/insol/monthly/hours_sun_{month}_sum.tif\"\n\n            # Sum global radiation\n            global_vrt = f\"/tmp/global_{month}.vrt\"\n            subprocess.run([\n                'gdalbuildvrt', '-separate', global_vrt\n            ] + global_files, check=True)\n\n            subprocess.run([\n                'gdal_calc.py', '-A', global_vrt, '--calc=sum(A,axis=0)',\n                '--outfile', global_output, '--co', 'COMPRESS=LZW'\n            ], check=True)\n\n            # Sum insolation hours  \n            insol_vrt = f\"/tmp/insol_{month}.vrt\"\n            subprocess.run([\n                'gdalbuildvrt', '-separate', insol_vrt\n            ] + insol_files, check=True)\n\n            subprocess.run([\n                'gdal_calc.py', '-A', insol_vrt, '--calc=sum(A,axis=0)', \n                '--outfile', insol_output, '--co', 'COMPRESS=LZW'\n            ], check=True)\n\n            print(f\"‚úì {month} monthly summary completed\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='EEMT Solar Radiation Calculator')\n    parser.add_argument('dem', help='Input DEM file path')\n    parser.add_argument('--output', '-o', default='./solar_output', \n                       help='Output directory')\n    parser.add_argument('--threads', '-t', type=int, default=mp.cpu_count(),\n                       help='Number of parallel threads')\n    parser.add_argument('--step', type=float, default=0.25,\n                       help='Solar calculation time step (hours)')\n    parser.add_argument('--linke', type=float, default=3.0,\n                       help='Linke atmospheric turbidity factor')\n    parser.add_argument('--albedo', type=float, default=0.2,\n                       help='Surface albedo value')\n    parser.add_argument('--days', nargs='+', type=int,\n                       help='Specific days to calculate (default: full year)')\n\n    args = parser.parse_args()\n\n    # Initialize calculator\n    calculator = GrassSolarCalculator(\n        args.dem, \n        args.output, \n        args.threads\n    )\n\n    # Calculate solar radiation\n    days = args.days if args.days else range(1, 366)\n    completed = calculator.calculate_annual_solar(\n        days, args.step, args.linke, args.albedo\n    )\n\n    # Calculate monthly summaries\n    if len(completed) &gt;= 300:  # Most of year calculated\n        calculator.calculate_monthly_summaries()\n\n    print(f\"Solar radiation calculation complete!\")\n    print(f\"Output directory: {args.output}\")\n    print(f\"Completed days: {len(completed)}\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#optimized-rsun-configuration","level":3,"title":"Optimized r.sun Configuration","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#memory-management","level":4,"title":"Memory Management","text":"<pre><code># For large DEMs, set memory limits\nexport GRASS_VECTOR_TMPDIR_MAPSET=/tmp\nexport GRASS_RASTER_TMPDIR_MAPSET=/tmp\n\n# Increase cache size\ng.gisenv set=\"GRASS_CACHE_SIZE=2048\"\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#multi-core-configuration","level":4,"title":"Multi-core Configuration","text":"<pre><code># Optimize for system architecture\nexport OMP_NUM_THREADS=$(nproc)\nexport GRASS_NUM_THREADS=$(nproc)\n\n# NUMA-aware processing (large systems)\nnumactl --cpunodebind=0 --membind=0 r.sun elevation=dem ...\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#high-performance-rsun-workflow","level":3,"title":"High-Performance r.sun Workflow","text":"<p>Based on the <code>/sol/rsun.sh</code> analysis, here's the optimized workflow:</p> <pre><code>#!/bin/bash\n# High-performance solar radiation calculation\n# Enhanced version of sol/rsun.sh\n\nset -e\n\n# Parse command line arguments\nwhile [[ $# -gt 0 ]]; do\n  case $1 in\n    -d|--day)\n      DAY=\"$2\"\n      shift 2\n      ;;\n    -s|--step)  \n      STEP=\"$2\"\n      shift 2\n      ;;\n    -t|--threads)\n      NUM_THREADS=\"$2\"\n      shift 2\n      ;;\n    -l|--linke)\n      LINKE_VALUE=\"$2\"\n      shift 2\n      ;;\n    -a|--albedo)\n      ALBEDO_VALUE=\"$2\"\n      shift 2\n      ;;\n    -o|--output)\n      OUTPUT_DIR=\"$2\"\n      shift 2\n      ;;\n    *)\n      DEM_FILE=\"$1\"\n      shift\n      ;;\n  esac\ndone\n\n# Set defaults\nNUM_THREADS=${NUM_THREADS:-$(nproc)}\nSTEP=${STEP:-0.25}\nLINKE_VALUE=${LINKE_VALUE:-3.0}\nALBEDO_VALUE=${ALBEDO_VALUE:-0.2}\nOUTPUT_DIR=${OUTPUT_DIR:-\"./solar_output\"}\n\n# Validate inputs\nif [[ ! -f \"$DEM_FILE\" ]]; then\n    echo \"Error: DEM file not found: $DEM_FILE\"\n    exit 1\nfi\n\n# Create temporary GRASS location\nTEMP_LOCATION=$(mktemp -d)\nGRASS_LOCATION=\"$TEMP_LOCATION/solar_calc\"\n\n# Setup GRASS environment\nexport GISDBASE=\"$TEMP_LOCATION\"\nexport LOCATION_NAME=\"solar_calc\"\nexport MAPSET=\"PERMANENT\"\nexport GRASS_GUI=\"text\"\nexport GRASS_VERBOSE=0\n\necho \"Starting solar calculation for day $DAY\"\necho \"Threads: $NUM_THREADS, Step: ${STEP}h, Linke: $LINKE_VALUE, Albedo: $ALBEDO_VALUE\"\n\n# Create output directories\nmkdir -p \"$OUTPUT_DIR/global/daily\"\nmkdir -p \"$OUTPUT_DIR/insol/daily\"\n\n# Run GRASS commands\ngrass --text \"$GRASS_LOCATION\" --exec &lt;&lt; EOF\n# Create location from DEM\ng.proj -c georef=$DEM_FILE\n\n# Import DEM\nr.in.gdal input=$DEM_FILE output=dem\n\n# Set computational region\ng.region raster=dem\n\n# Calculate terrain derivatives\necho \"Calculating slope and aspect...\"\nr.slope.aspect elevation=dem slope=slope_deg aspect=aspect_deg\n\n# Calculate solar radiation with optimal threading\necho \"Running r.sun for day $DAY...\"\nr.sun elevation=dem aspect=aspect_deg slope=slope_deg \\\\\n         day=$DAY step=$STEP \\\\\n         linke_value=$LINKE_VALUE albedo_value=$ALBEDO_VALUE \\\\\n         nprocs=$NUM_THREADS \\\\\n         glob_rad=solar_global insol_time=solar_hours\n\n# Export results with compression\necho \"Exporting results...\"\nr.out.gdal input=solar_global \\\\\n           output=$OUTPUT_DIR/global/daily/total_sun_day_$DAY.tif \\\\\n           createopt=\"COMPRESS=LZW,TILED=YES,BLOCKXSIZE=512,BLOCKYSIZE=512\"\n\nr.out.gdal input=solar_hours \\\\\n           output=$OUTPUT_DIR/insol/daily/hours_sun_day_$DAY.tif \\\\\n           createopt=\"COMPRESS=LZW,TILED=YES,BLOCKXSIZE=512,BLOCKYSIZE=512\"\n\necho \"Day $DAY completed successfully\"\nEOF\n\n# Cleanup\nrm -rf \"$TEMP_LOCATION\"\n\necho \"Solar calculation for day $DAY finished\"\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#batch-processing-all-days","level":3,"title":"Batch Processing All Days","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nBatch process full year of solar radiation calculations\nEnhanced version of sol/run-workflow\n\"\"\"\n\nimport subprocess\nimport multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor\nimport argparse\nfrom pathlib import Path\n\ndef run_daily_solar(day, dem_file, output_dir, step, linke, albedo, threads_per_day):\n    \"\"\"Run solar calculation for single day\"\"\"\n\n    cmd = [\n        'bash', 'enhanced_rsun.sh',\n        '--day', str(day),\n        '--step', str(step),\n        '--linke', str(linke),\n        '--albedo', str(albedo),  \n        '--threads', str(threads_per_day),\n        '--output', output_dir,\n        dem_file\n    ]\n\n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)\n        if result.returncode == 0:\n            return day, True, None\n        else:\n            return day, False, result.stderr\n    except subprocess.TimeoutExpired:\n        return day, False, \"Timeout after 1 hour\"\n    except Exception as e:\n        return day, False, str(e)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dem', help='Input DEM file')\n    parser.add_argument('--output', '-o', default='./solar_annual', help='Output directory')\n    parser.add_argument('--workers', '-w', type=int, default=mp.cpu_count()//4, \n                       help='Number of parallel day processes')\n    parser.add_argument('--threads-per-day', '-t', type=int, default=4,\n                       help='Threads per daily calculation')\n    parser.add_argument('--step', type=float, default=0.25, help='Time step (hours)')\n    parser.add_argument('--linke', type=float, default=3.0, help='Linke turbidity')  \n    parser.add_argument('--albedo', type=float, default=0.2, help='Surface albedo')\n    parser.add_argument('--start-day', type=int, default=1, help='Start day of year')\n    parser.add_argument('--end-day', type=int, default=365, help='End day of year')\n\n    args = parser.parse_args()\n\n    # Create output directory\n    Path(args.output).mkdir(parents=True, exist_ok=True)\n\n    # Days to process\n    days = range(args.start_day, args.end_day + 1)\n\n    print(f\"Processing {len(days)} days using {args.workers} parallel workers\")\n    print(f\"Each day uses {args.threads_per_day} threads\")\n    print(f\"Total system load: {args.workers * args.threads_per_day} threads\")\n\n    # Process days in parallel\n    completed = []\n    failed = []\n\n    with ProcessPoolExecutor(max_workers=args.workers) as executor:\n\n        # Submit all day calculations\n        future_to_day = {\n            executor.submit(\n                run_daily_solar,\n                day, args.dem, args.output, \n                args.step, args.linke, args.albedo, args.threads_per_day\n            ): day for day in days\n        }\n\n        # Collect results\n        for future in future_to_day:\n            day, success, error = future.result()\n\n            if success:\n                completed.append(day)\n                print(f\"‚úì Day {day} ({len(completed)}/{len(days)})\")\n            else:\n                failed.append((day, error))\n                print(f\"‚úó Day {day} failed: {error}\")\n\n    # Summary\n    print(f\"\\nCompleted: {len(completed)} days\")\n    print(f\"Failed: {len(failed)} days\")\n\n    if failed:\n        print(\"\\nFailed days:\")\n        for day, error in failed:\n            print(f\"  Day {day}: {error}\")\n\n    return len(completed) &gt; 0\n\nif __name__ == '__main__':\n    success = main()\n    sys.exit(0 if success else 1)\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#gpu-acceleration-grass-8x","level":2,"title":"GPU Acceleration (GRASS 8.x)","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#opencl-setup-for-rsun","level":3,"title":"OpenCL Setup for r.sun","text":"<pre><code># Check OpenCL availability\ngrass --text &lt;&lt; EOF\nr.sun --help | grep -i opencl\nEOF\n\n# Enable GPU acceleration (if available)\ngrass --text &lt;&lt; EOF\n# Set OpenCL device\ng.gisenv set=\"GRASS_OPENCL_DEVICE=0\"\n\n# Run r.sun with GPU acceleration  \nr.sun elevation=dem aspect=aspect slope=slope \\\\\n         day=180 step=0.1 opencl=yes \\\\\n         glob_rad=solar_gpu insol_time=hours_gpu\nEOF\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#memory-optimization-for-large-dems","level":2,"title":"Memory Optimization for Large DEMs","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#tiled-processing","level":3,"title":"Tiled Processing","text":"<pre><code># Split large DEM into manageable tiles\ngdal_retile.py -ps 2048 2048 -overlap 128 \\\\\n               -targetDir dem_tiles/ large_dem.tif\n\n# Process each tile separately\nfor tile in dem_tiles/*.tif; do\n    echo \"Processing $tile...\"\n    python enhanced_solar_calc.py \"$tile\" --output \"results_$(basename $tile .tif)\"\ndone\n\n# Merge results\ngdal_merge.py -o final_solar.tif results_*/global/monthly/*.tif\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#chunked-processing-with-grass","level":3,"title":"Chunked Processing with GRASS","text":"<pre><code>def process_dem_chunks(dem_path, chunk_size=2048, overlap=128):\n    \"\"\"Process large DEM in chunks\"\"\"\n\n    import rasterio\n    from rasterio.windows import Window\n\n    with rasterio.open(dem_path) as src:\n        height, width = src.shape\n\n        # Calculate chunk coordinates\n        chunks = []\n        for row in range(0, height, chunk_size - overlap):\n            for col in range(0, width, chunk_size - overlap):\n\n                # Define window\n                window = Window(\n                    col, row,\n                    min(chunk_size, width - col),\n                    min(chunk_size, height - row)\n                )\n\n                chunks.append(window)\n\n        print(f\"Processing {len(chunks)} chunks\")\n\n        # Process each chunk\n        for i, window in enumerate(chunks):\n\n            # Extract chunk\n            chunk_data = src.read(1, window=window)\n            chunk_transform = src.window_transform(window)\n\n            # Save chunk as temporary file\n            chunk_profile = src.profile.copy()\n            chunk_profile.update({\n                'height': window.height,\n                'width': window.width,\n                'transform': chunk_transform\n            })\n\n            chunk_file = f'chunk_{i}.tif'\n            with rasterio.open(chunk_file, 'w', **chunk_profile) as dst:\n                dst.write(chunk_data, 1)\n\n            # Process chunk with solar calculator\n            calculator = GrassSolarCalculator(chunk_file, f'chunk_output_{i}')\n            calculator.calculate_annual_solar()\n\n            # Cleanup chunk file\n            os.remove(chunk_file)\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#performance-monitoring","level":2,"title":"Performance Monitoring","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#resource-usage-tracking","level":3,"title":"Resource Usage Tracking","text":"<pre><code>import psutil\nimport time\nfrom functools import wraps\n\ndef monitor_performance(func):\n    \"\"\"Decorator to monitor CPU and memory usage\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n\n        # Initial measurements\n        start_time = time.time()\n        process = psutil.Process()\n        start_cpu = process.cpu_percent()\n        start_memory = process.memory_info().rss / 1024**2  # MB\n\n        try:\n            # Run function\n            result = func(*args, **kwargs)\n\n            # Final measurements\n            end_time = time.time()\n            end_cpu = process.cpu_percent()\n            end_memory = process.memory_info().rss / 1024**2\n\n            # Print performance stats\n            duration = end_time - start_time\n            print(f\"\\nPerformance Summary:\")\n            print(f\"  Duration: {duration:.1f} seconds\")\n            print(f\"  CPU usage: {end_cpu:.1f}%\")\n            print(f\"  Memory usage: {end_memory:.1f} MB\")\n            print(f\"  Memory change: {end_memory - start_memory:+.1f} MB\")\n\n            return result\n\n        except Exception as e:\n            print(f\"Error during execution: {e}\")\n            raise\n\n    return wrapper\n\n# Apply to solar calculations\n@monitor_performance\ndef calculate_solar_monitored(day, dem_file, output_dir):\n    \"\"\"Solar calculation with performance monitoring\"\"\"\n    calculator = GrassSolarCalculator(dem_file, output_dir)\n    return calculator.calculate_daily_solar(day)\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#advanced-grass-configuration","level":2,"title":"Advanced GRASS Configuration","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#parallel-processing-environment","level":3,"title":"Parallel Processing Environment","text":"<pre><code># ~/.bashrc configuration for GRASS parallel processing\nexport GRASS_NUM_THREADS=$(nproc)\nexport OMP_NUM_THREADS=$(nproc)\nexport GRASS_CACHE_SIZE=2048\nexport GRASS_RENDER_IMMEDIATE=FALSE\nexport GRASS_COMPRESS_NULLS=1\n\n# For HPC environments\nexport GRASS_BATCH_JOB=TRUE\nexport GRASS_GUI=text\nexport GRASS_VERBOSE=0\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#clusterhpc-integration","level":3,"title":"Cluster/HPC Integration","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=eemt_solar\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=24\n#SBATCH --memory=64GB\n#SBATCH --time=12:00:00\n\n# Load modules\nmodule load grass/8.3 gdal/3.6 python/3.11\n\n# Set GRASS environment\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport GRASS_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Run solar calculations\npython enhanced_solar_calc.py $DEM_FILE \\\\\n  --threads $SLURM_CPUS_PER_TASK \\\\\n  --output $SLURM_SUBMIT_DIR/solar_results\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#common-issues","level":3,"title":"Common Issues","text":"","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#memory-errors","level":4,"title":"Memory Errors","text":"<pre><code># Reduce processing extent\ng.region -s res=30  # Decrease resolution temporarily\n\n# Use tiled processing\nr.tile input=large_dem output=dem_tile prefix=chunk_ width=2048 height=2048\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#projection-issues","level":4,"title":"Projection Issues","text":"<pre><code># Check DEM projection\ngdalinfo dem.tif | grep -i \"coordinate system\"\n\n# Reproject if needed\ngdalwarp -t_srs EPSG:4326 input_dem.tif output_dem_wgs84.tif\n</code></pre>","path":["GRASS GIS"],"tags":[]},{"location":"grass-gis/#rsun-errors","level":4,"title":"r.sun Errors","text":"<pre><code># Validate slope/aspect values\nr.info slope_deg\nr.info aspect_deg\n\n# Check for null values\nr.null setnull=\"-9999\" slope_deg\n</code></pre> <p>Next: Complete EEMT Workflows</p>","path":["GRASS GIS"],"tags":[]},{"location":"infrastructure/","level":1,"title":"Infrastructure Documentation","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#overview","level":2,"title":"Overview","text":"<p>The EEMT infrastructure documentation provides comprehensive technical details about the system architecture, deployment configurations, and operational considerations for running EEMT at scale.</p>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#documentation-sections","level":2,"title":"Documentation Sections","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#container-architecture","level":3,"title":"Container Architecture","text":"<p>Detailed documentation of the Docker container ecosystem including: - Multi-layered container design - Image composition and dependencies - Volume management strategies - Network configuration - Security considerations - Performance optimization</p>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#docker-deployment-guide","level":3,"title":"Docker Deployment Guide","text":"<p>Step-by-step instructions for deploying EEMT using Docker: - Prerequisites and installation - Quick start procedures - Deployment modes (local, distributed, documentation) - Configuration options - Monitoring and maintenance</p>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#web-interface-architecture","level":3,"title":"Web Interface Architecture","text":"<p>Technical architecture of the FastAPI web application: - System design and components - Request handling flow - Database architecture - Frontend implementation - Container orchestration - Performance considerations</p>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#distributed-deployment","level":3,"title":"Distributed Deployment","text":"<p>Guide for scaling EEMT across multiple nodes: - Master-worker architecture - HPC integration examples - Container orchestration platforms - Network and storage configuration</p>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#infrastructure-components","level":2,"title":"Infrastructure Components","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#container-stack","level":3,"title":"Container Stack","text":"<pre><code>graph TD\n    subgraph \"EEMT Container Ecosystem\"\n        A[Ubuntu 24.04 Base]\n        B[Scientific Stack Layer]\n        C[EEMT Core Layer]\n        D1[Web Interface Container]\n        D2[Worker Container]\n        D3[Documentation Container]\n\n        A --&gt; B\n        B --&gt; C\n        C --&gt; D1\n        C --&gt; D2\n        A --&gt; D3\n    end</code></pre>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#key-technologies","level":3,"title":"Key Technologies","text":"Component Technology Version Purpose Base OS Ubuntu 24.04 LTS Container operating system Container Runtime Docker 20.10+ Container execution Orchestration Docker Compose v2.0+ Multi-container management Web Framework FastAPI 0.100+ REST API and web interface Workflow Engine CCTools 7.8.2 Distributed task execution GIS Engine GRASS GIS 8.4+ Geospatial processing Database SQLite 3.x Job tracking and persistence","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#deployment-architecture","level":3,"title":"Deployment Architecture","text":"<pre><code>graph LR\n    subgraph \"User Access\"\n        U1[Web Browser]\n        U2[REST API Client]\n        U3[CLI Tools]\n    end\n\n    subgraph \"Application Layer\"\n        W[Web Interface]\n        A[API Gateway]\n    end\n\n    subgraph \"Processing Layer\"\n        M[Master Node]\n        W1[Worker 1]\n        W2[Worker 2]\n        WN[Worker N]\n    end\n\n    subgraph \"Storage Layer\"\n        V1[Data Volumes]\n        V2[Results Storage]\n        DB[Database]\n    end\n\n    U1 --&gt; W\n    U2 --&gt; A\n    U3 --&gt; A\n    W --&gt; M\n    A --&gt; M\n    M --&gt; W1\n    M --&gt; W2\n    M --&gt; WN\n    W1 --&gt; V1\n    W2 --&gt; V1\n    WN --&gt; V2\n    M --&gt; DB</code></pre>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#resource-requirements","level":2,"title":"Resource Requirements","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#minimum-infrastructure","level":3,"title":"Minimum Infrastructure","text":"Resource Minimum Recommended Notes CPU 4 cores 8+ cores More cores enable parallel processing RAM 8 GB 16+ GB 2GB per worker thread Storage 50 GB 200+ GB Depends on dataset size Network 10 Mbps 100+ Mbps For climate data downloads Docker 20.10 Latest stable Required for container execution","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#scaling-considerations","level":3,"title":"Scaling Considerations","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#vertical-scaling-single-node","level":4,"title":"Vertical Scaling (Single Node)","text":"<ul> <li>Increase CPU cores for more parallel workers</li> <li>Add RAM for larger datasets</li> <li>Use SSD storage for improved I/O</li> <li>GPU acceleration (future enhancement)</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#horizontal-scaling-multi-node","level":4,"title":"Horizontal Scaling (Multi-Node)","text":"<ul> <li>Deploy master node for coordination</li> <li>Add worker nodes for processing</li> <li>Use shared storage (NFS, S3)</li> <li>Implement load balancing</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#network-architecture","level":2,"title":"Network Architecture","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#port-allocations","level":3,"title":"Port Allocations","text":"Service Port Protocol Purpose Web Interface 5000 HTTP Browser access Work Queue 9123 TCP Master-worker communication Documentation 8000 HTTP MkDocs server Monitoring 9090 HTTP Prometheus (future) Database 5432 TCP PostgreSQL (future)","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#security-zones","level":3,"title":"Security Zones","text":"<pre><code>graph TB\n    subgraph \"Public Zone\"\n        I[Internet]\n        LB[Load Balancer]\n    end\n\n    subgraph \"DMZ\"\n        WEB[Web Interface]\n        API[API Gateway]\n    end\n\n    subgraph \"Private Zone\"\n        MASTER[Master Node]\n        WORKERS[Worker Pool]\n        STORAGE[Storage]\n        DB[Database]\n    end\n\n    I --&gt; LB\n    LB --&gt; WEB\n    LB --&gt; API\n    WEB --&gt; MASTER\n    API --&gt; MASTER\n    MASTER --&gt; WORKERS\n    WORKERS --&gt; STORAGE\n    MASTER --&gt; DB</code></pre>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#storage-architecture","level":2,"title":"Storage Architecture","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#volume-types","level":3,"title":"Volume Types","text":"Volume Type Persistence Purpose uploads Bind mount Persistent DEM file uploads results Bind mount Persistent Workflow outputs temp tmpfs Ephemeral Processing scratch cache Bind mount Semi-persistent Workflow caching shared NFS/S3 Persistent Distributed storage","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#data-flow","level":3,"title":"Data Flow","text":"<ol> <li>Input Stage: DEM files uploaded to <code>uploads/</code> volume</li> <li>Processing Stage: Temporary data in <code>temp/</code> volume</li> <li>Output Stage: Results written to <code>results/</code> volume</li> <li>Archive Stage: Results compressed and stored</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#monitoring-and-observability","level":2,"title":"Monitoring and Observability","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#health-checks","level":3,"title":"Health Checks","text":"<pre><code># Docker health check configuration\nhealthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 40s\n</code></pre>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#metrics-collection","level":3,"title":"Metrics Collection","text":"<ul> <li>System Metrics: CPU, memory, disk, network</li> <li>Application Metrics: Job count, processing time, success rate</li> <li>Container Metrics: Resource usage, restart count</li> <li>Custom Metrics: EEMT-specific calculations</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#logging-strategy","level":3,"title":"Logging Strategy","text":"Component Log Location Retention Level Web Interface <code>/app/logs/</code> 7 days INFO Workers Container stdout 24 hours INFO System <code>/var/log/</code> 30 days WARNING Audit Database 90 days ALL","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#disaster-recovery","level":2,"title":"Disaster Recovery","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#backup-strategy","level":3,"title":"Backup Strategy","text":"<ol> <li>Database Backups: Daily SQLite dumps</li> <li>Volume Snapshots: Weekly filesystem snapshots</li> <li>Configuration Backup: Version controlled in Git</li> <li>Container Images: Registry backups</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#recovery-procedures","level":3,"title":"Recovery Procedures","text":"<ol> <li>Service Failure: Auto-restart via Docker</li> <li>Node Failure: Failover to standby node</li> <li>Data Loss: Restore from backups</li> <li>Complete Disaster: Rebuild from infrastructure-as-code</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#performance-tuning","level":2,"title":"Performance Tuning","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#container-optimization","level":3,"title":"Container Optimization","text":"<pre><code># Resource limits and reservations\ndeploy:\n  resources:\n    limits:\n      cpus: '4.0'\n      memory: 8G\n    reservations:\n      cpus: '2.0'\n      memory: 4G\n</code></pre>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#network-optimization","level":3,"title":"Network Optimization","text":"<ul> <li>Use bridge networks for local communication</li> <li>Enable host networking for performance-critical workers</li> <li>Implement connection pooling</li> <li>Configure DNS caching</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#storage-optimization","level":3,"title":"Storage Optimization","text":"<ul> <li>Use SSD for temporary processing</li> <li>Enable compression for results</li> <li>Implement data deduplication</li> <li>Regular cleanup of temporary files</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#best-practices","level":2,"title":"Best Practices","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#deployment","level":3,"title":"Deployment","text":"<ol> <li>Use infrastructure-as-code (Docker Compose, Kubernetes manifests)</li> <li>Implement blue-green deployments</li> <li>Maintain staging environments</li> <li>Automate deployment pipelines</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#security","level":3,"title":"Security","text":"<ol> <li>Run containers as non-root users</li> <li>Implement network segmentation</li> <li>Enable TLS for all communications</li> <li>Regular security updates</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#operations","level":3,"title":"Operations","text":"<ol> <li>Monitor all critical metrics</li> <li>Implement automated alerts</li> <li>Maintain runbooks for common issues</li> <li>Regular disaster recovery testing</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#troubleshooting-guide","level":2,"title":"Troubleshooting Guide","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#common-issues","level":3,"title":"Common Issues","text":"Issue Cause Solution Container won't start Missing image Run <code>docker-compose build</code> Out of memory Resource limits Increase memory allocation Slow performance I/O bottleneck Use SSD storage Network timeouts Firewall rules Check port accessibility Job failures Invalid parameters Review parameter validation","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#diagnostic-commands","level":3,"title":"Diagnostic Commands","text":"<pre><code># Check container status\ndocker ps -a\n\n# View container logs\ndocker logs &lt;container_name&gt;\n\n# Inspect container\ndocker inspect &lt;container_name&gt;\n\n# Monitor resources\ndocker stats\n\n# Network diagnostics\ndocker network ls\ndocker network inspect &lt;network_name&gt;\n\n# Volume inspection\ndocker volume ls\ndocker volume inspect &lt;volume_name&gt;\n</code></pre>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#future-enhancements","level":2,"title":"Future Enhancements","text":"","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#planned-infrastructure-improvements","level":3,"title":"Planned Infrastructure Improvements","text":"<ol> <li>Kubernetes Migration: Helm charts and operators</li> <li>Service Mesh: Istio/Linkerd integration</li> <li>Observability Stack: Prometheus + Grafana + Loki</li> <li>CI/CD Pipeline: GitHub Actions + ArgoCD</li> <li>Multi-Cloud Support: AWS, GCP, Azure deployments</li> </ol>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#roadmap","level":3,"title":"Roadmap","text":"<ul> <li>Q1 2025: Kubernetes deployment support</li> <li>Q2 2025: Enhanced monitoring and alerting</li> <li>Q3 2025: Multi-region deployment</li> <li>Q4 2025: Serverless execution options</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/#support-resources","level":2,"title":"Support Resources","text":"<ul> <li>GitHub Issues</li> <li>Discussion Forum</li> <li>Container Registry</li> <li>Documentation</li> </ul>","path":["Infrastructure","Infrastructure Documentation"],"tags":[]},{"location":"infrastructure/container-architecture/","level":1,"title":"Container Architecture","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#overview","level":2,"title":"Overview","text":"<p>The EEMT system utilizes a multi-layered container architecture designed for scalability, reproducibility, and ease of deployment. This document provides a comprehensive overview of the container ecosystem, including design decisions, component interactions, and deployment patterns.</p>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#architecture-design-principles","level":2,"title":"Architecture Design Principles","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#layered-container-strategy","level":3,"title":"Layered Container Strategy","text":"<p>The EEMT container architecture follows a layered approach with clear separation of concerns:</p> <ol> <li>Base Layer: Ubuntu 24.04 LTS with system dependencies</li> <li>Scientific Stack Layer: GDAL, GRASS GIS, geospatial libraries</li> <li>EEMT Core Layer: Workflow scripts and scientific algorithms</li> <li>Application Layer: Web interface, API, orchestration tools</li> </ol> <pre><code>graph TD\n    subgraph \"Container Layers\"\n        A[Ubuntu 24.04 LTS Base] --&gt; B[Scientific Stack]\n        B --&gt; C[EEMT Core]\n        C --&gt; D1[Web Interface]\n        C --&gt; D2[Worker Node]\n        C --&gt; D3[Documentation]\n    end</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-images","level":3,"title":"Container Images","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#base-container-eemtubuntu2404","level":4,"title":"Base Container: <code>eemt:ubuntu24.04</code>","text":"<p>Purpose: Core computational environment with all scientific dependencies</p> <p>Key Components: - Ubuntu 24.04 LTS base operating system - Python 3.12 with Miniconda environment management - GDAL 3.11+ with complete geospatial stack - GRASS GIS 8.4+ compiled with EEMT extensions - CCTools 7.8.2 (Makeflow + Work Queue) - Scientific Python libraries (numpy, pandas, xarray, rasterio)</p> <p>Build Process: <pre><code>cd docker/ubuntu/24.04/\n./build.sh\n# Or manually:\ndocker build -t eemt:ubuntu24.04 .\n</code></pre></p> <p>Size: ~3.5 GB Base Image: <code>ubuntu:24.04</code></p>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#web-interface-container-eemt-web","level":4,"title":"Web Interface Container: <code>eemt-web</code>","text":"<p>Purpose: FastAPI application for job submission and monitoring</p> <p>Key Components: - Inherits from <code>eemt:ubuntu24.04</code> - FastAPI web framework - SQLite job database - Docker SDK for container management - Workflow orchestration logic</p> <p>Build Process: <pre><code>docker build -t eemt-web -f docker/web-interface/Dockerfile .\n</code></pre></p> <p>Size: ~3.8 GB (includes base) Exposed Ports: 5000 (web), 9123 (work queue)</p>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#documentation-container-eemt-docs","level":4,"title":"Documentation Container: <code>eemt-docs</code>","text":"<p>Purpose: MkDocs documentation server</p> <p>Key Components: - Python 3.11 slim base - MkDocs Material theme - Documentation plugins - Live reload capability</p> <p>Build Process: <pre><code>docker build -t eemt-docs -f docker/docs/Dockerfile .\n</code></pre></p> <p>Size: ~200 MB Exposed Port: 8000</p>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-orchestration","level":2,"title":"Container Orchestration","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#docker-compose-architecture","level":3,"title":"Docker Compose Architecture","text":"<p>The <code>docker-compose.yml</code> defines three deployment profiles:</p> <pre><code># Default Profile: Local single-node deployment\nservices:\n  eemt-web:\n    profiles: [default]\n    ports: [\"5000:5000\"]\n    volumes:\n      - ./data:/app/data\n      - /var/run/docker.sock:/var/run/docker.sock\n\n# Distributed Profile: Multi-node cluster\n  eemt-master:\n    profiles: [distributed]\n    ports: [\"5000:5000\", \"9123:9123\"]\n\n  eemt-worker:\n    profiles: [distributed]\n    deploy:\n      replicas: 5\n\n# Documentation Profile\n  eemt-docs:\n    profiles: [docs]\n    ports: [\"8000:8000\"]\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-communication","level":3,"title":"Container Communication","text":"<pre><code>graph LR\n    subgraph \"Host Machine\"\n        subgraph \"Docker Network: eemt-network\"\n            WEB[Web Interface&lt;br/&gt;Container]\n            WORK1[Worker 1&lt;br/&gt;Container]\n            WORK2[Worker 2&lt;br/&gt;Container]\n            WORKN[Worker N&lt;br/&gt;Container]\n        end\n\n        subgraph \"Volumes\"\n            UPLOADS[uploads/]\n            RESULTS[results/]\n            TEMP[temp/]\n            CACHE[cache/]\n        end\n\n        DOCKER[Docker&lt;br/&gt;Daemon]\n    end\n\n    USER[User&lt;br/&gt;Browser] --&gt; WEB\n    WEB --&gt; DOCKER\n    DOCKER --&gt; WORK1\n    DOCKER --&gt; WORK2\n    DOCKER --&gt; WORKN\n\n    WORK1 --&gt; UPLOADS\n    WORK1 --&gt; RESULTS\n    WORK2 --&gt; TEMP\n    WORKN --&gt; CACHE</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#volume-management","level":2,"title":"Volume Management","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#data-persistence-strategy","level":3,"title":"Data Persistence Strategy","text":"<p>The container architecture uses Docker volumes for data persistence with specific mount points:</p> Volume Container Path Purpose Persistence <code>uploads</code> <code>/app/uploads</code> DEM file uploads Persistent <code>results</code> <code>/app/results</code> Workflow outputs Persistent <code>temp</code> <code>/app/temp</code> Processing scratch space Ephemeral <code>cache</code> <code>/app/cache</code> Workflow caching Semi-persistent <code>shared</code> <code>/app/shared</code> Distributed mode shared data Persistent","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#volume-configuration","level":3,"title":"Volume Configuration","text":"<pre><code># Docker Compose volume definitions\nvolumes:\n  eemt-data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /data/eemt  # Host directory\n\n# Container mount configuration\nvolumes:\n  - type: bind\n    source: ./data/uploads\n    target: /app/uploads\n    read_only: false\n  - type: bind\n    source: ./data/results\n    target: /app/results\n    read_only: false\n  - type: tmpfs\n    target: /tmp\n    tmpfs:\n      size: 2G\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#data-flow-architecture","level":3,"title":"Data Flow Architecture","text":"<pre><code>sequenceDiagram\n    participant User\n    participant WebUI\n    participant Container\n    participant Volume\n    participant Results\n\n    User-&gt;&gt;WebUI: Upload DEM\n    WebUI-&gt;&gt;Volume: Save to uploads/\n    WebUI-&gt;&gt;Container: Start workflow\n    Container-&gt;&gt;Volume: Read DEM from uploads/\n    Container-&gt;&gt;Volume: Write temp data\n    Container-&gt;&gt;Container: Process workflow\n    Container-&gt;&gt;Results: Write outputs\n    Container-&gt;&gt;WebUI: Report completion\n    WebUI-&gt;&gt;User: Provide download link</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#network-configuration","level":2,"title":"Network Configuration","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#bridge-network","level":3,"title":"Bridge Network","text":"<p>The default <code>eemt-network</code> uses Docker's bridge driver:</p> <pre><code>networks:\n  eemt-network:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.28.0.0/16\n          gateway: 172.28.0.1\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#service-discovery","level":3,"title":"Service Discovery","text":"<p>Containers use Docker's internal DNS for service discovery:</p> <ul> <li>Web interface: <code>eemt-web</code> or <code>eemt-master</code></li> <li>Workers: <code>eemt-worker</code>, <code>eemt-worker-2</code>, etc.</li> <li>Documentation: <code>eemt-docs</code></li> </ul>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#port-exposure","level":3,"title":"Port Exposure","text":"Service Internal Port External Port Protocol Purpose Web Interface 5000 5000 HTTP FastAPI application Work Queue 9123 9123 TCP CCTools master Documentation 8000 8000 HTTP MkDocs server Monitoring 9090 9090 HTTP Prometheus (future)","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-lifecycle-management","level":2,"title":"Container Lifecycle Management","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#startup-sequence","level":3,"title":"Startup Sequence","text":"<ol> <li>Network Creation: Docker creates <code>eemt-network</code></li> <li>Volume Initialization: Bind mounts are established</li> <li>Base Services: Database, cache services start</li> <li>Web Interface: FastAPI application initializes</li> <li>Workers: Worker containers connect to master</li> <li>Health Checks: Containers report ready status</li> </ol>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#health-monitoring","level":3,"title":"Health Monitoring","text":"<pre><code># Health check configuration\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n  CMD curl -f http://localhost:5000/health || exit 1\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#resource-limits","level":3,"title":"Resource Limits","text":"<pre><code># Container resource constraints\ndeploy:\n  resources:\n    limits:\n      cpus: '4'\n      memory: 8G\n    reservations:\n      cpus: '2'\n      memory: 4G\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#security-considerations","level":2,"title":"Security Considerations","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-security","level":3,"title":"Container Security","text":"<ol> <li>Non-root Execution: Containers run as user <code>eemt</code> (UID 1000)</li> <li>Minimal Base Images: Only essential packages installed</li> <li>Read-only Root Filesystem: Where applicable</li> <li>Secrets Management: Environment variables for sensitive data</li> </ol> <pre><code># Security configuration in Dockerfile\nRUN useradd -m -s /bin/bash -u 1000 eemt\nUSER eemt\nWORKDIR /home/eemt\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#network-security","level":3,"title":"Network Security","text":"<pre><code># Network isolation configuration\nnetworks:\n  frontend:\n    internal: false\n  backend:\n    internal: true\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#volume-security","level":3,"title":"Volume Security","text":"<ul> <li>Read-only mounts for input data</li> <li>Restricted permissions on result directories</li> <li>Temporary data cleaned after processing</li> </ul>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#build-optimization","level":3,"title":"Build Optimization","text":"<pre><code># Multi-stage build for smaller images\nFROM ubuntu:24.04 as builder\nRUN apt-get update &amp;&amp; apt-get install -y build-essential\n# Build steps...\n\nFROM ubuntu:24.04\nCOPY --from=builder /app /app\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#layer-caching","level":3,"title":"Layer Caching","text":"<pre><code># Leverage build cache\ndocker build --cache-from eemt:ubuntu24.04 -t eemt:ubuntu24.04 .\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#runtime-optimization","level":3,"title":"Runtime Optimization","text":"<ul> <li>Shared memory for inter-process communication</li> <li>Volume caching for frequently accessed data</li> <li>CPU affinity for computational tasks</li> </ul>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#common-issues","level":3,"title":"Common Issues","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-fails-to-start","level":4,"title":"Container Fails to Start","text":"<pre><code># Check logs\ndocker logs eemt-web\n\n# Inspect container\ndocker inspect eemt-web\n\n# Debug interactively\ndocker run -it --entrypoint /bin/bash eemt-web\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#volume-permission-issues","level":4,"title":"Volume Permission Issues","text":"<pre><code># Fix ownership\ndocker exec eemt-web chown -R eemt:eemt /app/data\n\n# Check permissions\ndocker exec eemt-web ls -la /app/data\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#network-connectivity","level":4,"title":"Network Connectivity","text":"<pre><code># Test internal DNS\ndocker exec eemt-worker ping eemt-master\n\n# Check network configuration\ndocker network inspect eemt-network\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#debugging-tools","level":3,"title":"Debugging Tools","text":"<pre><code># Container shell access\ndocker exec -it eemt-web /bin/bash\n\n# Process monitoring\ndocker top eemt-worker\n\n# Resource usage\ndocker stats --no-stream\n\n# Network debugging\ndocker run --rm --network eemt-network nicolaka/netshoot\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#best-practices","level":2,"title":"Best Practices","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#image-management","level":3,"title":"Image Management","text":"<ol> <li>Version Tags: Always use specific version tags</li> <li>Regular Updates: Rebuild images monthly for security patches</li> <li>Image Scanning: Use <code>docker scan</code> for vulnerability detection</li> <li>Registry Usage: Push to registry for distributed deployments</li> </ol>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-operations","level":3,"title":"Container Operations","text":"<ol> <li>Graceful Shutdown: Implement SIGTERM handlers</li> <li>Log Management: Use centralized logging</li> <li>Monitoring: Implement health checks and metrics</li> <li>Backup Strategy: Regular volume backups</li> </ol>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#development-workflow","level":3,"title":"Development Workflow","text":"<ol> <li>Local Development: Use bind mounts for code changes</li> <li>Testing: Separate test containers with isolated data</li> <li>Staging: Mirror production configuration</li> <li>CI/CD Integration: Automated builds and deployments</li> </ol>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#migration-and-upgrades","level":2,"title":"Migration and Upgrades","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-image-updates","level":3,"title":"Container Image Updates","text":"<pre><code># Pull latest images\ndocker pull eemt:ubuntu24.04:latest\n\n# Stop running containers\ndocker-compose down\n\n# Update and restart\ndocker-compose up -d\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#data-migration","level":3,"title":"Data Migration","text":"<pre><code># Backup volumes\ndocker run --rm -v eemt_data:/data -v $(pwd):/backup \\\n  ubuntu tar czf /backup/eemt-backup.tar.gz /data\n\n# Restore volumes\ndocker run --rm -v eemt_data:/data -v $(pwd):/backup \\\n  ubuntu tar xzf /backup/eemt-backup.tar.gz -C /\n</code></pre>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#future-enhancements","level":2,"title":"Future Enhancements","text":"","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#planned-improvements","level":3,"title":"Planned Improvements","text":"<ol> <li>Kubernetes Support: Helm charts for K8s deployment</li> <li>GPU Acceleration: CUDA-enabled containers</li> <li>Service Mesh: Istio/Linkerd integration</li> <li>Observability: Prometheus + Grafana stack</li> <li>Registry Integration: Harbor/Nexus private registry</li> </ol>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#container-roadmap","level":3,"title":"Container Roadmap","text":"<ul> <li>Q1 2025: GPU-accelerated GRASS GIS container</li> <li>Q2 2025: Kubernetes operators for EEMT</li> <li>Q3 2025: Multi-architecture images (ARM64)</li> <li>Q4 2025: Serverless container deployments</li> </ul>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/container-architecture/#related-documentation","level":2,"title":"Related Documentation","text":"<ul> <li>Docker Deployment Guide</li> <li>Distributed Deployment</li> <li>Web Interface Architecture</li> <li>Development Guide</li> </ul>","path":["Infrastructure","Container Architecture"],"tags":[]},{"location":"infrastructure/job-cleanup/","level":1,"title":"Job Data Cleanup System","text":"<p>The EEMT infrastructure includes an automated job data cleanup system designed to manage disk space efficiently while preserving important job metadata for auditing and analysis.</p>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#overview","level":2,"title":"Overview","text":"<p>The cleanup system provides intelligent data lifecycle management with the following key features:</p> <ul> <li>Automated retention policies - Different policies for successful vs failed jobs</li> <li>Selective data deletion - Preserves job configurations while removing output data</li> <li>Configurable scheduling - Cron, systemd, or manual execution options</li> <li>Container integration - Works seamlessly with Docker and Kubernetes deployments</li> <li>API endpoints - RESTful interface for programmatic cleanup control</li> </ul> <p>Quick Start</p> <p>For most deployments, the default configuration works out of the box: <pre><code># Install automated cleanup (runs daily at 2 AM)\ncd web-interface/\n./setup_cleanup_cron.sh --user --method cron\n</code></pre></p>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#system-architecture","level":2,"title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"Cleanup Components\"\n        JCM[JobCleanupManager&lt;br/&gt;Core Python Class]\n        CS[cleanup_jobs.py&lt;br/&gt;CLI Script]\n        API[FastAPI Endpoint&lt;br/&gt;/api/cleanup]\n        CRON[Cron/Systemd&lt;br/&gt;Schedulers]\n    end\n\n    subgraph \"Data Storage\"\n        DB[(SQLite Database&lt;br/&gt;jobs.db)]\n        FS[File System&lt;br/&gt;uploads/ &amp; results/]\n    end\n\n    subgraph \"Cleanup Process\"\n        SCAN[Scan Eligible Jobs]\n        EVAL[Evaluate Retention]\n        DELETE[Delete Data]\n        UPDATE[Update Records]\n    end\n\n    CS --&gt; JCM\n    API --&gt; JCM\n    CRON --&gt; CS\n\n    JCM --&gt; SCAN\n    SCAN --&gt; DB\n    SCAN --&gt; EVAL\n    EVAL --&gt; DELETE\n    DELETE --&gt; FS\n    DELETE --&gt; UPDATE\n    UPDATE --&gt; DB</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#retention-policies","level":2,"title":"Retention Policies","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#default-retention-rules","level":3,"title":"Default Retention Rules","text":"Job Status Data Component Default Retention Action After Retention Successful Output data (<code>results/</code>) 7 days Delete files, preserve job record Successful Input DEM (<code>uploads/</code>) 7 days Delete file if exists Successful Job configuration Forever Preserved in database Successful Execution logs 7 days Delete with output data Failed All data 12 hours Complete deletion Failed Error information Forever Preserved in job record Running All data N/A Never auto-deleted Pending All data N/A Never auto-deleted","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#rationale","level":3,"title":"Rationale","text":"<p>The retention policies are designed based on typical usage patterns:</p> <ul> <li>Successful jobs (7 days): Provides sufficient time for users to download results while managing disk space</li> <li>Failed jobs (12 hours): Quick cleanup since failed job data is rarely needed after debugging</li> <li>Configuration preservation: Maintains audit trail and enables job re-execution if needed</li> <li>Running/Pending protection: Ensures active jobs are never interrupted</li> </ul>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#implementation-details","level":2,"title":"Implementation Details","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#core-components","level":3,"title":"Core Components","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#jobcleanupmanager-class","level":4,"title":"JobCleanupManager Class","text":"<p>The <code>JobCleanupManager</code> class in <code>cleanup_jobs.py</code> provides the core cleanup functionality:</p> <pre><code>class JobCleanupManager:\n    \"\"\"Manages automated cleanup of EEMT job data\"\"\"\n\n    def __init__(self, base_dir: Path, dry_run: bool = False):\n        self.base_dir = base_dir\n        self.dry_run = dry_run\n        self.db_path = base_dir / \"jobs.db\"\n        self.results_dir = base_dir / \"results\"\n        self.uploads_dir = base_dir / \"uploads\"\n\n        # Configurable retention periods\n        self.success_retention_days = int(\n            os.getenv('EEMT_SUCCESS_RETENTION_DAYS', '7')\n        )\n        self.failed_retention_hours = int(\n            os.getenv('EEMT_FAILED_RETENTION_HOURS', '12')\n        )\n</code></pre> <p>Key methods:</p> <ul> <li><code>get_jobs_for_cleanup()</code>: Queries database for eligible jobs based on age and status</li> <li><code>cleanup_job_data()</code>: Performs actual deletion of files and directories</li> <li><code>run_cleanup()</code>: Orchestrates the complete cleanup process</li> <li><code>_get_directory_size_mb()</code>: Calculates space to be freed</li> </ul>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#database-schema","level":4,"title":"Database Schema","text":"<p>The cleanup system adds tracking fields to the jobs table:</p> <pre><code>-- Additional fields for cleanup tracking\nALTER TABLE jobs ADD COLUMN completed_at TIMESTAMP;\nALTER TABLE jobs ADD COLUMN data_cleaned_at TIMESTAMP;\nALTER TABLE jobs ADD COLUMN cleanup_summary TEXT;\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#file-system-operations","level":4,"title":"File System Operations","text":"<p>The cleanup process handles various file types:</p> <ol> <li>Results directory (<code>results/&lt;job_id&gt;/</code>):</li> <li>Contains all workflow outputs (GeoTIFF files, logs, metadata)</li> <li>Deleted recursively for eligible jobs</li> <li> <p>Size calculated before deletion for reporting</p> </li> <li> <p>Upload directory (<code>uploads/&lt;filename&gt;</code>):</p> </li> <li>Contains original DEM files</li> <li>Deleted if associated with cleaned job</li> <li> <p>Filename stored in job record for reference</p> </li> <li> <p>Temporary files (<code>temp/&lt;job_id&gt;/</code>):</p> </li> <li>Working directories for active jobs</li> <li>Cleaned immediately after job completion</li> <li>Not subject to retention policies</li> </ol>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#sql-queries","level":3,"title":"SQL Queries","text":"<p>The system uses optimized queries to identify cleanup candidates:</p> <pre><code>-- Find successful jobs older than retention period\nSELECT id, workflow_type, status, completed_at, dem_filename, parameters\nFROM jobs \nWHERE status = 'completed' \n  AND completed_at IS NOT NULL \n  AND datetime(completed_at) &lt; datetime('now', '-7 days')\nORDER BY completed_at ASC;\n\n-- Find failed jobs older than retention period\nSELECT id, workflow_type, status, completed_at, dem_filename, parameters\nFROM jobs \nWHERE status = 'failed' \n  AND completed_at IS NOT NULL \n  AND datetime(completed_at) &lt; datetime('now', '-12 hours')\nORDER BY completed_at ASC;\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#configuration-options","level":2,"title":"Configuration Options","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#environment-variables","level":3,"title":"Environment Variables","text":"<p>Configure cleanup behavior through environment variables:</p> <pre><code># Retention periods\nexport EEMT_SUCCESS_RETENTION_DAYS=7    # Days to keep successful job data\nexport EEMT_FAILED_RETENTION_HOURS=12   # Hours to keep failed job data\n\n# Operational settings\nexport EEMT_DRY_RUN=true               # Preview mode without deletion\nexport EEMT_CLEANUP_LOG_LEVEL=INFO     # Logging verbosity\nexport EEMT_CLEANUP_BATCH_SIZE=100     # Jobs per cleanup batch\n\n# Directory paths (if non-standard)\nexport EEMT_BASE_DIR=/path/to/web-interface\nexport EEMT_RESULTS_DIR=/path/to/results\nexport EEMT_UPLOADS_DIR=/path/to/uploads\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#command-line-arguments","level":3,"title":"Command-Line Arguments","text":"<p>Override defaults via command-line:</p> <pre><code>python cleanup_jobs.py \\\n    --success-retention-days 14 \\\n    --failed-retention-hours 24 \\\n    --base-dir /custom/path \\\n    --dry-run \\\n    --verbose\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#configuration-file-advanced","level":3,"title":"Configuration File (Advanced)","text":"<p>For complex deployments, use a YAML configuration:</p> <pre><code># cleanup_config.yaml\nretention:\n  successful_jobs:\n    days: 7\n    keep_metadata: true\n    keep_logs: false\n  failed_jobs:\n    hours: 12\n    keep_error_logs: true\n\nperformance:\n  batch_size: 100\n  parallel_delete: true\n  max_workers: 4\n\nnotifications:\n  enabled: true\n  email_on_error: admin@example.com\n  summary_webhook: https://hooks.slack.com/services/XXX\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#scheduling-options","level":2,"title":"Scheduling Options","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#cron-integration","level":3,"title":"Cron Integration","text":"<p>The provided <code>setup_cleanup_cron.sh</code> script automates cron configuration:</p> <pre><code># User-level cron (recommended)\n./setup_cleanup_cron.sh --user --method cron\n\n# System-wide cron (requires sudo)\nsudo ./setup_cleanup_cron.sh --system --method cron\n</code></pre> <p>This creates a cron entry: <pre><code># Daily at 2:00 AM\n0 2 * * * cd /path/to/web-interface &amp;&amp; python3 cleanup_jobs.py &gt;&gt; cleanup_jobs.log 2&gt;&amp;1\n</code></pre></p>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#systemd-timer","level":3,"title":"Systemd Timer","text":"<p>For systemd-based systems:</p> <pre><code># Install systemd timer\n./setup_cleanup_cron.sh --user --method systemd\n\n# Check status\nsystemctl --user status eemt-cleanup.timer\n\n# View logs\njournalctl --user -u eemt-cleanup.service\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#manual-execution","level":3,"title":"Manual Execution","text":"<p>Run cleanup on-demand:</p> <pre><code># Production cleanup\npython cleanup_jobs.py\n\n# Preview mode (no deletion)\npython cleanup_jobs.py --dry-run\n\n# Verbose output\npython cleanup_jobs.py --verbose\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#api-integration","level":2,"title":"API Integration","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#restful-endpoint","level":3,"title":"RESTful Endpoint","text":"<p>The FastAPI application exposes a cleanup endpoint:</p> <pre><code>@app.post(\"/api/cleanup\")\nasync def trigger_cleanup(\n    dry_run: bool = False,\n    success_retention_days: int = 7,\n    failed_retention_hours: int = 12\n):\n    \"\"\"Trigger job data cleanup via API\"\"\"\n\n    cleanup_manager = JobCleanupManager(\n        base_dir=Path(\"./\"),\n        dry_run=dry_run\n    )\n\n    # Override retention if specified\n    if success_retention_days:\n        cleanup_manager.success_retention_days = success_retention_days\n    if failed_retention_hours:\n        cleanup_manager.failed_retention_hours = failed_retention_hours\n\n    # Run cleanup\n    summary = cleanup_manager.run_cleanup()\n\n    return {\n        \"success\": True,\n        \"summary\": summary,\n        \"timestamp\": datetime.now().isoformat()\n    }\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#api-usage-examples","level":3,"title":"API Usage Examples","text":"<pre><code># Trigger cleanup with defaults\ncurl -X POST http://localhost:5000/api/cleanup\n\n# Dry run to preview\ncurl -X POST http://localhost:5000/api/cleanup?dry_run=true\n\n# Custom retention periods\ncurl -X POST http://localhost:5000/api/cleanup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"success_retention_days\": 3,\n    \"failed_retention_hours\": 6\n  }'\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#logging-and-monitoring","level":2,"title":"Logging and Monitoring","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#log-output","level":3,"title":"Log Output","text":"<p>The cleanup system provides detailed logging:</p> <pre><code>2024-01-20 02:00:01 INFO - Cleanup manager initialized:\n2024-01-20 02:00:01 INFO -   Base directory: /home/user/eemt/web-interface\n2024-01-20 02:00:01 INFO -   Success retention: 7 days\n2024-01-20 02:00:01 INFO -   Failed retention: 12 hours\n2024-01-20 02:00:01 INFO - Starting EEMT job data cleanup process\n2024-01-20 02:00:02 INFO - Found 3 successful jobs for data cleanup\n2024-01-20 02:00:02 INFO - Found 2 failed jobs for complete deletion\n2024-01-20 02:00:03 INFO - Processing job job-20240113-123456 (completed, 2024-01-13 14:23:45)\n2024-01-20 02:00:04 INFO - Deleted results directory: results/job-20240113-123456 (15234.5 MB)\n2024-01-20 02:00:04 INFO - Deleted uploaded DEM: uploads/job-20240113-123456_dem.tif (250.3 MB)\n2024-01-20 02:00:05 INFO - === CLEANUP SUMMARY ===\n2024-01-20 02:00:05 INFO - Successful jobs processed: 3\n2024-01-20 02:00:05 INFO - Failed jobs processed: 2\n2024-01-20 02:00:05 INFO - Total disk space freed: 45678.9 MB\n2024-01-20 02:00:05 INFO - Job configs preserved: 3\n2024-01-20 02:00:05 INFO - Job configs deleted: 2\n2024-01-20 02:00:05 INFO - Cleanup completed successfully\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#summary-reports","level":3,"title":"Summary Reports","text":"<p>Each cleanup run generates a JSON summary:</p> <pre><code>{\n  \"start_time\": \"2024-01-20T02:00:01.123456\",\n  \"end_time\": \"2024-01-20T02:00:05.789012\",\n  \"dry_run\": false,\n  \"successful_jobs_processed\": 3,\n  \"failed_jobs_processed\": 2,\n  \"total_size_freed_mb\": 45678.9,\n  \"configs_preserved\": 3,\n  \"configs_deleted\": 2,\n  \"errors\": [],\n  \"job_details\": [\n    {\n      \"job_id\": \"job-20240113-123456\",\n      \"status\": \"completed\",\n      \"completed_at\": \"2024-01-13T14:23:45\",\n      \"data_deleted\": true,\n      \"config_preserved\": true,\n      \"size_freed_mb\": 15484.8,\n      \"files_deleted\": [\"uploads/job-20240113-123456_dem.tif\"],\n      \"directories_deleted\": [\"results/job-20240113-123456\"]\n    }\n  ]\n}\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#monitoring-metrics","level":3,"title":"Monitoring Metrics","text":"<p>Track cleanup effectiveness:</p> <pre><code># Example monitoring query\nSELECT \n    DATE(completed_at) as date,\n    COUNT(*) as jobs_completed,\n    COUNT(CASE WHEN data_cleaned_at IS NOT NULL THEN 1 END) as jobs_cleaned,\n    SUM(CASE WHEN data_cleaned_at IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as cleanup_rate\nFROM jobs\nWHERE completed_at &gt;= date('now', '-30 days')\nGROUP BY DATE(completed_at)\nORDER BY date DESC;\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#docker-integration","level":2,"title":"Docker Integration","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#container-configuration","level":3,"title":"Container Configuration","text":"<p>The cleanup system works seamlessly with Docker:</p> <pre><code># docker-compose.yml\nservices:\n  eemt-web:\n    image: eemt-web:latest\n    environment:\n      - EEMT_SUCCESS_RETENTION_DAYS=7\n      - EEMT_FAILED_RETENTION_HOURS=12\n      - EEMT_ENABLE_AUTO_CLEANUP=true\n    volumes:\n      - ./uploads:/app/uploads\n      - ./results:/app/results\n      - ./jobs.db:/app/jobs.db\n    command: &gt;\n      sh -c \"\n      python app.py &amp;\n      while true; do\n        sleep 86400\n        python cleanup_jobs.py\n      done\n      \"\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#container-execution","level":3,"title":"Container Execution","text":"<p>Run cleanup within containers:</p> <pre><code># Execute in running container\ndocker exec eemt-web python cleanup_jobs.py\n\n# Run in dedicated container\ndocker run --rm \\\n  -v $(pwd)/uploads:/app/uploads \\\n  -v $(pwd)/results:/app/results \\\n  -v $(pwd)/jobs.db:/app/jobs.db \\\n  eemt-web python cleanup_jobs.py --dry-run\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#security-considerations","level":2,"title":"Security Considerations","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#access-control","level":3,"title":"Access Control","text":"<p>The cleanup system implements several security measures:</p> <ol> <li>Path validation: Ensures deletions only occur within designated directories</li> <li>Job ownership verification: Confirms files belong to the job being cleaned</li> <li>Database integrity: Uses transactions to prevent partial updates</li> <li>Audit logging: Tracks all cleanup operations for accountability</li> </ol>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#safe-deletion","level":3,"title":"Safe Deletion","text":"<p>The cleanup process includes safeguards:</p> <pre><code>def cleanup_job_data(self, job: Dict, keep_job_config: bool = True):\n    \"\"\"Clean up job data with safety checks\"\"\"\n\n    # Validate job ID format\n    if not re.match(r'^job-\\d{8}-\\d{6}$', job['id']):\n        raise ValueError(f\"Invalid job ID format: {job['id']}\")\n\n    # Verify paths are within allowed directories\n    results_path = self.results_dir / job['id']\n    if not str(results_path).startswith(str(self.results_dir)):\n        raise ValueError(f\"Results path outside allowed directory\")\n\n    # Perform deletion with error handling\n    try:\n        if results_path.exists():\n            shutil.rmtree(results_path)\n    except Exception as e:\n        logger.error(f\"Failed to delete {results_path}: {e}\")\n        raise\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#common-issues","level":3,"title":"Common Issues","text":"<ol> <li>Permission Denied</li> <li>Check file ownership: <code>ls -la uploads/ results/</code></li> <li> <p>Fix permissions: <code>chmod -R 755 uploads/ results/</code></p> </li> <li> <p>Database Locked</p> </li> <li>Check for concurrent access: <code>fuser jobs.db</code></li> <li> <p>Wait for operations to complete or restart services</p> </li> <li> <p>Disk Space Not Freed</p> </li> <li>Verify deletion: <code>du -sh results/job-*</code></li> <li>Check for open file handles: <code>lsof | grep results</code></li> <li> <p>Force filesystem sync: <code>sync</code></p> </li> <li> <p>Cleanup Not Running</p> </li> <li>Check cron: <code>crontab -l</code> and <code>grep CRON /var/log/syslog</code></li> <li>Check systemd: <code>systemctl --user status eemt-cleanup.timer</code></li> </ol>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#debug-mode","level":3,"title":"Debug Mode","text":"<p>Enable detailed debugging:</p> <pre><code># Maximum verbosity\nexport EEMT_CLEANUP_LOG_LEVEL=DEBUG\npython cleanup_jobs.py --verbose --dry-run\n\n# Debug output includes:\n# - SQL queries executed\n# - Files and directories examined\n# - Size calculations\n# - Decision logic for each job\n</code></pre>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#best-practices","level":2,"title":"Best Practices","text":"<ol> <li>Test with dry-run first: Always preview cleanup actions before production use</li> <li>Monitor disk usage: Set up alerts for high disk usage</li> <li>Backup critical data: Archive important results before retention period expires</li> <li>Regular monitoring: Check cleanup logs and summaries weekly</li> <li>Adjust retention as needed: Modify periods based on usage patterns</li> <li>Document changes: Log any modifications to retention policies</li> </ol>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"infrastructure/job-cleanup/#summary","level":2,"title":"Summary","text":"<p>The EEMT job data cleanup system provides automated, configurable management of job data lifecycle. It balances disk space efficiency with data preservation needs, ensuring system performance while maintaining important job metadata for analysis and auditing. The flexible architecture supports various deployment scenarios from single-node installations to distributed container orchestrations.</p>","path":["Infrastructure","Job Data Cleanup System"],"tags":[]},{"location":"installation/","level":1,"title":"Installation Guide","text":"","path":["Installation"],"tags":[]},{"location":"installation/#overview","level":2,"title":"Overview","text":"<p>The EEMT (Effective Energy and Mass Transfer) suite can be deployed through multiple methods, each suited to different use cases and environments. This guide provides comprehensive installation instructions for all deployment options.</p>","path":["Installation"],"tags":[]},{"location":"installation/#installation-methods","level":2,"title":"Installation Methods","text":"","path":["Installation"],"tags":[]},{"location":"installation/#docker-deployment-recommended","level":3,"title":"üê≥ Docker Deployment (Recommended)","text":"<p>The containerized approach provides the most reliable and reproducible installation method: - Advantages: No dependency conflicts, consistent environment, easy updates - Best for: Most users, production deployments, cloud environments - Requirements: Docker Engine 20.10+ and Docker Compose v2.0+</p>","path":["Installation"],"tags":[]},{"location":"installation/#manual-installation","level":3,"title":"üîß Manual Installation","text":"<p>Direct installation on your system for development or customization: - Advantages: Full control, easier debugging, native performance - Best for: Developers, HPC environments, custom integrations - Requirements: Python 3.12+, GRASS GIS 8.4+, GDAL 3.8+</p>","path":["Installation"],"tags":[]},{"location":"installation/#requirements-dependencies","level":3,"title":"üìã Requirements &amp; Dependencies","text":"<p>Detailed list of all software dependencies and system requirements: - Hardware specifications - Software prerequisites - Python package requirements - Optional components</p>","path":["Installation"],"tags":[]},{"location":"installation/#troubleshooting","level":3,"title":"üîç Troubleshooting","text":"<p>Common installation issues and their solutions: - Docker-specific problems - Dependency conflicts - Permission issues - Platform-specific considerations</p>","path":["Installation"],"tags":[]},{"location":"installation/#quick-start","level":2,"title":"Quick Start","text":"<p>For most users, we recommend the Docker deployment:</p> <pre><code># Clone the repository\ngit clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n\n# Start with Docker Compose\ndocker-compose up\n\n# Access web interface\n# Open browser to http://localhost:5000\n</code></pre>","path":["Installation"],"tags":[]},{"location":"installation/#system-requirements","level":2,"title":"System Requirements","text":"","path":["Installation"],"tags":[]},{"location":"installation/#minimum-hardware","level":3,"title":"Minimum Hardware","text":"<ul> <li>CPU: 4 cores (8+ recommended for parallel processing)</li> <li>RAM: 8 GB (16+ GB recommended for large datasets)</li> <li>Storage: 50 GB free space (more for large study areas)</li> <li>GPU: Optional but recommended for r.sun calculations</li> </ul>","path":["Installation"],"tags":[]},{"location":"installation/#operating-systems","level":3,"title":"Operating Systems","text":"<ul> <li>Linux: Ubuntu 20.04+, CentOS 7+, Debian 10+</li> <li>macOS: 11.0+ (Big Sur or later)</li> <li>Windows: Windows 10+ with WSL2 or Docker Desktop</li> </ul>","path":["Installation"],"tags":[]},{"location":"installation/#verification","level":2,"title":"Verification","text":"<p>After installation, verify your setup:</p> <pre><code># Docker installation\ndocker run --rm eemt:ubuntu24.04 python -c \"import eemt; print('EEMT installed successfully')\"\n\n# Manual installation  \npython -c \"import eemt; print('EEMT installed successfully')\"\ngrass --version\nmakeflow --version\n</code></pre>","path":["Installation"],"tags":[]},{"location":"installation/#next-steps","level":2,"title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Review the Quick Start Guide for your first analysis</li> <li>Explore Example Workflows for real-world applications</li> <li>Check the API Documentation for detailed usage</li> <li>Join our community forum for support</li> </ol>","path":["Installation"],"tags":[]},{"location":"installation/#support","level":2,"title":"Support","text":"<p>If you encounter issues during installation:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Search existing issues</li> <li>Create a new issue with:</li> <li>Your operating system and version</li> <li>Installation method attempted</li> <li>Complete error messages</li> <li>Output of diagnostic commands</li> </ol> <p>For development setup and contribution guidelines, see the Development Guide.</p>","path":["Installation"],"tags":[]},{"location":"installation/docker/","level":1,"title":"Docker Deployment Guide","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#overview","level":2,"title":"Overview","text":"<p>This guide provides comprehensive instructions for deploying EEMT using Docker containers. The containerized deployment offers the most reliable and reproducible way to run EEMT workflows, eliminating complex dependency management and ensuring consistent execution across different systems.</p>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#prerequisites","level":2,"title":"Prerequisites","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#system-requirements","level":3,"title":"System Requirements","text":"<p>Minimum Hardware: - CPU: 4 cores (8+ recommended) - RAM: 8 GB (16+ GB recommended) - Storage: 50 GB free space - Network: Stable internet connection for climate data downloads</p> <p>Software Requirements: - Docker Engine 20.10+ or Docker Desktop - Docker Compose v2.0+ - Git (for cloning repository) - Modern web browser (Chrome, Firefox, Safari, Edge)</p>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#docker-installation","level":3,"title":"Docker Installation","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#linux-ubuntudebian","level":4,"title":"Linux (Ubuntu/Debian)","text":"<pre><code># Update package index\nsudo apt update\n\n# Install prerequisites\nsudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n\n# Add Docker GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add Docker repository\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker\nsudo apt update\nsudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n\n# Add user to docker group (log out and back in after)\nsudo usermod -aG docker $USER\n\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#macos","level":4,"title":"macOS","text":"<pre><code># Install Docker Desktop\n# Download from: https://www.docker.com/products/docker-desktop\n\n# Or use Homebrew\nbrew install --cask docker\n\n# Start Docker Desktop from Applications\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#windows","level":4,"title":"Windows","text":"<pre><code># Install Docker Desktop\n# Download from: https://www.docker.com/products/docker-desktop\n\n# Enable WSL2 backend (recommended)\nwsl --install\n\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#quick-start","level":2,"title":"Quick Start","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#1-clone-repository","level":3,"title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#2-build-containers","level":3,"title":"2. Build Containers","text":"<pre><code># Build all required containers\ndocker compose build\n\n# Or build specific service\ndocker compose build eemt-web\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#3-start-services","level":3,"title":"3. Start Services","text":"<pre><code># Start in foreground (see logs)\ndocker compose up\n\n# Start in background\ndocker compose up -d\n\n# Access web interface\nopen http://localhost:5000  # macOS\nxdg-open http://localhost:5000  # Linux\nstart http://localhost:5000  # Windows\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#4-submit-a-job","level":3,"title":"4. Submit a Job","text":"<ol> <li>Navigate to http://localhost:5000</li> <li>Select workflow type (Solar or EEMT)</li> <li>Upload your DEM file</li> <li>Configure parameters</li> <li>Click \"Submit Job\"</li> <li>Monitor progress at http://localhost:5000/monitor</li> </ol>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#deployment-modes","level":2,"title":"Deployment Modes","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#local-mode-default","level":3,"title":"Local Mode (Default)","text":"<p>Single-container deployment for development and small-scale processing:</p> <pre><code># docker-compose.yml (simplified)\nservices:\n  eemt-web:\n    build:\n      context: .\n      dockerfile: docker/web-interface/Dockerfile\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./data:/app/data\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - EEMT_MODE=local\n</code></pre> <p>Start Command: <pre><code>docker compose up eemt-web\n</code></pre></p> <p>Use Cases: - Development and testing - Small to medium DEM processing - Single-user environments - Educational purposes</p>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#distributed-mode","level":3,"title":"Distributed Mode","text":"<p>Multi-container deployment with master-worker architecture:</p> <pre><code># docker-compose.yml (distributed profile)\nservices:\n  eemt-master:\n    profiles: [distributed]\n    ports:\n      - \"5000:5000\"  # Web interface\n      - \"9123:9123\"  # Work Queue port\n    environment:\n      - EEMT_MODE=master\n      - MAX_WORKERS=10\n\n  eemt-worker:\n    profiles: [distributed]\n    environment:\n      - MASTER_HOST=eemt-master\n      - WORKER_CORES=4\n    deploy:\n      replicas: 5\n</code></pre> <p>Start Command: <pre><code># Start distributed cluster\ndocker compose --profile distributed up\n\n# Scale workers dynamically\ndocker compose --profile distributed up --scale eemt-worker=10\n</code></pre></p> <p>Use Cases: - Large-scale processing - Multi-user environments - Production deployments - HPC integration</p>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#documentation-mode","level":3,"title":"Documentation Mode","text":"<p>Serve documentation alongside the application:</p> <pre><code># Start with documentation\ndocker compose --profile docs up\n\n# Access documentation at http://localhost:8000\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#configuration","level":2,"title":"Configuration","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#environment-variables","level":3,"title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Application Configuration\nEEMT_HOST=0.0.0.0\nEEMT_PORT=5000\nEEMT_MODE=local\n\n# Resource Limits\nCONTAINER_CPU_LIMIT=4\nCONTAINER_MEMORY_LIMIT=8G\nCONTAINER_DISK_LIMIT=50G\n\n# Directory Configuration\nEEMT_UPLOAD_DIR=./data/uploads\nEEMT_RESULTS_DIR=./data/results\nEEMT_TEMP_DIR=./data/temp\nEEMT_CACHE_DIR=./data/cache\n\n# Distributed Mode (optional)\nWORK_QUEUE_PORT=9123\nWORK_QUEUE_PROJECT=EEMT-Production\nMAX_WORKERS=20\n\n# Worker Configuration (optional)\nMASTER_HOST=eemt-master\nMASTER_PORT=9123\nWORKER_CORES=8\nWORKER_MEMORY=16G\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#docker-compose-override","level":3,"title":"Docker Compose Override","text":"<p>Create <code>docker-compose.override.yml</code> for local customizations:</p> <pre><code>version: '3.8'\n\nservices:\n  eemt-web:\n    environment:\n      - DEBUG=true\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./custom-data:/app/custom-data\n    ports:\n      - \"8080:5000\"  # Use different port\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#volume-configuration","level":3,"title":"Volume Configuration","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#persistent-data-volumes","level":4,"title":"Persistent Data Volumes","text":"<pre><code>volumes:\n  # Named volumes for persistence\n  eemt-uploads:\n    driver: local\n  eemt-results:\n    driver: local\n  eemt-cache:\n    driver: local\n\nservices:\n  eemt-web:\n    volumes:\n      - eemt-uploads:/app/uploads\n      - eemt-results:/app/results\n      - eemt-cache:/app/cache\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#bind-mounts-for-development","level":4,"title":"Bind Mounts for Development","text":"<pre><code>services:\n  eemt-web:\n    volumes:\n      # Mount source code for live updates\n      - ./web-interface:/app/web-interface\n      - ./sol:/opt/eemt/sol\n      - ./eemt:/opt/eemt/eemt\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#advanced-configuration","level":2,"title":"Advanced Configuration","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#resource-management","level":3,"title":"Resource Management","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#cpu-and-memory-limits","level":4,"title":"CPU and Memory Limits","text":"<pre><code>services:\n  eemt-worker:\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 8G\n        reservations:\n          cpus: '2.0'\n          memory: 4G\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#gpu-support-future","level":4,"title":"GPU Support (Future)","text":"<pre><code>services:\n  eemt-gpu-worker:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#network-configuration","level":3,"title":"Network Configuration","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#custom-network","level":4,"title":"Custom Network","text":"<pre><code>networks:\n  eemt-network:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.20.0.0/16\n          gateway: 172.20.0.1\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#external-network-access","level":4,"title":"External Network Access","text":"<pre><code>services:\n  eemt-web:\n    networks:\n      - eemt-network\n      - external-network\n\nnetworks:\n  external-network:\n    external: true\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#security-configuration","level":3,"title":"Security Configuration","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#read-only-root-filesystem","level":4,"title":"Read-Only Root Filesystem","text":"<pre><code>services:\n  eemt-worker:\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /run\n    volumes:\n      - ./data:/data:ro\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#secrets-management","level":4,"title":"Secrets Management","text":"<pre><code>secrets:\n  db_password:\n    file: ./secrets/db_password.txt\n\nservices:\n  eemt-web:\n    secrets:\n      - db_password\n    environment:\n      - DB_PASSWORD_FILE=/run/secrets/db_password\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#monitoring-and-logging","level":2,"title":"Monitoring and Logging","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#view-logs","level":3,"title":"View Logs","text":"<pre><code># All services\ndocker compose logs\n\n# Specific service\ndocker compose logs eemt-web\n\n# Follow logs in real-time\ndocker compose logs -f\n\n# Last 100 lines\ndocker compose logs --tail=100\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#container-statistics","level":3,"title":"Container Statistics","text":"<pre><code># Resource usage\ndocker stats\n\n# Specific containers\ndocker stats eemt-web eemt-worker\n\n# Format output\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#health-checks","level":3,"title":"Health Checks","text":"<pre><code>services:\n  eemt-web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#backup-and-recovery","level":2,"title":"Backup and Recovery","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#backup-data-volumes","level":3,"title":"Backup Data Volumes","text":"<pre><code># Stop containers\ndocker compose down\n\n# Backup uploads directory\ndocker run --rm -v eemt_data-uploads:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar czf /backup/uploads-backup.tar.gz /data\n\n# Backup results directory  \ndocker run --rm -v eemt_data-results:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar czf /backup/results-backup.tar.gz /data\n\n# Backup database\ndocker run --rm -v eemt_data:/data -v $(pwd):/backup \\\n  ubuntu:24.04 cp /data/jobs.db /backup/jobs-backup.db\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#restore-data-volumes","level":3,"title":"Restore Data Volumes","text":"<pre><code># Restore uploads\ndocker run --rm -v eemt_data-uploads:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar xzf /backup/uploads-backup.tar.gz -C /\n\n# Restore results\ndocker run --rm -v eemt_data-results:/data -v $(pwd):/backup \\\n  ubuntu:24.04 tar xzf /backup/results-backup.tar.gz -C /\n\n# Restore database\ndocker run --rm -v eemt_data:/data -v $(pwd):/backup \\\n  ubuntu:24.04 cp /backup/jobs-backup.db /data/jobs.db\n\n# Restart services\ndocker compose up -d\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#common-issues","level":3,"title":"Common Issues","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#port-already-in-use","level":4,"title":"Port Already in Use","text":"<pre><code># Check what's using port 5000\nlsof -i :5000  # macOS/Linux\nnetstat -ano | findstr :5000  # Windows\n\n# Use different port\ndocker compose up -e EEMT_PORT=8080\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#docker-daemon-not-running","level":4,"title":"Docker Daemon Not Running","text":"<pre><code># Linux\nsudo systemctl start docker\n\n# macOS/Windows\n# Start Docker Desktop application\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#container-wont-start","level":4,"title":"Container Won't Start","text":"<pre><code># Check logs\ndocker compose logs eemt-web\n\n# Inspect container\ndocker inspect eemt-web\n\n# Debug interactively\ndocker compose run --entrypoint /bin/bash eemt-web\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#permission-denied-errors","level":4,"title":"Permission Denied Errors","text":"<pre><code># Fix Docker socket permissions (Linux)\nsudo chmod 666 /var/run/docker.sock\n\n# Fix volume permissions\ndocker compose exec eemt-web chown -R eemt:eemt /app/data\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#out-of-disk-space","level":4,"title":"Out of Disk Space","text":"<pre><code># Check disk usage\ndf -h\n\n# Clean up Docker\ndocker system prune -a\ndocker volume prune\ndocker image prune\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#debugging","level":3,"title":"Debugging","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#interactive-shell-access","level":4,"title":"Interactive Shell Access","text":"<pre><code># Access running container\ndocker compose exec eemt-web /bin/bash\n\n# Start new container with shell\ndocker compose run --rm eemt-web /bin/bash\n\n# Override entrypoint\ndocker compose run --entrypoint /bin/bash eemt-web\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#network-debugging","level":4,"title":"Network Debugging","text":"<pre><code># Test connectivity between containers\ndocker compose exec eemt-worker ping eemt-master\n\n# Inspect network\ndocker network inspect eemt_eemt-network\n\n# Use network debugging container\ndocker run --rm -it --network eemt_eemt-network nicolaka/netshoot\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#performance-optimization","level":2,"title":"Performance Optimization","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#build-optimization","level":3,"title":"Build Optimization","text":"<pre><code># Use BuildKit for faster builds\nDOCKER_BUILDKIT=1 docker compose build\n\n# Parallel builds\ndocker compose build --parallel\n\n# Use cache\ndocker compose build --no-cache=false\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#runtime-optimization","level":3,"title":"Runtime Optimization","text":"<pre><code>services:\n  eemt-web:\n    # Enable shared memory\n    shm_size: '2gb'\n\n    # Optimize logging\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#storage-optimization","level":3,"title":"Storage Optimization","text":"<pre><code># Use tmpfs for temporary data\ndocker compose run --tmpfs /tmp:size=2G eemt-worker\n\n# Enable compression\ndocker save eemt:ubuntu24.04 | gzip &gt; eemt.tar.gz\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#production-deployment","level":2,"title":"Production Deployment","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#ssltls-configuration","level":3,"title":"SSL/TLS Configuration","text":"<pre><code>services:\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./certs:/etc/nginx/certs\n    depends_on:\n      - eemt-web\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#docker-swarm-deployment","level":3,"title":"Docker Swarm Deployment","text":"<pre><code># Initialize swarm\ndocker swarm init\n\n# Deploy stack\ndocker stack deploy -c docker-compose.yml eemt\n\n# Scale service\ndocker service scale eemt_eemt-worker=10\n\n# Monitor services\ndocker service ls\ndocker service ps eemt_eemt-web\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#kubernetes-deployment","level":3,"title":"Kubernetes Deployment","text":"<pre><code># Convert docker-compose to Kubernetes\nkompose convert\n\n# Deploy to Kubernetes\nkubectl apply -f eemt-deployment.yaml\nkubectl apply -f eemt-service.yaml\n\n# Check deployment\nkubectl get pods\nkubectl get services\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#maintenance","level":2,"title":"Maintenance","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#update-containers","level":3,"title":"Update Containers","text":"<pre><code># Pull latest images\ndocker compose pull\n\n# Rebuild containers\ndocker compose build --pull\n\n# Restart with new images\ndocker compose up -d\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#clean-up","level":3,"title":"Clean Up","text":"<pre><code># Stop and remove containers\ndocker compose down\n\n# Remove volumes (WARNING: deletes data)\ndocker compose down -v\n\n# Remove everything including images\ndocker compose down --rmi all -v\n\n# System-wide cleanup\ndocker system prune -a --volumes\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#log-rotation","level":3,"title":"Log Rotation","text":"<pre><code>services:\n  eemt-web:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"100m\"\n        max-file: \"10\"\n        compress: \"true\"\n</code></pre>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#best-practices","level":2,"title":"Best Practices","text":"","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#development","level":3,"title":"Development","text":"<ol> <li>Use <code>.env</code> files for configuration</li> <li>Mount source code as volumes for hot-reloading</li> <li>Use override files for local settings</li> <li>Keep images small with multi-stage builds</li> </ol>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#production","level":3,"title":"Production","text":"<ol> <li>Use specific image tags (not <code>latest</code>)</li> <li>Implement health checks</li> <li>Set resource limits</li> <li>Use secrets for sensitive data</li> <li>Enable log rotation</li> <li>Regular backups</li> <li>Monitor resource usage</li> </ol>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#security","level":3,"title":"Security","text":"<ol> <li>Run containers as non-root user</li> <li>Use read-only filesystems where possible</li> <li>Limit network exposure</li> <li>Scan images for vulnerabilities</li> <li>Keep base images updated</li> </ol>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/docker/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Web Interface Guide - Learn to use the web interface</li> <li>API Reference - Integrate with the REST API</li> <li>Distributed Deployment - Scale across multiple nodes</li> <li>Container Architecture - Understand the container design</li> </ul>","path":["Installation","Docker Deployment Guide"],"tags":[]},{"location":"installation/manual/","level":1,"title":"Manual Installation Guide","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#overview","level":2,"title":"Overview","text":"<p>This guide provides instructions for manually installing EEMT and its dependencies directly on your system. Manual installation offers full control over the environment and is suitable for development, customization, or integration with existing HPC systems.</p>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#prerequisites","level":2,"title":"Prerequisites","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#system-requirements","level":3,"title":"System Requirements","text":"<p>Minimum Hardware: - CPU: 4 cores (8+ recommended for parallel processing) - RAM: 8 GB (16+ GB recommended for large datasets) - Storage: 50 GB free space - GPU: Optional but recommended for r.sun calculations</p> <p>Operating Systems: - Linux: Ubuntu 20.04+, CentOS 7+, Debian 10+ - macOS: 11.0+ (Big Sur or later) - Windows: Windows 10+ with WSL2</p>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#core-dependencies","level":2,"title":"Core Dependencies","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#1-python-environment","level":3,"title":"1. Python Environment","text":"<p>EEMT requires Python 3.12 or later:</p> <pre><code># Check Python version\npython3 --version\n\n# Ubuntu/Debian\nsudo apt update\nsudo apt install python3.12 python3.12-venv python3.12-dev\n\n# macOS (via Homebrew)\nbrew install python@3.12\n\n# CentOS/RHEL\nsudo yum install python3.12 python3.12-devel\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#2-grass-gis-installation","level":3,"title":"2. GRASS GIS Installation","text":"<p>GRASS GIS 8.4+ is required for geospatial processing:</p>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#ubuntudebian","level":4,"title":"Ubuntu/Debian","text":"<pre><code># Add GRASS GIS repository\nsudo add-apt-repository ppa:ubuntugis/ppa\nsudo apt update\n\n# Install GRASS GIS\nsudo apt install grass grass-dev grass-doc\n\n# Verify installation\ngrass --version\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#macos","level":4,"title":"macOS","text":"<pre><code># Install via Homebrew\nbrew tap OSGeo/osgeo4mac\nbrew install grass\n\n# Or download from official site\n# https://grass.osgeo.org/download/mac/\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#centosrhel","level":4,"title":"CentOS/RHEL","text":"<pre><code># Enable EPEL repository\nsudo yum install epel-release\n\n# Install GRASS GIS\nsudo yum install grass grass-libs grass-devel\n\n# Verify installation\ngrass --version\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#3-gdal-installation","level":3,"title":"3. GDAL Installation","text":"<p>GDAL 3.8+ is required for raster data handling:</p> <pre><code># Ubuntu/Debian\nsudo apt install gdal-bin python3-gdal libgdal-dev\n\n# macOS\nbrew install gdal\n\n# CentOS/RHEL  \nsudo yum install gdal gdal-devel gdal-python\n\n# Verify installation\ngdalinfo --version\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#4-cctools-installation-makeflow-work-queue","level":3,"title":"4. CCTools Installation (Makeflow + Work Queue)","text":"<p>CCTools provides workflow management capabilities:</p> <pre><code># Download latest CCTools\nwget https://github.com/cooperative-computing-lab/cctools/releases/download/release/7.8.2/cctools-7.8.2-source.tar.gz\ntar -xzf cctools-7.8.2-source.tar.gz\ncd cctools-7.8.2-source\n\n# Configure and compile\n./configure --prefix=$HOME/cctools\nmake\nmake install\n\n# Add to PATH\necho 'export PATH=$HOME/cctools/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verify installation\nmakeflow --version\nwork_queue_status --version\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#eemt-installation","level":2,"title":"EEMT Installation","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#1-clone-repository","level":3,"title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#2-create-python-virtual-environment","level":3,"title":"2. Create Python Virtual Environment","text":"<pre><code># Create virtual environment\npython3 -m venv eemt-env\n\n# Activate environment\nsource eemt-env/bin/activate  # Linux/macOS\n# eemt-env\\Scripts\\activate   # Windows\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#3-install-python-dependencies","level":3,"title":"3. Install Python Dependencies","text":"<pre><code># Install required packages\npip install -r requirements.txt\n\n# Core dependencies include:\n# numpy&gt;=1.26\n# pandas&gt;=2.1\n# xarray&gt;=2024.1\n# rasterio&gt;=1.3\n# geopandas&gt;=0.14\n# dask&gt;=2024.1\n# scipy&gt;=1.11\n# matplotlib&gt;=3.8\n# requests&gt;=2.31\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#4-install-eemt-package","level":3,"title":"4. Install EEMT Package","text":"<pre><code># Install in development mode\npip install -e .\n\n# Or for production installation\npip install .\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#5-configure-cctools-password","level":3,"title":"5. Configure CCTools Password","text":"<pre><code># Create password file for Makeflow authentication\necho \"your_secure_password\" &gt; ~/.eemt-makeflow-password\nchmod 600 ~/.eemt-makeflow-password\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#verification","level":2,"title":"Verification","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#test-core-components","level":3,"title":"Test Core Components","text":"<pre><code># Test Python installation\npython -c \"import numpy, pandas, rasterio, xarray; print('Python packages OK')\"\n\n# Test GRASS GIS\ngrass --exec r.info --help\n\n# Test GDAL\ngdalinfo --formats | grep -i \"GTiff\"\n\n# Test CCTools\nmakeflow --version\nwork_queue_worker --version\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#run-test-workflow","level":3,"title":"Run Test Workflow","text":"<pre><code># Navigate to solar workflow directory\ncd sol/sol/\n\n# Run test with example DEM\npython run-workflow --step 15 --num_threads 2 ../examples/mcn_10m.tif\n\n# Check output\nls sol_data/global/daily/total_sun_day_*.tif | wc -l  # Should be 365\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#platform-specific-notes","level":2,"title":"Platform-Specific Notes","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#ubuntudebian_1","level":3,"title":"Ubuntu/Debian","text":"<p>Additional packages for optimal performance:</p> <pre><code>sudo apt install \\\n  build-essential \\\n  libproj-dev \\\n  libgeos-dev \\\n  libspatialindex-dev \\\n  libnetcdf-dev \\\n  libhdf5-dev\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#macos_1","level":3,"title":"macOS","text":"<p>Ensure Xcode Command Line Tools are installed:</p> <pre><code>xcode-select --install\n</code></pre> <p>For M1/M2 Macs, use arch-specific builds:</p> <pre><code># Install Rosetta 2 if needed\nsoftwareupdate --install-rosetta\n\n# Use arch flag for compilation\narch -arm64 make\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#centosrhel_1","level":3,"title":"CentOS/RHEL","text":"<p>Enable additional repositories:</p> <pre><code># Enable PowerTools/CodeReady\nsudo yum config-manager --set-enabled powertools  # CentOS 8\n# or\nsudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms  # RHEL 8\n\n# Install development tools\nsudo yum groupinstall \"Development Tools\"\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#windows-wsl2","level":3,"title":"Windows (WSL2)","text":"<p>Install WSL2 and use Ubuntu distribution:</p> <pre><code># Install WSL2\nwsl --install\n\n# Set WSL2 as default\nwsl --set-default-version 2\n\n# Install Ubuntu\nwsl --install -d Ubuntu-22.04\n\n# Follow Ubuntu installation instructions within WSL2\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#environment-variables","level":2,"title":"Environment Variables","text":"<p>Set required environment variables:</p> <pre><code># Add to ~/.bashrc or ~/.zshrc\nexport EEMT_HOME=/path/to/eemt\nexport GRASSBIN=$(which grass)\nexport GISBASE=$(grass --config path)\nexport PATH=$EEMT_HOME/bin:$PATH\nexport PYTHONPATH=$EEMT_HOME:$PYTHONPATH\n\n# Optional performance tuning\nexport GRASS_NPROCS=8  # Number of parallel processes\nexport OMP_NUM_THREADS=4  # OpenMP threads\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#common-issues","level":3,"title":"Common Issues","text":"","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#grass-gis-not-found","level":4,"title":"GRASS GIS Not Found","text":"<pre><code># Check GRASS installation\nwhich grass\ngrass --config path\n\n# Set GISBASE manually if needed\nexport GISBASE=/usr/lib/grass84\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#python-import-errors","level":4,"title":"Python Import Errors","text":"<pre><code># Ensure virtual environment is activated\nwhich python  # Should show path within eemt-env\n\n# Reinstall dependencies\npip install --force-reinstall -r requirements.txt\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#gdal-version-conflicts","level":4,"title":"GDAL Version Conflicts","text":"<pre><code># Check GDAL versions\ngdalinfo --version\npython -c \"from osgeo import gdal; print(gdal.__version__)\"\n\n# Ensure versions match\npip install GDAL==$(gdal-config --version)\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#permission-denied-errors","level":4,"title":"Permission Denied Errors","text":"<pre><code># Check file permissions\nls -la ~/.eemt-makeflow-password\n\n# Fix permissions\nchmod 600 ~/.eemt-makeflow-password\nchmod +x sol/sol/run-workflow\nchmod +x eemt/eemt/run-workflow\n</code></pre>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#getting-help","level":3,"title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the FAQ</li> <li>Search GitHub Issues</li> <li>Post on Discussions</li> <li>Create a new issue with:</li> <li>System information (OS, versions)</li> <li>Complete error messages</li> <li>Steps to reproduce</li> </ol>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/manual/#next-steps","level":2,"title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Review Workflow Documentation</li> <li>Try Example Datasets</li> <li>Explore API Reference</li> <li>Configure for Distributed Computing</li> </ol> <p>For containerized deployment (recommended), see the Docker Installation Guide.</p>","path":["Installation","Manual Installation"],"tags":[]},{"location":"installation/requirements/","level":1,"title":"Requirements &amp; Dependencies","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#overview","level":2,"title":"Overview","text":"<p>This document provides comprehensive details about all hardware requirements, software dependencies, and optional components for running EEMT workflows at various scales.</p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#hardware-requirements","level":2,"title":"Hardware Requirements","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#minimum-specifications","level":3,"title":"Minimum Specifications","text":"Component Minimum Recommended Notes CPU 4 cores 8-16 cores More cores enable parallel processing RAM 8 GB 16-32 GB Scales with DEM resolution Storage 50 GB 200+ GB Depends on study area size GPU Optional NVIDIA with 4+ GB VRAM Accelerates r.sun calculations Network 10 Mbps 100+ Mbps For climate data downloads","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#performance-scaling","level":3,"title":"Performance Scaling","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#by-dataset-size","level":4,"title":"By Dataset Size","text":"<ul> <li>Small (&lt; 100 km¬≤): 4 cores, 8 GB RAM</li> <li>Medium (100-1000 km¬≤): 8 cores, 16 GB RAM</li> <li>Large (1000-10000 km¬≤): 16 cores, 32 GB RAM</li> <li>Continental (&gt; 10000 km¬≤): 32+ cores, 64+ GB RAM, distributed computing</li> </ul>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#by-resolution","level":4,"title":"By Resolution","text":"<ul> <li>30m DEM: Base requirements</li> <li>10m DEM: 2x memory requirement</li> <li>1m LiDAR: 10x memory requirement, tiling recommended</li> </ul>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#software-dependencies","level":2,"title":"Software Dependencies","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#core-geospatial-stack","level":3,"title":"Core Geospatial Stack","text":"Software Minimum Version Recommended Purpose GRASS GIS 8.0 8.4+ Geospatial processing engine GDAL 3.0 3.8+ Raster data I/O PROJ 7.0 9.0+ Coordinate transformations GEOS 3.8 3.12+ Geometric operations","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#python-environment","level":3,"title":"Python Environment","text":"Package Minimum Version Purpose Python 3.10 3.12+ recommended numpy 1.22 Numerical arrays pandas 1.5 Data manipulation xarray 2023.1 NetCDF/climate data rasterio 1.3 Raster I/O geopandas 0.12 Vector data dask 2023.1 Parallel computing scipy 1.10 Scientific computing matplotlib 3.6 Visualization requests 2.28 HTTP/API access","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#workflow-management","level":3,"title":"Workflow Management","text":"Software Version Purpose Required CCTools 7.8+ Makeflow + Work Queue Yes Docker 20.10+ Container runtime Recommended Docker Compose 2.0+ Multi-container orchestration Recommended Nextflow 23.10+ Alternative workflow engine Optional","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#web-interface-dependencies","level":3,"title":"Web Interface Dependencies","text":"Package Version Purpose FastAPI 0.104+ REST API framework Uvicorn 0.24+ ASGI server Pydantic 2.5+ Data validation Jinja2 3.1+ Template engine python-multipart 0.0.6+ File uploads aiofiles 23.2+ Async file operations","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#operating-system-support","level":2,"title":"Operating System Support","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#linux-primary-platform","level":3,"title":"Linux (Primary Platform)","text":"<p>Supported Distributions: - Ubuntu 20.04 LTS, 22.04 LTS, 24.04 LTS - Debian 10 (Buster), 11 (Bullseye), 12 (Bookworm) - CentOS 7, CentOS Stream 8/9 - RHEL 7, 8, 9 - Rocky Linux 8, 9 - AlmaLinux 8, 9 - Fedora 36+</p> <p>Package Managers: <pre><code># APT (Ubuntu/Debian)\nsudo apt update &amp;&amp; sudo apt install [packages]\n\n# YUM/DNF (RHEL/CentOS/Fedora)\nsudo yum install [packages]  # or\nsudo dnf install [packages]\n</code></pre></p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#macos","level":3,"title":"macOS","text":"<p>Supported Versions: - macOS 11 (Big Sur) - macOS 12 (Monterey) - macOS 13 (Ventura) - macOS 14 (Sonoma)</p> <p>Architecture Support: - Intel x86_64 - Apple Silicon (M1/M2/M3) with Rosetta 2</p> <p>Package Manager: <pre><code># Homebrew installation\nbrew install grass gdal python@3.12\n</code></pre></p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#windows","level":3,"title":"Windows","text":"<p>Supported Versions: - Windows 10 (version 2004+) - Windows 11 - Windows Server 2019, 2022</p> <p>Installation Methods: 1. WSL2 (Recommended):    <pre><code>wsl --install\n# Then follow Linux instructions\n</code></pre></p> <ol> <li>Docker Desktop:</li> <li>Native Windows containers</li> <li> <p>WSL2 backend for Linux containers</p> </li> <li> <p>Native (Limited support):</p> </li> <li>OSGeo4W installer</li> <li>Conda/Mamba environments</li> </ol>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#optional-components","level":2,"title":"Optional Components","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#performance-enhancements","level":3,"title":"Performance Enhancements","text":"Component Purpose Impact NVIDIA CUDA GPU acceleration for r.sun 10-50x speedup Intel MKL Optimized linear algebra 2-5x speedup OpenMP Multi-threading support Scales with cores HDF5 Efficient data storage Reduced I/O overhead NetCDF Climate data format Direct DAYMET access","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#development-tools","level":3,"title":"Development Tools","text":"Tool Purpose Required For Git Version control Development Make Build automation Compiling from source GCC/Clang C/C++ compiler Building extensions pytest Testing framework Running tests mkdocs Documentation Building docs Jupyter Interactive notebooks Data exploration","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#monitoring-debugging","level":3,"title":"Monitoring &amp; Debugging","text":"Tool Purpose htop Process monitoring nvidia-smi GPU monitoring gdalinfo Raster metadata inspection grass --exec GRASS command testing docker stats Container resource usage","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#container-requirements","level":2,"title":"Container Requirements","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#docker-installation","level":3,"title":"Docker Installation","text":"<p>Minimum Docker Version: 20.10+</p> <pre><code># Check Docker version\ndocker --version\n\n# Required features\n- Buildkit support\n- Multi-stage builds\n- Volume mounts\n- Network creation\n</code></pre>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#container-resources","level":3,"title":"Container Resources","text":"<p>Default Limits: <pre><code># docker-compose.yml\nservices:\n  eemt-web:\n    deploy:\n      resources:\n        limits:\n          cpus: '4'\n          memory: 8G\n        reservations:\n          cpus: '2'\n          memory: 4G\n</code></pre></p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#storage-requirements","level":3,"title":"Storage Requirements","text":"<p>Container Images: - Base Ubuntu: ~500 MB - EEMT with dependencies: ~2.5 GB - With cached climate data: ~10 GB</p> <p>Volume Mounts: - Input data: <code>/data/input</code> - Results: <code>/data/output</code> - Cache: <code>/data/cache</code></p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#network-requirements","level":2,"title":"Network Requirements","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#internet-connectivity","level":3,"title":"Internet Connectivity","text":"<p>Required for: - Climate data downloads (DAYMET) - Container image pulls - Package installations - Updates</p> <p>Bandwidth Requirements: - Minimum: 10 Mbps for basic operations - Recommended: 100+ Mbps for large datasets - DAYMET downloads: ~1 GB per year of data</p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#firewall-ports","level":3,"title":"Firewall Ports","text":"Port Service Protocol Direction 5000 Web Interface TCP Inbound 8000 Documentation TCP Inbound 9123 Work Queue Master TCP Inbound 9124-9200 Work Queue Workers TCP Bidirectional","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#cloud-platform-requirements","level":2,"title":"Cloud Platform Requirements","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#aws","level":3,"title":"AWS","text":"<p>EC2 Instance Types: - Development: t3.large (2 vCPU, 8 GB) - Production: c5.4xlarge (16 vCPU, 32 GB) - GPU-enabled: p3.2xlarge (8 vCPU, 61 GB, V100 GPU)</p> <p>Storage: - EBS: gp3 volumes, 100+ GB - S3: For input/output data</p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#google-cloud","level":3,"title":"Google Cloud","text":"<p>Compute Engine: - Development: e2-standard-4 - Production: c2-standard-16 - GPU-enabled: n1-standard-8 with T4 GPU</p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#azure","level":3,"title":"Azure","text":"<p>Virtual Machines: - Development: Standard_D4s_v3 - Production: Standard_F16s_v2 - GPU-enabled: Standard_NC6s_v3</p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#hpc-systems","level":3,"title":"HPC Systems","text":"<p>SLURM Requirements: <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks=16\n#SBATCH --mem=32GB\n#SBATCH --time=24:00:00\n#SBATCH --partition=standard\n</code></pre></p> <p>Modules: <pre><code>module load grass/8.4\nmodule load gdal/3.8\nmodule load python/3.12\n</code></pre></p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#verification-commands","level":2,"title":"Verification Commands","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#check-all-dependencies","level":3,"title":"Check All Dependencies","text":"<pre><code>#!/bin/bash\n# Save as check_requirements.sh\n\necho \"Checking EEMT Requirements...\"\necho \"==============================\"\n\n# Python\necho -n \"Python: \"\npython3 --version 2&gt;/dev/null || echo \"NOT FOUND\"\n\n# GRASS GIS\necho -n \"GRASS GIS: \"\ngrass --version 2&gt;/dev/null | head -1 || echo \"NOT FOUND\"\n\n# GDAL\necho -n \"GDAL: \"\ngdalinfo --version 2&gt;/dev/null || echo \"NOT FOUND\"\n\n# Docker\necho -n \"Docker: \"\ndocker --version 2&gt;/dev/null || echo \"NOT FOUND\"\n\n# CCTools\necho -n \"Makeflow: \"\nmakeflow --version 2&gt;/dev/null | head -1 || echo \"NOT FOUND\"\n\n# Python packages\necho -e \"\\nPython Packages:\"\npython3 -c \"\nimport importlib\npackages = ['numpy', 'pandas', 'rasterio', 'xarray', 'geopandas']\nfor pkg in packages:\n    try:\n        mod = importlib.import_module(pkg)\n        print(f'  {pkg}: {mod.__version__}')\n    except ImportError:\n        print(f'  {pkg}: NOT INSTALLED')\n\" 2&gt;/dev/null\n\n# System resources\necho -e \"\\nSystem Resources:\"\necho \"  CPU Cores: $(nproc 2&gt;/dev/null || sysctl -n hw.ncpu 2&gt;/dev/null || echo 'Unknown')\"\necho \"  Total RAM: $(free -h 2&gt;/dev/null | awk '/^Mem:/{print $2}' || echo 'Unknown')\"\necho \"  Available Disk: $(df -h . | awk 'NR==2{print $4}')\"\n</code></pre>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#troubleshooting-dependencies","level":2,"title":"Troubleshooting Dependencies","text":"","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#version-conflicts","level":3,"title":"Version Conflicts","text":"<pre><code># Check for conflicts\npip check\n\n# Force reinstall with specific versions\npip install --force-reinstall numpy==1.26.0 pandas==2.1.0\n</code></pre>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#missing-libraries","level":3,"title":"Missing Libraries","text":"<pre><code># Find missing libraries (Linux)\nldd $(which grass) | grep \"not found\"\n\n# Install missing libraries\nsudo apt install libgdal-dev libproj-dev  # Ubuntu/Debian\nsudo yum install gdal-devel proj-devel     # CentOS/RHEL\n</code></pre>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/requirements/#permission-issues","level":3,"title":"Permission Issues","text":"<pre><code># Fix permission problems\nsudo chown -R $USER:$USER ~/.grass8\nchmod 755 ~/grassdata\nchmod 600 ~/.eemt-makeflow-password\n</code></pre> <p>For installation instructions, see the Installation Overview.</p>","path":["Installation","Requirements & Dependencies"],"tags":[]},{"location":"installation/troubleshooting/","level":1,"title":"Installation Troubleshooting Guide","text":"<p>Updated January 2025 - Version 2.0.0</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#overview","level":2,"title":"Overview","text":"<p>This guide helps resolve common installation issues for both Docker and manual installations of EEMT. Each section includes symptoms, diagnosis steps, and solutions. Many critical issues have been fixed in version 2.0.0.</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#recently-fixed-issues-v200","level":2,"title":"üéâ Recently Fixed Issues (v2.0.0)","text":"<p>The following issues have been resolved and should no longer occur:</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#web-interface-workflow-submission","level":3,"title":"Web Interface Workflow Submission","text":"<p>Previous Issue: JSON parsing errors, container preparation hanging at 25% Status: ‚úÖ FIXED Solution Implemented: Enhanced error handling with proper content-type checking</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#system-resource-detection","level":3,"title":"System Resource Detection","text":"<p>Previous Issue: Displayed \"unknown (subprocess mode)\" instead of actual resources Status: ‚úÖ FIXED Solution Implemented: Added psutil-based CPU and memory detection</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#job-monitoring","level":3,"title":"Job Monitoring","text":"<p>Previous Issue: Jobs not appearing, progress bars stuck Status: ‚úÖ FIXED Solution Implemented: Enhanced progress parsing and job persistence</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#system-status-updates","level":3,"title":"System Status Updates","text":"<p>Previous Issue: Timestamp stuck at \"Updating...\" Status: ‚úÖ FIXED Solution Implemented: Fixed API error handling and response processing</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#docker-installation-issues","level":2,"title":"Docker Installation Issues","text":"","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#docker-daemon-not-running","level":3,"title":"Docker Daemon Not Running","text":"<p>Symptoms: <pre><code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre></p> <p>Solution: <pre><code># Linux\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# macOS - Start Docker Desktop from Applications\n\n# Windows - Start Docker Desktop from Start Menu\n\n# Verify Docker is running\ndocker ps\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#permission-denied-errors","level":3,"title":"Permission Denied Errors","text":"<p>Symptoms: <pre><code>permission denied while trying to connect to the Docker daemon socket\n</code></pre></p> <p>Solution: <pre><code># Add user to docker group (Linux)\nsudo usermod -aG docker $USER\n\n# Log out and back in, then verify\ngroups | grep docker\n\n# Alternative: use sudo (not recommended)\nsudo docker-compose up\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#container-build-failures","level":3,"title":"Container Build Failures","text":"<p>Symptoms: <pre><code>ERROR: failed to solve: process \"/bin/sh -c apt-get update\" did not complete successfully\n</code></pre></p> <p>Current Container Versions (v2.0.0): - eemt:ubuntu24.04 - Image ID: e3a84eb59c8e - eemt-web:latest - Image ID: e8e8fa0d382d</p> <p>Solutions:</p> <ol> <li> <p>Network Issues: <pre><code># Check DNS settings\ndocker run --rm busybox nslookup google.com\n\n# Use alternative DNS\necho '{\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]}' | sudo tee /etc/docker/daemon.json\nsudo systemctl restart docker\n</code></pre></p> </li> <li> <p>Proxy Configuration: <pre><code># Set Docker proxy\nmkdir -p ~/.docker\ncat &gt; ~/.docker/config.json &lt;&lt; EOF\n{\n  \"proxies\": {\n    \"default\": {\n      \"httpProxy\": \"http://proxy.example.com:8080\",\n      \"httpsProxy\": \"http://proxy.example.com:8080\",\n      \"noProxy\": \"localhost,127.0.0.1\"\n    }\n  }\n}\nEOF\n</code></pre></p> </li> <li> <p>Disk Space: <pre><code># Check available space\ndf -h /var/lib/docker\n\n# Clean up Docker resources\ndocker system prune -a --volumes\n\n# Remove unused images\ndocker image prune -a\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#port-already-in-use","level":3,"title":"Port Already in Use","text":"<p>Symptoms: <pre><code>Error: bind: address already in use\n</code></pre></p> <p>Solution: <pre><code># Find process using port 5000\nsudo lsof -i :5000  # Linux/macOS\nnetstat -ano | findstr :5000  # Windows\n\n# Kill the process or use different port\n# Edit docker-compose.yml\nports:\n  - \"5001:5000\"  # Change host port to 5001\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#volume-mount-issues","level":3,"title":"Volume Mount Issues","text":"<p>Symptoms: <pre><code>Error: invalid mount config for type \"bind\"\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Path Format (Windows): <pre><code># Use forward slashes or escaped backslashes\n-v C:/Users/username/data:/data  # Correct\n-v C:\\Users\\username\\data:/data  # Incorrect\n</code></pre></p> </li> <li> <p>Permissions: <pre><code># Ensure directory exists and has correct permissions\nmkdir -p ./data\nchmod 755 ./data\n\n# For SELinux systems\nchcon -Rt svirt_sandbox_file_t ./data\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#manual-installation-issues","level":2,"title":"Manual Installation Issues","text":"","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#python-version-conflicts","level":3,"title":"Python Version Conflicts","text":"<p>Symptoms: <pre><code>ERROR: This package requires Python &gt;=3.12\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Install Python 3.12: <pre><code># Ubuntu/Debian\nsudo apt install python3.12 python3.12-venv\n\n# macOS\nbrew install python@3.12\n\n# From source\nwget https://www.python.org/ftp/python/3.12.0/Python-3.12.0.tgz\ntar -xf Python-3.12.0.tgz\ncd Python-3.12.0\n./configure --enable-optimizations\nmake -j $(nproc)\nsudo make altinstall\n</code></pre></p> </li> <li> <p>Use pyenv: <pre><code># Install pyenv\ncurl https://pyenv.run | bash\n\n# Install Python 3.12\npyenv install 3.12.0\npyenv local 3.12.0\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#grass-gis-not-found","level":3,"title":"GRASS GIS Not Found","text":"<p>Symptoms: <pre><code>grass: command not found\nERROR: GRASS GIS not installed\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Verify Installation: <pre><code># Check if installed\nwhich grass\napt list --installed | grep grass  # Debian/Ubuntu\nrpm -qa | grep grass  # RHEL/CentOS\n</code></pre></p> </li> <li> <p>Add to PATH: <pre><code># Find GRASS installation\nfind /usr -name \"grass*\" -type f -executable 2&gt;/dev/null\n\n# Add to PATH in ~/.bashrc\nexport PATH=/usr/lib/grass84/bin:$PATH\nexport GISBASE=/usr/lib/grass84\nsource ~/.bashrc\n</code></pre></p> </li> <li> <p>Install Missing Dependencies: <pre><code># Ubuntu/Debian\nsudo apt install grass-core grass-dev\n\n# Fix library issues\nsudo ldconfig\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#gdal-import-errors","level":3,"title":"GDAL Import Errors","text":"<p>Symptoms: <pre><code>ImportError: cannot import name 'gdal' from 'osgeo'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Version Mismatch: <pre><code># Check system GDAL version\ngdalinfo --version\n\n# Install matching Python bindings\npip install GDAL==$(gdal-config --version)\n</code></pre></p> </li> <li> <p>Missing Libraries: <pre><code># Ubuntu/Debian\nsudo apt install gdal-bin libgdal-dev\n\n# Set environment variables\nexport CPLUS_INCLUDE_PATH=/usr/include/gdal\nexport C_INCLUDE_PATH=/usr/include/gdal\npip install --no-binary GDAL GDAL\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#cctoolsmakeflow-issues","level":3,"title":"CCTools/Makeflow Issues","text":"<p>Symptoms: <pre><code>makeflow: command not found\nwork_queue_worker: error while loading shared libraries\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Compilation Errors: <pre><code># Install build dependencies\nsudo apt install build-essential zlib1g-dev\n\n# Recompile with debugging\ncd cctools-source\nmake clean\n./configure --prefix=$HOME/cctools --debug\nmake\nmake install\n</code></pre></p> </li> <li> <p>Library Path Issues: <pre><code># Add to LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$HOME/cctools/lib:$LD_LIBRARY_PATH\necho 'export LD_LIBRARY_PATH=$HOME/cctools/lib:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#numpyscipy-installation-failures","level":3,"title":"NumPy/SciPy Installation Failures","text":"<p>Symptoms: <pre><code>ERROR: Failed building wheel for numpy\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Install System Dependencies: <pre><code># Ubuntu/Debian\nsudo apt install python3-dev libblas-dev liblapack-dev gfortran\n\n# CentOS/RHEL\nsudo yum install python3-devel blas-devel lapack-devel gcc-gfortran\n\n# macOS\nbrew install openblas gfortran\n</code></pre></p> </li> <li> <p>Use Pre-built Wheels: <pre><code># Upgrade pip\npip install --upgrade pip wheel setuptools\n\n# Install with pre-built wheels\npip install --only-binary :all: numpy scipy\n</code></pre></p> </li> </ol>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#platform-specific-issues","level":2,"title":"Platform-Specific Issues","text":"","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#wsl2-windows","level":3,"title":"WSL2 (Windows)","text":"<p>GPU Access: <pre><code># Install CUDA support for WSL2\n# Download from: https://developer.nvidia.com/cuda/wsl\n\n# Verify GPU access\nnvidia-smi\n\n# Enable in Docker\ndocker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n</code></pre></p> <p>File System Performance: <pre><code># Use native Linux filesystem for better performance\ncd /home/username  # Good performance\n# Avoid /mnt/c/    # Poor performance\n\n# Move data to WSL filesystem\ncp -r /mnt/c/Users/username/data ~/data\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#macos-apple-silicon","level":3,"title":"macOS Apple Silicon","text":"<p>Architecture Issues: <pre><code># Install Rosetta 2\nsoftwareupdate --install-rosetta\n\n# Run with specific architecture\narch -x86_64 python script.py  # Intel\narch -arm64 python script.py   # ARM\n\n# Check binary architecture\nfile $(which python)\n</code></pre></p> <p>Homebrew Paths: <pre><code># M1/M2 Homebrew location\nexport PATH=/opt/homebrew/bin:$PATH\n\n# Intel Homebrew location  \nexport PATH=/usr/local/bin:$PATH\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#selinux-centosrhel","level":3,"title":"SELinux (CentOS/RHEL)","text":"<p>Permission Denials: <pre><code># Check SELinux status\ngetenforce\n\n# Temporary disable (testing only)\nsudo setenforce 0\n\n# Proper fix - set context\nsudo chcon -Rt svirt_sandbox_file_t /path/to/data\n\n# Or add SELinux rule\nsudo setsebool -P container_manage_cgroup on\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#web-interface-specific-issues","level":2,"title":"Web Interface Specific Issues","text":"","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#resource-detection-shows-incorrect-values","level":3,"title":"Resource Detection Shows Incorrect Values","text":"<p>Symptoms: <pre><code>System shows incorrect CPU/memory values\n</code></pre></p> <p>Solution: <pre><code># Ensure psutil is installed\npip install psutil\n\n# Restart web interface\npython app.py\n\n# Verify detection is working\ncurl http://localhost:5000/api/system/status\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#docker-subprocess-mode-issues","level":3,"title":"Docker Subprocess Mode Issues","text":"<p>Symptoms: <pre><code>Docker commands fail in subprocess mode\n</code></pre></p> <p>Solution: <pre><code># Ensure user has Docker permissions\nsudo usermod -aG docker $USER\n# Log out and back in\n\n# Test Docker access\ndocker ps\n\n# For Docker Compose deployments\ndocker-compose up --build\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#container-orchestration-problems","level":3,"title":"Container Orchestration Problems","text":"<p>Symptoms: <pre><code>Containers start but workflows don't execute\n</code></pre></p> <p>Solution: <pre><code># Check container logs\ndocker logs &lt;container_id&gt;\n\n# Verify volume mounts\ndocker inspect &lt;container_id&gt; | grep -A 10 Mounts\n\n# Ensure workflow scripts are accessible\ndocker run --rm eemt:ubuntu24.04 ls /opt/eemt/\n</code></pre></p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#diagnostic-commands","level":2,"title":"Diagnostic Commands","text":"","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#enhanced-system-information-v200","level":3,"title":"Enhanced System Information (v2.0.0)","text":"<pre><code>#!/bin/bash\n# Save as diagnose.sh\n\necho \"System Diagnostics for EEMT\"\necho \"============================\"\necho \"\"\n\n# OS Information\necho \"Operating System:\"\nif [ -f /etc/os-release ]; then\n    . /etc/os-release\n    echo \"  $NAME $VERSION\"\nelse\n    echo \"  $(uname -s) $(uname -r)\"\nfi\n\n# Architecture\necho \"  Architecture: $(uname -m)\"\necho \"\"\n\n# Python\necho \"Python Environment:\"\necho \"  Python: $(python3 --version 2&gt;&amp;1)\"\necho \"  Pip: $(pip --version 2&gt;&amp;1)\"\necho \"  Virtual Env: ${VIRTUAL_ENV:-Not activated}\"\necho \"\"\n\n# Docker\necho \"Docker Status:\"\nif command -v docker &amp;&gt; /dev/null; then\n    echo \"  $(docker --version)\"\n    echo \"  Daemon: $(docker ps &amp;&gt; /dev/null &amp;&amp; echo 'Running' || echo 'Not running')\"\n    # Check EEMT images\n    echo \"  EEMT Images:\"\n    docker images | grep eemt | sed 's/^/    /'\n    # Check running containers\n    echo \"  Running Containers:\"\n    docker ps --filter \"ancestor=eemt:ubuntu24.04\" --filter \"ancestor=eemt-web:latest\" | sed 's/^/    /'\nelse\n    echo \"  Docker: Not installed\"\nfi\necho \"\"\n\n# GRASS GIS\necho \"GRASS GIS:\"\nif command -v grass &amp;&gt; /dev/null; then\n    grass --version 2&gt;&amp;1 | head -1 | sed 's/^/  /'\nelse\n    echo \"  Not found in PATH\"\nfi\necho \"\"\n\n# GDAL\necho \"GDAL:\"\nif command -v gdalinfo &amp;&gt; /dev/null; then\n    gdalinfo --version | sed 's/^/  /'\nelse\n    echo \"  Not found in PATH\"\nfi\necho \"\"\n\n# Disk Space\necho \"Disk Space:\"\ndf -h . | tail -1 | awk '{print \"  Available: \"$4\" of \"$2}'\necho \"\"\n\n# Memory\necho \"Memory:\"\nif command -v free &amp;&gt; /dev/null; then\n    free -h | grep Mem | awk '{print \"  Total: \"$2\", Available: \"$7}'\nelse\n    echo \"  Unable to determine\"\nfi\n</code></pre>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#dependency-check","level":3,"title":"Dependency Check","text":"<pre><code>#!/usr/bin/env python3\n# Save as check_deps.py\n\nimport sys\nimport subprocess\n\ndef check_import(module):\n    \"\"\"Check if a Python module can be imported.\"\"\"\n    try:\n        __import__(module)\n        return True, None\n    except ImportError as e:\n        return False, str(e)\n\ndef check_command(cmd):\n    \"\"\"Check if a system command exists.\"\"\"\n    try:\n        subprocess.run([cmd, '--version'], \n                      capture_output=True, \n                      check=False)\n        return True\n    except FileNotFoundError:\n        return False\n\n# Python modules to check (updated for v2.0.0)\nmodules = [\n    'numpy', 'pandas', 'xarray', 'rasterio', \n    'geopandas', 'dask', 'scipy', 'matplotlib',\n    'fastapi', 'uvicorn', 'psutil', 'docker'\n]\n\n# System commands to check\ncommands = ['grass', 'gdalinfo', 'docker', 'makeflow']\n\nprint(\"EEMT Dependency Check\")\nprint(\"=\" * 40)\n\nprint(\"\\nPython Modules:\")\nfor module in modules:\n    success, error = check_import(module)\n    status = \"‚úì\" if success else \"‚úó\"\n    print(f\"  {status} {module:20} {'OK' if success else error}\")\n\nprint(\"\\nSystem Commands:\")\nfor cmd in commands:\n    success = check_command(cmd)\n    status = \"‚úì\" if success else \"‚úó\"\n    print(f\"  {status} {cmd:20} {'Found' if success else 'Not found'}\")\n\nprint(\"\\nPython Version:\", sys.version)\n</code></pre>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"installation/troubleshooting/#getting-help","level":2,"title":"Getting Help","text":"<p>If these solutions don't resolve your issue:</p> <ol> <li> <p>Collect Diagnostic Information:    <pre><code>./diagnose.sh &gt; diagnostics.txt\npython check_deps.py &gt;&gt; diagnostics.txt\n</code></pre></p> </li> <li> <p>Search Existing Issues:</p> </li> <li>GitHub Issues</li> <li> <p>Stack Overflow</p> </li> <li> <p>Create New Issue with:</p> </li> <li>Diagnostic output</li> <li>Complete error messages</li> <li>Steps to reproduce</li> <li> <p>Installation method attempted</p> </li> <li> <p>Community Support:</p> </li> <li>GitHub Discussions</li> <li>GRASS GIS Mailing List</li> </ol> <p>Return to Installation Overview or proceed to Quick Start Guide.</p>","path":["Installation","Installation Troubleshooting"],"tags":[]},{"location":"web-interface/","level":1,"title":"Web Interface","text":"<p>The EEMT Web Interface provides a modern, user-friendly way to submit and monitor EEMT and solar radiation workflows through a browser-based application with full Docker integration.</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#overview","level":2,"title":"Overview","text":"<p>The web interface is built with FastAPI and provides:</p> <ul> <li>üåê Browser-based Job Submission: Upload DEM files and configure parameters</li> <li>üìä Real-time Monitoring: Track progress with live updates</li> <li>üê≥ Containerized Execution: Full Docker integration with workflow containers</li> <li>üíæ Results Management: Download processed data as ZIP archives</li> <li>üèóÔ∏è Scalable Deployment: Single-node or distributed multi-container architecture</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#docker-deployment-recommended","level":2,"title":"Docker Deployment (Recommended)","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#quick-start-with-docker-compose","level":3,"title":"Quick Start with Docker Compose","text":"<p>The easiest way to deploy EEMT is using Docker Compose, which handles all dependencies and container orchestration:</p> <pre><code># Clone repository\ngit clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n\n# Start local mode (web interface + single worker)\ndocker-compose up\n\n# Access web interface at http://localhost:5000\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#deployment-modes","level":3,"title":"Deployment Modes","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#1-local-mode-default","level":4,"title":"1. Local Mode (Default)","text":"<p>Single-container deployment for development and small jobs:</p> <pre><code># Start local web interface\ndocker-compose up eemt-web\n\n# Or build and run manually\ndocker build -t eemt-web -f docker/web-interface/Dockerfile .\ndocker run -p 5000:5000 -v $(pwd)/data:/app/data eemt-web\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#2-distributed-mode","level":4,"title":"2. Distributed Mode","text":"<p>Multi-container deployment with master and worker nodes:</p> <pre><code># Start distributed cluster\ndocker-compose --profile distributed up\n\n# Scale workers\ndocker-compose --profile distributed --profile scale up --scale eemt-worker-2=2 --scale eemt-worker-3=3\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#3-documentation-server","level":4,"title":"3. Documentation Server","text":"<pre><code># Start documentation alongside web interface\ndocker-compose --profile docs up eemt-docs\n\n# Access docs at http://localhost:8000\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#docker-environment-variables","level":3,"title":"Docker Environment Variables","text":"<p>Configure deployment through environment variables:</p> <pre><code># Local mode configuration\nEEMT_MODE=local\nEEMT_HOST=0.0.0.0\nEEMT_PORT=5000\n\n# Distributed mode configuration\nEEMT_MODE=master\nWORK_QUEUE_PORT=9123\nWORK_QUEUE_PROJECT=EEMT-Production\nMAX_WORKERS=50\n\n# Worker configuration\nMASTER_HOST=eemt-master\nMASTER_PORT=9123\nWORKER_CORES=8\nWORKER_MEMORY=16G\nWORKER_DISK=100G\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#manual-installation","level":2,"title":"Manual Installation","text":"<p>For development or custom deployments:</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#1-build-base-container","level":3,"title":"1. Build Base Container","text":"<pre><code># Build EEMT base container with all dependencies\ncd docker/ubuntu/24.04/\n./build.sh\n\n# Verify image\ndocker images | grep eemt\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#2-build-web-interface-container","level":3,"title":"2. Build Web Interface Container","text":"<pre><code># Build web interface container\ndocker build -t eemt-web -f docker/web-interface/Dockerfile .\n\n# Or use pre-built image (if available)\ndocker pull eemt/web-interface:latest\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#3-run-web-interface","level":3,"title":"3. Run Web Interface","text":"<pre><code># Create data directories\nmkdir -p data/{uploads,results,temp,cache,shared}\n\n# Run web interface container\ndocker run -d \\\n  --name eemt-web \\\n  -p 5000:5000 \\\n  -v $(pwd)/data:/app/data \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  eemt-web\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#4-access-interface","level":3,"title":"4. Access Interface","text":"<ul> <li>Web Interface: http://localhost:5000</li> <li>Job Monitor: http://localhost:5000/monitor</li> <li>API Documentation: http://localhost:5000/docs</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#features","level":2,"title":"Features","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#job-submission-interface","level":3,"title":"Job Submission Interface","text":"<ul> <li>Workflow Selection: Choose between Solar Radiation and Full EEMT workflows</li> <li>DEM Upload: Drag-and-drop or select GeoTIFF files</li> <li>Parameter Configuration: <ul> <li>Time step (3-15 minutes)</li> <li>Atmospheric turbidity (Linke value)</li> <li>Surface albedo</li> <li>CPU threads</li> <li>Climate data range (EEMT only)</li> </ul> </li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#real-time-monitoring","level":3,"title":"Real-time Monitoring","text":"<ul> <li>Summary Dashboard: Overview of pending, running, completed, and failed jobs</li> <li>Job Table: Detailed status with progress bars</li> <li>Auto-refresh: Updates every 5 seconds</li> <li>Job Details: Click for detailed information and logs</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#system-status","level":3,"title":"System Status","text":"<p>The interface automatically checks:</p> <ul> <li>‚úÖ Docker daemon availability</li> <li>‚úÖ Container image presence</li> <li>‚úÖ Resource availability</li> <li>‚ö†Ô∏è Setup warnings and instructions</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#workflow-types","level":2,"title":"Workflow Types","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#solar-radiation-modeling","level":3,"title":"Solar Radiation Modeling","text":"<p>Purpose: Calculate daily solar irradiation for topographic analysis</p> <p>Process: 1. Processes DEM through GRASS GIS r.sun 2. Calculates 365 daily solar radiation maps 3. Generates monthly aggregated products 4. Outputs global and direct solar radiation</p> <p>Typical Runtime: 5-30 minutes depending on DEM resolution</p> <p>Outputs: - <code>global/daily/total_sun_day_*.tif</code> - Daily solar radiation (365 files) - <code>global/monthly/total_sun_*_sum.tif</code> - Monthly aggregates (12 files) - <code>insol/daily/hours_sun_day_*.tif</code> - Daily sunshine hours (365 files)</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#full-eemt-analysis","level":3,"title":"Full EEMT Analysis","text":"<p>Purpose: Complete energy-mass transfer calculation with climate integration</p> <p>Process: 1. Performs solar radiation calculations 2. Downloads DAYMET climate data 3. Calculates topographic indices (slope, aspect, TWI) 4. Computes EEMT values combining solar and climate data 5. Generates multi-year energy transfer maps</p> <p>Typical Runtime: 1-4 hours depending on time period and resolution</p> <p>Outputs: - All solar radiation products (above) - <code>eemt/EEMT_Topo_*_*.tif</code> - Topographic EEMT values - <code>eemt/EEMT_Trad_*_*.tif</code> - Traditional EEMT values - <code>daymet/</code> - Downloaded climate data</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#container-architecture","level":2,"title":"Container Architecture","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#execution-flow","level":3,"title":"Execution Flow","text":"<pre><code>graph TB\n    subgraph \"Docker Host\"\n        subgraph \"Web Interface Container\"\n            WI[FastAPI Web App]\n            WM[Workflow Manager]\n            DB[SQLite Database]\n        end\n\n        subgraph \"Workflow Containers\"\n            WC1[EEMT Worker 1]\n            WC2[EEMT Worker 2]\n            WC3[EEMT Worker N]\n        end\n\n        subgraph \"Data Volumes\"\n            UP[Uploads Volume]\n            RES[Results Volume]\n            TMP[Temp Volume]\n            CACHE[Cache Volume]\n        end\n    end\n\n    subgraph \"External\"\n        USER[User Browser]\n        DOCKER[Docker Daemon]\n    end\n\n    USER --&gt; WI\n    WI --&gt; WM\n    WM --&gt; DOCKER\n    DOCKER --&gt; WC1\n    DOCKER --&gt; WC2\n    DOCKER --&gt; WC3\n\n    WC1 --&gt; UP\n    WC1 --&gt; RES\n    WC2 --&gt; TMP\n    WC3 --&gt; CACHE</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#container-images","level":3,"title":"Container Images","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#base-container-eemtubuntu2404","level":4,"title":"Base Container (<code>eemt:ubuntu24.04</code>)","text":"<p>Contains all scientific computing dependencies: - GRASS GIS 8.4+: With r.sun extensions for solar modeling - CCTools 7.8.2: Makeflow + Work Queue for distributed processing - Python 3.12: Complete geospatial environment with scientific libraries - GDAL 3.11: Modern geospatial data access and format support - Workflow Scripts: Container entry points and scientific computing utilities</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#web-interface-container-eemt-web","level":4,"title":"Web Interface Container (<code>eemt-web</code>)","text":"<p>Extends base container with web application: - FastAPI Application: Web interface and REST API - Workflow Manager: Docker orchestration and job management - Monitoring Tools: Real-time progress tracking and log aggregation - Docker Integration: Direct container management capabilities</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#volume-management","level":3,"title":"Volume Management","text":"<p>Docker Compose automatically creates and manages data volumes:</p> <pre><code># Volume mounts in docker-compose.yml\nvolumes:\n  - ./data/uploads:/app/uploads      # DEM file uploads\n  - ./data/results:/app/results      # Workflow outputs\n  - ./data/temp:/app/temp            # Temporary processing data\n  - ./data/cache:/app/cache          # Workflow caching\n  - ./data/shared:/app/shared        # Shared data (distributed mode)\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#container-networking","level":3,"title":"Container Networking","text":"<pre><code># Docker network configuration\nnetworks:\n  eemt-network:\n    driver: bridge\n</code></pre> <p>This allows containers to communicate using service names (e.g., <code>eemt-master</code>, <code>eemt-worker</code>).</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#rest-api","level":2,"title":"REST API","text":"<p>The web interface exposes a REST API for programmatic access:</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#submit-job","level":3,"title":"Submit Job","text":"<pre><code>POST /api/submit-job\nContent-Type: multipart/form-data\n\nParameters:\n- workflow_type: \"sol\" or \"eemt\"\n- dem_file: GeoTIFF file upload\n- step: float (time step in minutes)\n- linke_value: float (atmospheric turbidity)\n- albedo_value: float (surface reflectance)  \n- num_threads: int (CPU threads)\n- start_year: int (EEMT only)\n- end_year: int (EEMT only)\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#monitor-jobs","level":3,"title":"Monitor Jobs","text":"<pre><code># List all jobs\nGET /api/jobs\n\n# Get specific job details\nGET /api/jobs/{job_id}\n\n# Download results\nGET /api/jobs/{job_id}/results\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#system-status_1","level":3,"title":"System Status","text":"<pre><code>GET /api/system/status\n</code></pre> <p>Returns: <pre><code>{\n  \"docker_available\": true,\n  \"container_stats\": {\n    \"total_containers\": 2,\n    \"running_jobs\": [\"job-123\"],\n    \"system_stats\": {\n      \"cpus\": 8,\n      \"memory\": 16777216000\n    }\n  },\n  \"image_name\": \"eemt:ubuntu24.04\"\n}\n</code></pre></p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#configuration","level":2,"title":"Configuration","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#environment-variables","level":3,"title":"Environment Variables","text":"<pre><code># Host and port configuration\nEEMT_HOST=\"127.0.0.1\"        # Bind address\nEEMT_PORT=\"5000\"             # Service port\n\n# Directory configuration\nEEMT_UPLOAD_DIR=\"./uploads\"  # DEM upload directory\nEEMT_RESULTS_DIR=\"./results\" # Job output directory\nEEMT_TEMP_DIR=\"./temp\"       # Temporary processing\nEEMT_CACHE_DIR=\"./cache\"     # Workflow cache\n\n# Container configuration  \nDOCKER_IMAGE=\"eemt:ubuntu24.04\"  # Container image name\nCONTAINER_CPU_LIMIT=\"4\"          # Default CPU limit\nCONTAINER_MEMORY_LIMIT=\"8G\"      # Default memory limit\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#database-configuration","level":3,"title":"Database Configuration","text":"<p>The interface uses SQLite for job tracking:</p> <ul> <li>Location: <code>./jobs.db</code> (auto-created)</li> <li>Schema: Automatically initialized on first run</li> <li>Backup: Standard SQLite tools (<code>sqlite3 .backup</code>)</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#common-issues","level":3,"title":"Common Issues","text":"<ol> <li> <p>\"Docker not available\" <pre><code># Check Docker daemon\ndocker info\n\n# Ensure Docker service is running\nsudo systemctl start docker\n</code></pre></p> </li> <li> <p>\"Container image not found\" <pre><code># Build the container\ncd docker/ubuntu/24.04/\n./build.sh\n\n# Verify image exists\ndocker images | grep eemt\n</code></pre></p> </li> <li> <p>\"Job execution failed\"</p> </li> <li>Check job details in monitor for container logs</li> <li>Verify DEM is valid GeoTIFF with proper projection</li> <li> <p>Ensure adequate disk space and memory</p> </li> <li> <p>\"Web interface not accessible\" <pre><code># Check if port is available\nnetstat -an | grep 5000\n\n# Try alternative port\nuvicorn app:app --host 127.0.0.1 --port 8080\n</code></pre></p> </li> </ol>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#performance-optimization","level":3,"title":"Performance Optimization","text":"<ul> <li>Large DEMs: Consider processing smaller tiles</li> <li>Concurrent Jobs: Limit based on available CPU/memory</li> <li>Container Resources: Adjust limits in <code>workflow_manager.py</code></li> <li>Disk Space: Monitor usage in uploads/, results/, temp/</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#logging","level":3,"title":"Logging","text":"<p>Application logs are available:</p> <pre><code># Start with debug logging\nuvicorn app:app --log-level debug\n\n# Container execution logs  \ndocker logs &lt;container_id&gt;\n\n# Job-specific logs in web interface monitor\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#development","level":2,"title":"Development","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#api-development","level":3,"title":"API Development","text":"<p>The FastAPI application supports:</p> <ul> <li>Auto-documentation: Available at <code>/docs</code></li> <li>Interactive testing: Try API endpoints in browser</li> <li>OpenAPI schema: Available at <code>/openapi.json</code></li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#frontend-development","level":3,"title":"Frontend Development","text":"<p>The HTML interface uses:</p> <ul> <li>Bootstrap 5: Responsive CSS framework</li> <li>Vanilla JavaScript: No heavy frontend dependencies</li> <li>WebSocket: For real-time progress updates (planned)</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#container-development","level":3,"title":"Container Development","text":"<p>To modify container workflows:</p> <ol> <li>Edit scripts in <code>docker/ubuntu/24.04/container-scripts/</code></li> <li>Rebuild container: <code>./build.sh</code></li> <li>Test with web interface</li> </ol>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#integration","level":2,"title":"Integration","text":"","path":["Web Interface"],"tags":[]},{"location":"web-interface/#distributed-mode","level":3,"title":"Distributed Mode","text":"<p>The web interface can be integrated with distributed computing environments:</p> <ul> <li>Multi-host execution: Deploy master node with web interface, connect remote workers</li> <li>Load balancing: Work Queue automatically distributes tasks across available workers</li> <li>Fault tolerance: Automatic retry and worker replacement mechanisms</li> <li>HPC Integration: Connect to SLURM, PBS, LSF batch schedulers</li> </ul> <p>See Distributed Deployment for complete setup guide.</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#master-node-configuration","level":3,"title":"Master Node Configuration","text":"<p>To run the web interface as a master node:</p> <pre><code>from containers.workflow_manager import DistributedWorkflowManager, NodeType, MasterConfig\n\n# Configure master with web interface\nmaster_config = MasterConfig(\n    port=9123,\n    max_workers=100,\n    work_queue_project=\"EEMT-Cluster\"\n)\n\n# Initialize master workflow manager  \nmaster = DistributedWorkflowManager(\n    base_dir=Path(\"/data/eemt-master\"),\n    node_type=NodeType.MASTER,\n    master_config=master_config\n)\n\n# Start master node\nmaster.start_master_node()\n</code></pre> <p>Workers can then connect from remote machines:</p> <pre><code># Start worker on remote machine\npython scripts/start-worker.py \\\n    --master-host your-master-ip \\\n    --master-port 9123 \\\n    --cores 8 --memory 16G\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#cloud-deployment","level":3,"title":"Cloud Deployment","text":"<p>Cloud integration examples:</p> <ul> <li>Kubernetes: Container orchestration with persistent volumes</li> <li>AWS/GCP/Azure: VM clusters with shared storage</li> <li>Singularity: HPC container deployment</li> </ul> <p>See PLAN.md for complete modernization roadmap.</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#job-data-management","level":2,"title":"Job Data Management","text":"<p>The web interface includes comprehensive job data lifecycle management through an automated cleanup system. This ensures optimal disk usage while preserving important job metadata.</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#cleanup-system-features","level":3,"title":"Cleanup System Features","text":"<ul> <li>Automated retention policies: 7-day retention for successful jobs, 12-hour for failed jobs</li> <li>Selective data deletion: Removes output data while preserving job configurations</li> <li>Multiple trigger methods: API endpoints, scheduled tasks, or manual execution</li> <li>Container integration: Works seamlessly with Docker deployments</li> <li>Comprehensive logging: Full audit trail of cleanup operations</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#quick-cleanup-setup","level":3,"title":"Quick Cleanup Setup","text":"<p>Enable automated cleanup with default settings:</p> <pre><code># Navigate to web interface directory\ncd web-interface/\n\n# Set up automated cleanup (runs daily at 2 AM)\n./setup_cleanup_cron.sh --user --method cron\n\n# Test cleanup in dry-run mode\npython cleanup_jobs.py --dry-run\n</code></pre> <p>Or configure via Docker Compose:</p> <pre><code>services:\n  eemt-web:\n    environment:\n      - EEMT_SUCCESS_RETENTION_DAYS=7\n      - EEMT_FAILED_RETENTION_HOURS=12\n      - EEMT_ENABLE_AUTO_CLEANUP=true\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#cleanup-api-endpoint","level":3,"title":"Cleanup API Endpoint","text":"<p>Trigger cleanup programmatically:</p> <pre><code># Manual cleanup via API\ncurl -X POST http://localhost:5000/api/cleanup\n\n# Dry run with custom retention\ncurl -X POST http://localhost:5000/api/cleanup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"dry_run\": true, \"success_retention_days\": 3}'\n</code></pre>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#related-documentation","level":3,"title":"Related Documentation","text":"<p>For detailed information about the cleanup system:</p> <ul> <li>Job Cleanup Reference - Complete cleanup system documentation</li> <li>Cleanup Scripts Guide - Step-by-step usage instructions</li> <li>Docker Integration - Container-specific deployment</li> <li>Infrastructure Details - System architecture and implementation</li> </ul>","path":["Web Interface"],"tags":[]},{"location":"web-interface/#summary","level":2,"title":"Summary","text":"<p>The EEMT web interface provides a modern, containerized solution for managing EEMT workflows with features including:</p> <ul> <li>Browser-based job submission with real-time monitoring</li> <li>Docker integration for reliable, reproducible execution</li> <li>Automated data cleanup to maintain system performance</li> <li>REST API for programmatic access</li> <li>Scalable architecture supporting single-node to distributed deployments</li> </ul> <p>The combination of user-friendly interface, container orchestration, and intelligent data management ensures your EEMT deployment remains efficient and maintainable at any scale.</p>","path":["Web Interface"],"tags":[]},{"location":"web-interface/api-reference/","level":1,"title":"API Reference","text":"<p>The EEMT Web Interface exposes a comprehensive REST API for programmatic workflow submission and monitoring.</p>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#base-url","level":2,"title":"Base URL","text":"<pre><code>http://127.0.0.1:5000\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#authentication","level":2,"title":"Authentication","text":"<p>Currently, the API does not require authentication. Future versions may include:</p> <ul> <li>API key authentication</li> <li>JWT token-based access</li> <li>Role-based permissions</li> </ul>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#content-types","level":2,"title":"Content Types","text":"<ul> <li>Request: <code>multipart/form-data</code> for file uploads, <code>application/json</code> for data</li> <li>Response: <code>application/json</code> for all endpoints except file downloads</li> </ul>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#job-management","level":2,"title":"Job Management","text":"","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#submit-workflow-job","level":3,"title":"Submit Workflow Job","text":"<p>Submit a new EEMT or solar radiation workflow for processing.</p> <pre><code>POST /api/submit-job\nContent-Type: multipart/form-data\n</code></pre> <p>Parameters:</p> Parameter Type Required Description <code>workflow_type</code> string Yes <code>\"sol\"</code> for solar radiation, <code>\"eemt\"</code> for full analysis <code>dem_file</code> file Yes GeoTIFF elevation model file <code>step</code> float No Time step in minutes (default: 15.0, range: 3-60) <code>linke_value</code> float No Atmospheric turbidity (default: 3.0, range: 1.0-8.0) <code>albedo_value</code> float No Surface reflectance (default: 0.2, range: 0.0-1.0) <code>num_threads</code> integer No CPU threads (default: 4, range: 1-32) <code>start_year</code> integer No Start year for EEMT (default: 2020, range: 1980-2024) <code>end_year</code> integer No End year for EEMT (default: 2020, range: 1980-2024) <p>Example Request:</p> <pre><code>curl -X POST \"http://127.0.0.1:5000/api/submit-job\" \\\n  -F \"workflow_type=sol\" \\\n  -F \"dem_file=@my_elevation_data.tif\" \\\n  -F \"step=15\" \\\n  -F \"linke_value=3.5\" \\\n  -F \"albedo_value=0.2\" \\\n  -F \"num_threads=8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"job_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n  \"status\": \"submitted\"\n}\n</code></pre> <p>Error Responses:</p> <pre><code>// Invalid file format\n{\n  \"detail\": \"DEM file must be a GeoTIFF (.tif)\"\n}\n\n// Missing required parameter\n{\n  \"detail\": \"workflow_type is required\"\n}\n\n// Docker not available\n{\n  \"detail\": \"Container execution failed: Docker daemon not available\"\n}\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#list-all-jobs","level":3,"title":"List All Jobs","text":"<p>Retrieve a list of all workflow jobs.</p> <pre><code>GET /api/jobs\n</code></pre> <p>Response:</p> <pre><code>[\n  {\n    \"id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"workflow_type\": \"sol\",\n    \"status\": \"completed\",\n    \"created_at\": \"2025-01-15T10:30:00Z\",\n    \"dem_filename\": \"my_elevation_data.tif\",\n    \"progress\": 100\n  },\n  {\n    \"id\": \"b2c3d4e5-f6g7-8901-bcde-f23456789012\",\n    \"workflow_type\": \"eemt\", \n    \"status\": \"running\",\n    \"created_at\": \"2025-01-15T11:15:00Z\",\n    \"dem_filename\": \"large_dem.tif\",\n    \"progress\": 65\n  }\n]\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#get-job-details","level":3,"title":"Get Job Details","text":"<p>Retrieve detailed information about a specific job.</p> <pre><code>GET /api/jobs/{job_id}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n  \"workflow_type\": \"sol\",\n  \"status\": \"completed\",\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"started_at\": \"2025-01-15T10:30:15Z\",\n  \"completed_at\": \"2025-01-15T10:45:30Z\",\n  \"parameters\": {\n    \"step\": 15.0,\n    \"linke_value\": 3.5,\n    \"albedo_value\": 0.2,\n    \"num_threads\": 8\n  },\n  \"dem_filename\": \"my_elevation_data.tif\",\n  \"error_message\": null,\n  \"progress\": 100\n}\n</code></pre> <p>Error Responses:</p> <pre><code>// Job not found\n{\n  \"detail\": \"Job not found\"\n}\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#download-job-results","level":3,"title":"Download Job Results","text":"<p>Download the complete results of a completed job as a ZIP archive.</p> <pre><code>GET /api/jobs/{job_id}/results\n</code></pre> <p>Response:</p> <ul> <li>Success: ZIP file download with filename <code>eemt_results_{job_id}.zip</code></li> <li>Content-Type: <code>application/zip</code></li> </ul> <p>Example Request:</p> <pre><code>curl -O \"http://127.0.0.1:5000/api/jobs/a1b2c3d4-e5f6-7890-abcd-ef1234567890/results\"\n</code></pre> <p>Error Responses:</p> <pre><code>// Job not completed\n{\n  \"detail\": \"Job not completed\"\n}\n\n// Results not found\n{\n  \"detail\": \"Results not found\"\n}\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#system-information","level":2,"title":"System Information","text":"","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#get-system-status","level":3,"title":"Get System Status","text":"<p>Retrieve Docker availability and container statistics.</p> <pre><code>GET /api/system/status\n</code></pre> <p>Response:</p> <pre><code>{\n  \"docker_available\": true,\n  \"container_stats\": {\n    \"total_containers\": 2,\n    \"running_jobs\": [\n      \"job-a1b2c3d4\",\n      \"job-b2c3d4e5\"\n    ],\n    \"system_stats\": {\n      \"cpus\": 8,\n      \"memory\": 16777216000,\n      \"containers_running\": 2\n    }\n  },\n  \"image_name\": \"eemt:ubuntu24.04\"\n}\n</code></pre> <p>Error Response (Docker unavailable):</p> <pre><code>{\n  \"docker_available\": false,\n  \"error\": \"Docker daemon not reachable\",\n  \"image_name\": \"eemt:ubuntu24.04\"\n}\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#job-status-values","level":2,"title":"Job Status Values","text":"Status Description <code>pending</code> Job queued, waiting to start <code>running</code> Container executing workflow <code>completed</code> Job finished successfully <code>failed</code> Job terminated with error","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#progress-tracking","level":2,"title":"Progress Tracking","text":"<p>Progress is reported as integer percentage (0-100):</p> <ul> <li>0-10: Job initialization and container startup</li> <li>10-90: Workflow execution (varies by complexity)</li> <li>90-100: Results collection and cleanup</li> </ul>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#rate-limiting","level":2,"title":"Rate Limiting","text":"<p>Currently no rate limiting is implemented. Consider implementing:</p> <ul> <li>Max concurrent jobs per client</li> <li>File upload size limits (currently ~1GB recommended)</li> <li>API request frequency limits</li> </ul>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#error-handling","level":2,"title":"Error Handling","text":"<p>All errors follow consistent format:</p> <pre><code>{\n  \"detail\": \"Human-readable error message\"\n}\n</code></pre> <p>Common HTTP status codes:</p> <ul> <li><code>200</code>: Success</li> <li><code>400</code>: Bad Request (invalid parameters)</li> <li><code>404</code>: Not Found (job/resource doesn't exist)</li> <li><code>422</code>: Unprocessable Entity (validation error)</li> <li><code>500</code>: Internal Server Error (system/container issue)</li> </ul>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#examples","level":2,"title":"Examples","text":"","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#python-client-example","level":3,"title":"Python Client Example","text":"<pre><code>import requests\nimport time\n\n# Submit job\nwith open('my_dem.tif', 'rb') as f:\n    response = requests.post(\n        'http://127.0.0.1:5000/api/submit-job',\n        files={'dem_file': f},\n        data={\n            'workflow_type': 'sol',\n            'step': 15,\n            'num_threads': 4\n        }\n    )\n\njob_id = response.json()['job_id']\nprint(f\"Job submitted: {job_id}\")\n\n# Monitor progress\nwhile True:\n    status = requests.get(f'http://127.0.0.1:5000/api/jobs/{job_id}').json()\n    print(f\"Status: {status['status']} ({status['progress']}%)\")\n\n    if status['status'] in ['completed', 'failed']:\n        break\n    time.sleep(10)\n\n# Download results if successful\nif status['status'] == 'completed':\n    results = requests.get(f'http://127.0.0.1:5000/api/jobs/{job_id}/results')\n    with open(f'results_{job_id}.zip', 'wb') as f:\n        f.write(results.content)\n    print(\"Results downloaded!\")\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#javascript-client-example","level":3,"title":"JavaScript Client Example","text":"<pre><code>// Submit job\nconst formData = new FormData();\nformData.append('workflow_type', 'sol');\nformData.append('dem_file', demFileInput.files[0]);\nformData.append('step', '15');\nformData.append('num_threads', '4');\n\nconst submitResponse = await fetch('/api/submit-job', {\n    method: 'POST',\n    body: formData\n});\n\nconst submitResult = await submitResponse.json();\nconsole.log('Job submitted:', submitResult.job_id);\n\n// Monitor progress\nconst jobId = submitResult.job_id;\nconst checkStatus = async () =&gt; {\n    const response = await fetch(`/api/jobs/${jobId}`);\n    const job = await response.json();\n\n    console.log(`Status: ${job.status} (${job.progress}%)`);\n\n    if (job.status === 'completed') {\n        // Download results\n        window.open(`/api/jobs/${jobId}/results`);\n    } else if (job.status === 'failed') {\n        console.error('Job failed:', job.error_message);\n    } else {\n        setTimeout(checkStatus, 5000); // Check again in 5 seconds\n    }\n};\n\ncheckStatus();\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#bash-script-example","level":3,"title":"Bash Script Example","text":"<pre><code>#!/bin/bash\n\n# Submit job\nRESPONSE=$(curl -s -X POST \"http://127.0.0.1:5000/api/submit-job\" \\\n  -F \"workflow_type=sol\" \\\n  -F \"dem_file=@dem.tif\" \\\n  -F \"step=15\" \\\n  -F \"num_threads=4\")\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n\n# Monitor progress\nwhile true; do\n    STATUS=$(curl -s \"http://127.0.0.1:5000/api/jobs/$JOB_ID\" | jq -r '.status')\n    PROGRESS=$(curl -s \"http://127.0.0.1:5000/api/jobs/$JOB_ID\" | jq -r '.progress')\n\n    echo \"Status: $STATUS ($PROGRESS%)\"\n\n    if [[ \"$STATUS\" == \"completed\" ]]; then\n        echo \"Downloading results...\"\n        curl -O \"http://127.0.0.1:5000/api/jobs/$JOB_ID/results\"\n        break\n    elif [[ \"$STATUS\" == \"failed\" ]]; then\n        echo \"Job failed!\"\n        break\n    fi\n\n    sleep 10\ndone\n</code></pre>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/api-reference/#openapi-documentation","level":2,"title":"OpenAPI Documentation","text":"<p>Interactive API documentation is automatically generated and available at:</p> <ul> <li>Swagger UI: http://127.0.0.1:5000/docs</li> <li>ReDoc: http://127.0.0.1:5000/redoc </li> <li>OpenAPI JSON: http://127.0.0.1:5000/openapi.json</li> </ul> <p>The interactive documentation allows you to:</p> <ul> <li>Browse all available endpoints</li> <li>Test API calls directly in the browser</li> <li>View detailed parameter descriptions</li> <li>See example requests and responses</li> </ul>","path":["Web Interface","API Reference"],"tags":[]},{"location":"web-interface/architecture/","level":1,"title":"Web Interface Architecture","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#overview","level":2,"title":"Overview","text":"<p>The EEMT Web Interface is a modern, containerized FastAPI application that provides browser-based access to EEMT workflows. This document details the architectural design, component interactions, and implementation patterns that power the web interface.</p>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#system-architecture","level":2,"title":"System Architecture","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#high-level-design","level":3,"title":"High-Level Design","text":"<pre><code>graph TB\n    subgraph \"Client Layer\"\n        Browser[Web Browser]\n        API[REST API Client]\n    end\n\n    subgraph \"Application Layer\"\n        subgraph \"FastAPI Application\"\n            Routes[API Routes]\n            Templates[HTML Templates]\n            Static[Static Assets]\n            WM[Workflow Manager]\n            DB[SQLite Database]\n        end\n    end\n\n    subgraph \"Container Layer\"\n        Docker[Docker SDK]\n        Containers[Workflow Containers]\n    end\n\n    subgraph \"Storage Layer\"\n        Uploads[Uploads Volume]\n        Results[Results Volume]\n        Temp[Temp Volume]\n    end\n\n    Browser --&gt; Routes\n    API --&gt; Routes\n    Routes --&gt; WM\n    Routes --&gt; DB\n    WM --&gt; Docker\n    Docker --&gt; Containers\n    Containers --&gt; Uploads\n    Containers --&gt; Results\n    Containers --&gt; Temp\n    Routes --&gt; Templates\n    Templates --&gt; Static</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#component-overview","level":3,"title":"Component Overview","text":"<p>The web interface consists of several interconnected components:</p> <ol> <li>FastAPI Application: Core web framework handling HTTP requests</li> <li>Workflow Manager: Orchestrates container execution and job management</li> <li>Database Layer: SQLite for job persistence and tracking</li> <li>Docker Integration: Container lifecycle management</li> <li>Frontend: HTML/JavaScript interface for user interaction</li> <li>Storage Management: Volume handling for data persistence</li> </ol>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#fastapi-application-structure","level":2,"title":"FastAPI Application Structure","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#directory-layout","level":3,"title":"Directory Layout","text":"<pre><code>web-interface/\n‚îú‚îÄ‚îÄ app.py                 # Main FastAPI application\n‚îú‚îÄ‚îÄ containers/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ workflow_manager.py # Container orchestration\n‚îú‚îÄ‚îÄ models/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ job.py            # Pydantic models\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îú‚îÄ‚îÄ index.html        # Job submission page\n‚îÇ   ‚îú‚îÄ‚îÄ monitor.html      # Job monitoring dashboard\n‚îÇ   ‚îî‚îÄ‚îÄ base.html         # Base template\n‚îú‚îÄ‚îÄ static/\n‚îÇ   ‚îú‚îÄ‚îÄ css/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css\n‚îÇ   ‚îú‚îÄ‚îÄ js/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.js\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitor.js\n‚îÇ   ‚îî‚îÄ‚îÄ img/\n‚îú‚îÄ‚îÄ uploads/              # DEM file uploads\n‚îú‚îÄ‚îÄ results/              # Job outputs\n‚îú‚îÄ‚îÄ temp/                 # Temporary processing\n‚îú‚îÄ‚îÄ cache/                # Workflow cache\n‚îú‚îÄ‚îÄ jobs.db               # SQLite database\n‚îî‚îÄ‚îÄ requirements.txt      # Python dependencies\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#application-initialization","level":3,"title":"Application Initialization","text":"<pre><code># app.py core structure\nfrom fastapi import FastAPI, BackgroundTasks\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nimport sqlite3\n\napp = FastAPI(\n    title=\"EEMT Web Interface\",\n    description=\"Effective Energy and Mass Transfer Workflow System\",\n    version=\"2.0.0\"\n)\n\n# Mount static files\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Configure templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Initialize database\ndef init_db():\n    conn = sqlite3.connect('jobs.db')\n    cursor = conn.cursor()\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS jobs (\n            id TEXT PRIMARY KEY,\n            workflow_type TEXT,\n            status TEXT,\n            created_at TIMESTAMP,\n            started_at TIMESTAMP,\n            completed_at TIMESTAMP,\n            parameters JSON,\n            dem_filename TEXT,\n            error_message TEXT,\n            progress INTEGER DEFAULT 0\n        )\n    ''')\n    conn.commit()\n    conn.close()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    init_db()\n    ensure_directories()\n    check_docker_availability()\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#request-handling-flow","level":2,"title":"Request Handling Flow","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#job-submission-workflow","level":3,"title":"Job Submission Workflow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant Validator\n    participant Storage\n    participant WorkflowManager\n    participant Docker\n    participant Container\n\n    User-&gt;&gt;Browser: Select DEM &amp; Parameters\n    Browser-&gt;&gt;FastAPI: POST /api/submit-job\n    FastAPI-&gt;&gt;Validator: Validate Input\n    Validator--&gt;&gt;FastAPI: Validation Result\n\n    alt Validation Success\n        FastAPI-&gt;&gt;Storage: Save DEM File\n        FastAPI-&gt;&gt;WorkflowManager: Create Job\n        WorkflowManager-&gt;&gt;Docker: Start Container\n        Docker-&gt;&gt;Container: Execute Workflow\n        FastAPI--&gt;&gt;Browser: Return Job ID\n        Browser--&gt;&gt;User: Show Success\n    else Validation Failed\n        FastAPI--&gt;&gt;Browser: Return Error\n        Browser--&gt;&gt;User: Show Error\n    end</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#api-route-structure","level":3,"title":"API Route Structure","text":"<pre><code># Core API routes\n@app.post(\"/api/submit-job\")\nasync def submit_job(\n    workflow_type: str = Form(...),\n    dem_file: UploadFile = File(...),\n    step: float = Form(15.0),\n    linke_value: float = Form(3.0),\n    albedo_value: float = Form(0.2),\n    num_threads: int = Form(4),\n    background_tasks: BackgroundTasks = BackgroundTasks()\n):\n    # Validate input\n    job_id = str(uuid.uuid4())\n\n    # Save uploaded file\n    file_path = save_upload(dem_file, job_id)\n\n    # Create job record\n    create_job_record(job_id, workflow_type, parameters)\n\n    # Start workflow in background\n    background_tasks.add_task(\n        workflow_manager.execute_workflow,\n        job_id, workflow_type, file_path, parameters\n    )\n\n    return {\"job_id\": job_id, \"status\": \"submitted\"}\n\n@app.get(\"/api/jobs\")\nasync def list_jobs():\n    return get_all_jobs()\n\n@app.get(\"/api/jobs/{job_id}\")\nasync def get_job(job_id: str):\n    job = get_job_by_id(job_id)\n    if not job:\n        raise HTTPException(404, \"Job not found\")\n    return job\n\n@app.get(\"/api/jobs/{job_id}/results\")\nasync def download_results(job_id: str):\n    results_path = f\"results/{job_id}\"\n    if not os.path.exists(results_path):\n        raise HTTPException(404, \"Results not found\")\n\n    zip_path = create_results_archive(job_id)\n    return FileResponse(zip_path, media_type='application/zip')\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#workflow-manager-architecture","level":2,"title":"Workflow Manager Architecture","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#container-orchestration","level":3,"title":"Container Orchestration","text":"<p>The Workflow Manager is responsible for container lifecycle management:</p> <pre><code>class WorkflowManager:\n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.docker_client = docker.from_env()\n        self.image_name = \"eemt:ubuntu24.04\"\n\n    async def execute_workflow(\n        self,\n        job_id: str,\n        workflow_type: str,\n        dem_path: Path,\n        parameters: dict\n    ):\n        \"\"\"Execute workflow in container\"\"\"\n\n        # Prepare container configuration\n        container_config = self._prepare_container_config(\n            job_id, workflow_type, dem_path, parameters\n        )\n\n        # Start container\n        container = self._start_container(container_config)\n\n        # Monitor execution\n        await self._monitor_container(container, job_id)\n\n        # Collect results\n        self._collect_results(container, job_id)\n\n        # Cleanup\n        self._cleanup_container(container)\n\n    def _prepare_container_config(self, job_id, workflow_type, dem_path, parameters):\n        \"\"\"Prepare Docker container configuration\"\"\"\n\n        # Base configuration\n        config = {\n            'image': self.image_name,\n            'name': f'eemt-job-{job_id}',\n            'detach': True,\n            'remove': False,\n            'volumes': {\n                str(self.base_dir / 'uploads'): {\n                    'bind': '/data/input',\n                    'mode': 'ro'\n                },\n                str(self.base_dir / 'results' / job_id): {\n                    'bind': '/data/output',\n                    'mode': 'rw'\n                },\n                str(self.base_dir / 'temp'): {\n                    'bind': '/tmp/eemt',\n                    'mode': 'rw'\n                }\n            },\n            'environment': {\n                'JOB_ID': job_id,\n                'WORKFLOW_TYPE': workflow_type,\n                **self._parameters_to_env(parameters)\n            },\n            'cpu_quota': parameters.get('num_threads', 4) * 100000,\n            'mem_limit': '8g'\n        }\n\n        # Workflow-specific command\n        if workflow_type == 'sol':\n            config['command'] = self._build_solar_command(parameters)\n        else:\n            config['command'] = self._build_eemt_command(parameters)\n\n        return config\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#container-monitoring","level":3,"title":"Container Monitoring","text":"<pre><code>class ContainerMonitor:\n    def __init__(self, docker_client):\n        self.docker_client = docker_client\n        self.active_containers = {}\n\n    async def monitor_container(self, container, job_id):\n        \"\"\"Monitor container execution and update progress\"\"\"\n\n        # Track container\n        self.active_containers[job_id] = container\n\n        try:\n            # Update job status to running\n            update_job_status(job_id, 'running')\n\n            # Stream logs and parse progress\n            for line in container.logs(stream=True, follow=True):\n                progress = self._parse_progress(line)\n                if progress:\n                    update_job_progress(job_id, progress)\n\n            # Wait for completion\n            result = container.wait()\n\n            if result['StatusCode'] == 0:\n                update_job_status(job_id, 'completed')\n            else:\n                error_msg = self._get_error_message(container)\n                update_job_status(job_id, 'failed', error_msg)\n\n        except Exception as e:\n            update_job_status(job_id, 'failed', str(e))\n\n        finally:\n            # Cleanup\n            del self.active_containers[job_id]\n\n    def _parse_progress(self, log_line):\n        \"\"\"Extract progress from container logs\"\"\"\n\n        # Parse progress indicators from workflow output\n        if b'Processing day' in log_line:\n            match = re.search(rb'Processing day (\\d+) of 365', log_line)\n            if match:\n                day = int(match.group(1))\n                return int((day / 365) * 100)\n\n        return None\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#database-architecture","level":2,"title":"Database Architecture","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#schema-design","level":3,"title":"Schema Design","text":"<pre><code>-- Jobs table\nCREATE TABLE jobs (\n    id TEXT PRIMARY KEY,\n    workflow_type TEXT NOT NULL,\n    status TEXT NOT NULL DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    parameters JSON,\n    dem_filename TEXT,\n    container_id TEXT,\n    error_message TEXT,\n    progress INTEGER DEFAULT 0,\n\n    CHECK (status IN ('pending', 'running', 'completed', 'failed')),\n    CHECK (workflow_type IN ('sol', 'eemt'))\n);\n\n-- Job logs table\nCREATE TABLE job_logs (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    job_id TEXT NOT NULL,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    level TEXT,\n    message TEXT,\n\n    FOREIGN KEY (job_id) REFERENCES jobs(id)\n);\n\n-- Metrics table\nCREATE TABLE metrics (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    job_id TEXT NOT NULL,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    cpu_usage REAL,\n    memory_usage INTEGER,\n    disk_io INTEGER,\n\n    FOREIGN KEY (job_id) REFERENCES jobs(id)\n);\n\n-- Indexes for performance\nCREATE INDEX idx_jobs_status ON jobs(status);\nCREATE INDEX idx_jobs_created ON jobs(created_at);\nCREATE INDEX idx_logs_job ON job_logs(job_id);\nCREATE INDEX idx_metrics_job ON metrics(job_id);\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#database-access-layer","level":3,"title":"Database Access Layer","text":"<pre><code>class JobDatabase:\n    def __init__(self, db_path='jobs.db'):\n        self.db_path = db_path\n\n    def create_job(self, job_id, workflow_type, parameters, dem_filename):\n        \"\"\"Create new job record\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute('''\n                INSERT INTO jobs (id, workflow_type, parameters, dem_filename)\n                VALUES (?, ?, ?, ?)\n            ''', (job_id, workflow_type, json.dumps(parameters), dem_filename))\n            conn.commit()\n\n    def update_job_status(self, job_id, status, error_message=None):\n        \"\"\"Update job status\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n\n            if status == 'running':\n                cursor.execute('''\n                    UPDATE jobs \n                    SET status = ?, started_at = CURRENT_TIMESTAMP\n                    WHERE id = ?\n                ''', (status, job_id))\n\n            elif status in ['completed', 'failed']:\n                cursor.execute('''\n                    UPDATE jobs \n                    SET status = ?, completed_at = CURRENT_TIMESTAMP, \n                        error_message = ?\n                    WHERE id = ?\n                ''', (status, error_message, job_id))\n\n            else:\n                cursor.execute('''\n                    UPDATE jobs SET status = ? WHERE id = ?\n                ''', (status, job_id))\n\n            conn.commit()\n\n    def get_job(self, job_id):\n        \"\"\"Get job by ID\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.row_factory = sqlite3.Row\n            cursor = conn.cursor()\n            cursor.execute('SELECT * FROM jobs WHERE id = ?', (job_id,))\n            row = cursor.fetchone()\n            return dict(row) if row else None\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#frontend-architecture","level":2,"title":"Frontend Architecture","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#html-templates","level":3,"title":"HTML Templates","text":"<p>The frontend uses Jinja2 templates with Bootstrap for styling:</p> <pre><code>&lt;!-- templates/base.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{% block title %}EEMT Web Interface{% endblock %}&lt;/title&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;\n    &lt;link href=\"{{ url_for('static', path='/css/style.css') }}\" rel=\"stylesheet\"&gt;\n    {% block head %}{% endblock %}\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;nav class=\"navbar navbar-dark bg-primary\"&gt;\n        &lt;div class=\"container-fluid\"&gt;\n            &lt;a class=\"navbar-brand\" href=\"/\"&gt;EEMT Web Interface&lt;/a&gt;\n            &lt;ul class=\"navbar-nav ms-auto\"&gt;\n                &lt;li class=\"nav-item\"&gt;\n                    &lt;a class=\"nav-link\" href=\"/monitor\"&gt;Monitor Jobs&lt;/a&gt;\n                &lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n    &lt;/nav&gt;\n\n    &lt;div class=\"container mt-4\"&gt;\n        {% block content %}{% endblock %}\n    &lt;/div&gt;\n\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"{{ url_for('static', path='/js/main.js') }}\"&gt;&lt;/script&gt;\n    {% block scripts %}{% endblock %}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#javascript-architecture","level":3,"title":"JavaScript Architecture","text":"<pre><code>// static/js/monitor.js\nclass JobMonitor {\n    constructor() {\n        this.jobs = new Map();\n        this.updateInterval = 5000; // 5 seconds\n        this.intervalId = null;\n    }\n\n    async start() {\n        // Initial load\n        await this.updateJobs();\n\n        // Start periodic updates\n        this.intervalId = setInterval(() =&gt; {\n            this.updateJobs();\n        }, this.updateInterval);\n    }\n\n    async updateJobs() {\n        try {\n            const response = await fetch('/api/jobs');\n            const jobs = await response.json();\n\n            // Update job list\n            this.renderJobs(jobs);\n\n            // Update statistics\n            this.updateStatistics(jobs);\n\n        } catch (error) {\n            console.error('Failed to update jobs:', error);\n        }\n    }\n\n    renderJobs(jobs) {\n        const tbody = document.getElementById('job-table-body');\n        tbody.innerHTML = '';\n\n        jobs.forEach(job =&gt; {\n            const row = this.createJobRow(job);\n            tbody.appendChild(row);\n        });\n    }\n\n    createJobRow(job) {\n        const row = document.createElement('tr');\n        row.innerHTML = `\n            &lt;td&gt;${job.id.substring(0, 8)}&lt;/td&gt;\n            &lt;td&gt;${job.workflow_type.toUpperCase()}&lt;/td&gt;\n            &lt;td&gt;${this.renderStatus(job.status)}&lt;/td&gt;\n            &lt;td&gt;${this.renderProgress(job.progress)}&lt;/td&gt;\n            &lt;td&gt;${new Date(job.created_at).toLocaleString()}&lt;/td&gt;\n            &lt;td&gt;${this.renderActions(job)}&lt;/td&gt;\n        `;\n        return row;\n    }\n\n    renderStatus(status) {\n        const badges = {\n            'pending': 'badge bg-secondary',\n            'running': 'badge bg-primary',\n            'completed': 'badge bg-success',\n            'failed': 'badge bg-danger'\n        };\n\n        return `&lt;span class=\"${badges[status]}\"&gt;${status}&lt;/span&gt;`;\n    }\n\n    renderProgress(progress) {\n        return `\n            &lt;div class=\"progress\"&gt;\n                &lt;div class=\"progress-bar\" style=\"width: ${progress}%\"&gt;\n                    ${progress}%\n                &lt;/div&gt;\n            &lt;/div&gt;\n        `;\n    }\n}\n\n// Initialize monitor on page load\ndocument.addEventListener('DOMContentLoaded', () =&gt; {\n    const monitor = new JobMonitor();\n    monitor.start();\n});\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#storage-management","level":2,"title":"Storage Management","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#volume-architecture","level":3,"title":"Volume Architecture","text":"<pre><code>graph LR\n    subgraph \"Host Filesystem\"\n        HD[Host Data Directory]\n    end\n\n    subgraph \"Docker Volumes\"\n        UV[Uploads Volume]\n        RV[Results Volume]\n        TV[Temp Volume]\n        CV[Cache Volume]\n    end\n\n    subgraph \"Container Mounts\"\n        CI[/data/input]\n        CO[/data/output]\n        CT[/tmp/eemt]\n        CC[/data/cache]\n    end\n\n    HD --&gt; UV\n    HD --&gt; RV\n    HD --&gt; TV\n    HD --&gt; CV\n\n    UV --&gt; CI\n    RV --&gt; CO\n    TV --&gt; CT\n    CV --&gt; CC</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#file-management","level":3,"title":"File Management","text":"<pre><code>class FileManager:\n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.uploads_dir = base_dir / 'uploads'\n        self.results_dir = base_dir / 'results'\n        self.temp_dir = base_dir / 'temp'\n        self.cache_dir = base_dir / 'cache'\n\n        # Ensure directories exist\n        for dir_path in [self.uploads_dir, self.results_dir, \n                         self.temp_dir, self.cache_dir]:\n            dir_path.mkdir(parents=True, exist_ok=True)\n\n    async def save_upload(self, file: UploadFile, job_id: str) -&gt; Path:\n        \"\"\"Save uploaded file\"\"\"\n        file_path = self.uploads_dir / f\"{job_id}_{file.filename}\"\n\n        async with aiofiles.open(file_path, 'wb') as f:\n            content = await file.read()\n            await f.write(content)\n\n        return file_path\n\n    def create_results_archive(self, job_id: str) -&gt; Path:\n        \"\"\"Create ZIP archive of results\"\"\"\n        results_path = self.results_dir / job_id\n        archive_path = self.temp_dir / f\"results_{job_id}.zip\"\n\n        with zipfile.ZipFile(archive_path, 'w') as zipf:\n            for root, dirs, files in os.walk(results_path):\n                for file in files:\n                    file_path = Path(root) / file\n                    arcname = file_path.relative_to(results_path)\n                    zipf.write(file_path, arcname)\n\n        return archive_path\n\n    def cleanup_job_files(self, job_id: str, keep_results=False):\n        \"\"\"Clean up job-related files\"\"\"\n\n        # Remove upload\n        upload_files = self.uploads_dir.glob(f\"{job_id}_*\")\n        for file in upload_files:\n            file.unlink()\n\n        # Remove temp files\n        temp_files = self.temp_dir.glob(f\"*{job_id}*\")\n        for file in temp_files:\n            file.unlink()\n\n        # Optionally remove results\n        if not keep_results:\n            results_path = self.results_dir / job_id\n            if results_path.exists():\n                shutil.rmtree(results_path)\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#security-architecture","level":2,"title":"Security Architecture","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#input-validation","level":3,"title":"Input Validation","text":"<pre><code>from pydantic import BaseModel, Field, validator\n\nclass WorkflowParameters(BaseModel):\n    workflow_type: str = Field(..., regex='^(sol|eemt)$')\n    step: float = Field(15.0, ge=3.0, le=60.0)\n    linke_value: float = Field(3.0, ge=1.0, le=8.0)\n    albedo_value: float = Field(0.2, ge=0.0, le=1.0)\n    num_threads: int = Field(4, ge=1, le=32)\n    start_year: Optional[int] = Field(None, ge=1980, le=2024)\n    end_year: Optional[int] = Field(None, ge=1980, le=2024)\n\n    @validator('end_year')\n    def validate_year_range(cls, v, values):\n        if v and 'start_year' in values:\n            if v &lt; values['start_year']:\n                raise ValueError('End year must be &gt;= start year')\n        return v\n\nclass FileValidator:\n    @staticmethod\n    def validate_dem_file(file: UploadFile) -&gt; bool:\n        \"\"\"Validate uploaded DEM file\"\"\"\n\n        # Check file extension\n        if not file.filename.lower().endswith(('.tif', '.tiff')):\n            raise ValueError(\"File must be GeoTIFF format\")\n\n        # Check file size (max 1GB)\n        if file.size &gt; 1024 * 1024 * 1024:\n            raise ValueError(\"File size exceeds 1GB limit\")\n\n        # Verify GDAL can read it\n        # (Implementation would check file headers)\n\n        return True\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#authentication-authorization-future","level":3,"title":"Authentication &amp; Authorization (Future)","text":"<pre><code>from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\nsecurity = HTTPBearer()\n\nasync def verify_token(credentials: HTTPAuthorizationCredentials):\n    \"\"\"Verify JWT token\"\"\"\n    token = credentials.credentials\n\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n        return payload\n    except jwt.InvalidTokenError:\n        raise HTTPException(401, \"Invalid token\")\n\n@app.post(\"/api/submit-job\")\nasync def submit_job(\n    credentials: HTTPAuthorizationCredentials = Security(security),\n    ...\n):\n    user = await verify_token(credentials)\n    # Process job with user context\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#performance-considerations","level":2,"title":"Performance Considerations","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#async-operations","level":3,"title":"Async Operations","text":"<pre><code>class AsyncWorkflowManager:\n    def __init__(self):\n        self.executor = ThreadPoolExecutor(max_workers=10)\n        self.loop = asyncio.get_event_loop()\n\n    async def execute_workflow_async(self, job_id, params):\n        \"\"\"Execute workflow asynchronously\"\"\"\n\n        # Run blocking Docker operations in thread pool\n        container = await self.loop.run_in_executor(\n            self.executor,\n            self._start_container_sync,\n            job_id, params\n        )\n\n        # Monitor asynchronously\n        await self._monitor_container_async(container, job_id)\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#caching-strategy","level":3,"title":"Caching Strategy","text":"<pre><code>from functools import lru_cache\nimport redis\n\nclass CacheManager:\n    def __init__(self):\n        self.redis_client = redis.Redis(\n            host='localhost', \n            port=6379, \n            decode_responses=True\n        )\n\n    @lru_cache(maxsize=128)\n    def get_cached_result(self, cache_key: str):\n        \"\"\"Get cached computation result\"\"\"\n        return self.redis_client.get(cache_key)\n\n    def cache_result(self, cache_key: str, data: dict, ttl=3600):\n        \"\"\"Cache computation result\"\"\"\n        self.redis_client.setex(\n            cache_key, \n            ttl, \n            json.dumps(data)\n        )\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#monitoring-observability","level":2,"title":"Monitoring &amp; Observability","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#metrics-collection","level":3,"title":"Metrics Collection","text":"<pre><code>from prometheus_client import Counter, Histogram, Gauge\n\n# Define metrics\njob_submissions = Counter('eemt_job_submissions_total', \n                          'Total job submissions',\n                          ['workflow_type'])\njob_duration = Histogram('eemt_job_duration_seconds',\n                        'Job execution duration',\n                        ['workflow_type'])\nactive_jobs = Gauge('eemt_active_jobs',\n                   'Currently active jobs')\n\n# Use in application\n@app.post(\"/api/submit-job\")\nasync def submit_job(...):\n    job_submissions.labels(workflow_type=workflow_type).inc()\n    active_jobs.inc()\n    # ...\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#health-checks","level":3,"title":"Health Checks","text":"<pre><code>@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Application health check endpoint\"\"\"\n\n    checks = {\n        'status': 'healthy',\n        'timestamp': datetime.utcnow().isoformat(),\n        'checks': {}\n    }\n\n    # Check Docker availability\n    try:\n        docker_client = docker.from_env()\n        docker_client.ping()\n        checks['checks']['docker'] = 'ok'\n    except:\n        checks['checks']['docker'] = 'failed'\n        checks['status'] = 'degraded'\n\n    # Check database\n    try:\n        conn = sqlite3.connect('jobs.db')\n        conn.execute('SELECT 1')\n        conn.close()\n        checks['checks']['database'] = 'ok'\n    except:\n        checks['checks']['database'] = 'failed'\n        checks['status'] = 'unhealthy'\n\n    # Check disk space\n    disk_usage = shutil.disk_usage('/')\n    if disk_usage.free &lt; 1024 * 1024 * 1024:  # Less than 1GB\n        checks['checks']['disk'] = 'low'\n        checks['status'] = 'degraded'\n    else:\n        checks['checks']['disk'] = 'ok'\n\n    return checks\n</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#future-enhancements","level":2,"title":"Future Enhancements","text":"","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#planned-features","level":3,"title":"Planned Features","text":"<ol> <li>WebSocket Support: Real-time progress updates</li> <li>User Authentication: JWT-based auth system</li> <li>Job Queuing: Advanced queue management with priorities</li> <li>Result Visualization: In-browser GeoTIFF viewing</li> <li>Workflow Templates: Pre-configured parameter sets</li> <li>API Rate Limiting: Request throttling</li> <li>Distributed Tracing: OpenTelemetry integration</li> </ol>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#architecture-evolution","level":3,"title":"Architecture Evolution","text":"<pre><code>graph TD\n    subgraph \"Current Architecture\"\n        A1[Monolithic FastAPI]\n        A2[SQLite]\n        A3[Local Docker]\n    end\n\n    subgraph \"Future Architecture\"\n        B1[API Gateway]\n        B2[Microservices]\n        B3[PostgreSQL]\n        B4[Redis Cache]\n        B5[Message Queue]\n        B6[Kubernetes]\n    end\n\n    A1 --&gt; B1\n    A1 --&gt; B2\n    A2 --&gt; B3\n    A3 --&gt; B6\n    B2 --&gt; B4\n    B2 --&gt; B5</code></pre>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/architecture/#related-documentation","level":2,"title":"Related Documentation","text":"<ul> <li>Web Interface User Guide</li> <li>API Reference</li> <li>Container Architecture</li> <li>Docker Deployment</li> </ul>","path":["Web Interface","Web Interface Architecture"],"tags":[]},{"location":"web-interface/job-cleanup/","level":1,"title":"Job Data Cleanup System","text":"<p>The EEMT web interface includes an automated job data cleanup system to manage disk space by removing old job data while preserving job metadata for auditing and analysis.</p>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#overview","level":2,"title":"Overview","text":"<p>The cleanup system automatically manages job data retention with configurable policies:</p> <ul> <li>Successful Jobs: Output data deleted after 7 days, job configurations preserved indefinitely</li> <li>Failed Jobs: All data deleted after 12 hours to free resources quickly</li> <li>Job Metadata: Always preserved in the database for historical analysis</li> <li>Configurable Retention: Adjust periods via environment variables</li> <li>Manual Cleanup: API endpoint and CLI script for on-demand cleanup</li> </ul> <p>Data Retention Philosophy</p> <p>The cleanup system balances disk space management with data preservation needs:</p> <ul> <li>Job configurations and metadata are always retained for analysis</li> <li>Output data for successful jobs is kept long enough for users to download</li> <li>Failed job data is cleaned quickly as it's typically not needed</li> <li>All retention periods are configurable to match your requirements</li> </ul>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#architecture","level":2,"title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"Cleanup Components\"\n        CS[cleanup_jobs.py&lt;br/&gt;Standalone Script]\n        API[/api/cleanup&lt;br/&gt;REST Endpoint]\n        CRON[Cron Job&lt;br/&gt;Automated Schedule]\n        SD[Systemd Timer&lt;br/&gt;Alternative Scheduler]\n    end\n\n    subgraph \"Data Storage\"\n        DB[(SQLite&lt;br/&gt;Job Database)]\n        UP[uploads/&lt;br/&gt;Input DEMs]\n        RES[results/&lt;br/&gt;Output Data]\n        TEMP[temp/&lt;br/&gt;Working Files]\n    end\n\n    subgraph \"Cleanup Process\"\n        SCAN[Scan Jobs]\n        CHECK[Check Age]\n        DEL[Delete Data]\n        LOG[Update Logs]\n    end\n\n    CS --&gt; SCAN\n    API --&gt; SCAN\n    CRON --&gt; CS\n    SD --&gt; CS\n\n    SCAN --&gt; CHECK\n    CHECK --&gt; DEL\n    DEL --&gt; LOG\n\n    DEL --&gt; UP\n    DEL --&gt; RES\n    DEL --&gt; TEMP\n    LOG --&gt; DB</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#retention-policies","level":2,"title":"Retention Policies","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#default-policies","level":3,"title":"Default Policies","text":"Job Status Data Type Retention Period Action Successful Output data 7 days Delete files, keep DB record Successful Input DEM 7 days Delete file if exists Successful Job metadata Forever Preserved in database Failed All data 12 hours Delete all files Failed Job metadata Forever Preserved with error info Running All data N/A Never deleted automatically Pending All data N/A Never deleted automatically","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#configurable-retention","level":3,"title":"Configurable Retention","text":"<p>Adjust retention periods using environment variables:</p> <pre><code># Set retention periods (in hours)\nexport EEMT_SUCCESS_RETENTION_HOURS=168  # 7 days (default)\nexport EEMT_FAILED_RETENTION_HOURS=12    # 12 hours (default)\n\n# Examples of common configurations\nexport EEMT_SUCCESS_RETENTION_HOURS=336  # 14 days for longer retention\nexport EEMT_SUCCESS_RETENTION_HOURS=72   # 3 days for rapid cleanup\nexport EEMT_FAILED_RETENTION_HOURS=24    # 24 hours for debugging\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#cleanup-script-usage","level":2,"title":"Cleanup Script Usage","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#standalone-script-cleanup_jobspy","level":3,"title":"Standalone Script (<code>cleanup_jobs.py</code>)","text":"<p>The cleanup script can be run manually or scheduled:</p>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#basic-usage","level":4,"title":"Basic Usage","text":"<pre><code># Run cleanup with default settings\npython scripts/cleanup_jobs.py\n\n# Specify custom database and directories\npython scripts/cleanup_jobs.py \\\n    --db-path /path/to/jobs.db \\\n    --uploads-dir /path/to/uploads \\\n    --results-dir /path/to/results\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#command-line-options","level":4,"title":"Command-Line Options","text":"Option Description Default <code>--db-path</code> Path to SQLite database <code>./jobs.db</code> <code>--uploads-dir</code> Directory containing uploaded DEMs <code>./uploads</code> <code>--results-dir</code> Directory containing job outputs <code>./results</code> <code>--success-retention</code> Hours to keep successful job data 168 (7 days) <code>--failed-retention</code> Hours to keep failed job data 12 <code>--dry-run</code> Preview cleanup without deleting False <code>--verbose</code> Enable detailed logging False","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#dry-run-mode","level":4,"title":"Dry Run Mode","text":"<p>Preview what would be deleted without making changes:</p> <pre><code># See what would be cleaned up\npython scripts/cleanup_jobs.py --dry-run --verbose\n\n# Example output\n[DRY RUN] Would clean job job-20240115-123456 (status: completed, age: 8.2 days)\n[DRY RUN] Would delete: uploads/job-20240115-123456_dem.tif\n[DRY RUN] Would delete: results/job-20240115-123456/ (15 GB)\n[DRY RUN] Summary: Would clean 3 jobs, freeing ~45 GB\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#verbose-output","level":4,"title":"Verbose Output","text":"<p>Get detailed information during cleanup:</p> <pre><code>python scripts/cleanup_jobs.py --verbose\n\n# Example output\n[INFO] Starting job cleanup process...\n[INFO] Checking job job-20240115-123456 (completed, 8 days old)\n[INFO] Deleting uploads/job-20240115-123456_dem.tif (250 MB)\n[INFO] Deleting results/job-20240115-123456/ (15 GB)\n[INFO] Updated database: marked job as cleaned\n[INFO] Cleaned 3 successful jobs (freed 45 GB)\n[INFO] Cleaned 2 failed jobs (freed 500 MB)\n[INFO] Cleanup completed successfully\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#automated-scheduling","level":2,"title":"Automated Scheduling","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#cron-setup","level":3,"title":"Cron Setup","text":"<p>Use the provided setup script for automatic cron configuration:</p> <pre><code># Install cron job (runs daily at 2 AM)\n./scripts/setup_cleanup_cron.sh install\n\n# Verify installation\n./scripts/setup_cleanup_cron.sh status\n\n# Remove cron job\n./scripts/setup_cleanup_cron.sh remove\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#manual-cron-configuration","level":4,"title":"Manual Cron Configuration","text":"<p>For custom scheduling, edit crontab directly:</p> <pre><code># Edit crontab\ncrontab -e\n\n# Add cleanup job (adjust schedule as needed)\n# Daily at 2:00 AM\n0 2 * * * cd /path/to/eemt/web-interface &amp;&amp; /usr/bin/python3 scripts/cleanup_jobs.py &gt;&gt; logs/cleanup.log 2&gt;&amp;1\n\n# Every 6 hours\n0 */6 * * * cd /path/to/eemt/web-interface &amp;&amp; /usr/bin/python3 scripts/cleanup_jobs.py\n\n# Weekly on Sunday at 3 AM\n0 3 * * 0 cd /path/to/eemt/web-interface &amp;&amp; /usr/bin/python3 scripts/cleanup_jobs.py\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#systemd-timer-setup","level":3,"title":"Systemd Timer Setup","text":"<p>For systems using systemd, use timer units for more control:</p> <pre><code># Install systemd timer and service\n./scripts/setup_cleanup_cron.sh install-systemd\n\n# Check timer status\nsystemctl --user status eemt-cleanup.timer\n\n# View cleanup logs\njournalctl --user -u eemt-cleanup.service\n\n# Manual trigger\nsystemctl --user start eemt-cleanup.service\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#systemd-configuration-files","level":4,"title":"Systemd Configuration Files","text":"<p>The setup script creates these systemd files:</p> <p><code>~/.config/systemd/user/eemt-cleanup.service</code>: <pre><code>[Unit]\nDescription=EEMT Job Cleanup Service\nAfter=network.target\n\n[Service]\nType=oneshot\nWorkingDirectory=/path/to/eemt/web-interface\nExecStart=/usr/bin/python3 scripts/cleanup_jobs.py\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=default.target\n</code></pre></p> <p><code>~/.config/systemd/user/eemt-cleanup.timer</code>: <pre><code>[Unit]\nDescription=Daily EEMT Job Cleanup Timer\nRequires=eemt-cleanup.service\n\n[Timer]\nOnCalendar=daily\nOnCalendar=02:00\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre></p>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#docker-integration","level":2,"title":"Docker Integration","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#docker-compose-configuration","level":3,"title":"Docker Compose Configuration","text":"<p>The cleanup system integrates with Docker deployments:</p> <pre><code># docker-compose.yml\nservices:\n  eemt-web:\n    image: eemt-web:latest\n    environment:\n      - EEMT_SUCCESS_RETENTION_HOURS=168\n      - EEMT_FAILED_RETENTION_HOURS=12\n      - EEMT_ENABLE_AUTO_CLEANUP=true\n      - EEMT_CLEANUP_SCHEDULE=\"0 2 * * *\"  # Cron expression\n    volumes:\n      - ./data/uploads:/app/uploads\n      - ./data/results:/app/results\n      - ./jobs.db:/app/jobs.db\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#container-cleanup-script","level":3,"title":"Container Cleanup Script","text":"<p>Run cleanup inside the web interface container:</p> <pre><code># Execute cleanup in running container\ndocker exec eemt-web python scripts/cleanup_jobs.py\n\n# Run cleanup in new container\ndocker run --rm \\\n  -v $(pwd)/uploads:/app/uploads \\\n  -v $(pwd)/results:/app/results \\\n  -v $(pwd)/jobs.db:/app/jobs.db \\\n  eemt-web python scripts/cleanup_jobs.py\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#kubernetes-cronjob","level":3,"title":"Kubernetes CronJob","text":"<p>For Kubernetes deployments, use a CronJob resource:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: eemt-cleanup\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: eemt-web:latest\n            command:\n            - python\n            - scripts/cleanup_jobs.py\n            env:\n            - name: EEMT_SUCCESS_RETENTION_HOURS\n              value: \"168\"\n            - name: EEMT_FAILED_RETENTION_HOURS\n              value: \"12\"\n            volumeMounts:\n            - name: data\n              mountPath: /app/data\n          volumes:\n          - name: data\n            persistentVolumeClaim:\n              claimName: eemt-data-pvc\n          restartPolicy: OnFailure\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#api-endpoint","level":2,"title":"API Endpoint","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#rest-api-for-manual-cleanup","level":3,"title":"REST API for Manual Cleanup","text":"<p>Trigger cleanup via the web interface API:</p>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#endpoint-details","level":4,"title":"Endpoint Details","text":"<pre><code>POST /api/cleanup\nContent-Type: application/json\n\n{\n  \"dry_run\": false,\n  \"success_retention_hours\": 168,\n  \"failed_retention_hours\": 12\n}\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#response-format","level":4,"title":"Response Format","text":"<pre><code>{\n  \"success\": true,\n  \"cleaned_jobs\": {\n    \"successful\": 3,\n    \"failed\": 2\n  },\n  \"space_freed\": {\n    \"bytes\": 48318382080,\n    \"human_readable\": \"45.0 GB\"\n  },\n  \"details\": [\n    {\n      \"job_id\": \"job-20240115-123456\",\n      \"status\": \"completed\",\n      \"age_days\": 8.2,\n      \"space_freed\": \"15.0 GB\"\n    }\n  ]\n}\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#usage-examples","level":4,"title":"Usage Examples","text":"<pre><code># Trigger cleanup with defaults\ncurl -X POST http://localhost:5000/api/cleanup\n\n# Dry run to preview\ncurl -X POST http://localhost:5000/api/cleanup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"dry_run\": true}'\n\n# Custom retention periods\ncurl -X POST http://localhost:5000/api/cleanup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"success_retention_hours\": 72,\n    \"failed_retention_hours\": 6\n  }'\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#python-client-example","level":3,"title":"Python Client Example","text":"<pre><code>import requests\nfrom datetime import datetime, timedelta\n\ndef cleanup_old_jobs(base_url=\"http://localhost:5000\", dry_run=True):\n    \"\"\"Trigger job cleanup via API\"\"\"\n\n    response = requests.post(\n        f\"{base_url}/api/cleanup\",\n        json={\n            \"dry_run\": dry_run,\n            \"success_retention_hours\": 168,  # 7 days\n            \"failed_retention_hours\": 12     # 12 hours\n        }\n    )\n\n    if response.status_code == 200:\n        result = response.json()\n        print(f\"Cleanup {'preview' if dry_run else 'completed'}:\")\n        print(f\"  Successful jobs cleaned: {result['cleaned_jobs']['successful']}\")\n        print(f\"  Failed jobs cleaned: {result['cleaned_jobs']['failed']}\")\n        print(f\"  Space freed: {result['space_freed']['human_readable']}\")\n    else:\n        print(f\"Cleanup failed: {response.text}\")\n\n# Preview cleanup\ncleanup_old_jobs(dry_run=True)\n\n# Execute cleanup\ncleanup_old_jobs(dry_run=False)\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#configuration-options","level":2,"title":"Configuration Options","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#environment-variables","level":3,"title":"Environment Variables","text":"<p>Configure cleanup behavior through environment variables:</p> Variable Description Default <code>EEMT_SUCCESS_RETENTION_HOURS</code> Hours to keep successful job data 168 (7 days) <code>EEMT_FAILED_RETENTION_HOURS</code> Hours to keep failed job data 12 <code>EEMT_ENABLE_AUTO_CLEANUP</code> Enable automatic cleanup on schedule false <code>EEMT_CLEANUP_SCHEDULE</code> Cron expression for auto cleanup \"0 2 * * *\" <code>EEMT_CLEANUP_LOG_LEVEL</code> Logging verbosity (DEBUG, INFO, WARNING, ERROR) INFO <code>EEMT_CLEANUP_BATCH_SIZE</code> Number of jobs to process per batch 100 <code>EEMT_CLEANUP_DRY_RUN</code> Always run in dry-run mode false","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#configuration-file","level":3,"title":"Configuration File","text":"<p>Create a configuration file for complex setups:</p> <p><code>cleanup_config.yaml</code>: <pre><code># Retention policies\nretention:\n  successful_jobs:\n    hours: 168  # 7 days\n    keep_metadata: true\n    keep_logs: false\n  failed_jobs:\n    hours: 12\n    keep_metadata: true\n    keep_error_logs: true\n\n# Cleanup schedule\nschedule:\n  enabled: true\n  cron: \"0 2 * * *\"  # Daily at 2 AM\n\n# Directories\npaths:\n  database: ./jobs.db\n  uploads: ./uploads\n  results: ./results\n  logs: ./logs\n\n# Performance\nperformance:\n  batch_size: 100\n  parallel_delete: true\n  max_workers: 4\n\n# Notifications (optional)\nnotifications:\n  email:\n    enabled: false\n    smtp_server: smtp.gmail.com\n    recipients:\n      - admin@example.com\n  slack:\n    enabled: false\n    webhook_url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL\n</code></pre></p> <p>Load configuration in cleanup script:</p> <pre><code>import yaml\n\nwith open('cleanup_config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# Use configuration\nsuccess_retention = config['retention']['successful_jobs']['hours']\nfailed_retention = config['retention']['failed_jobs']['hours']\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#monitoring-and-logging","level":2,"title":"Monitoring and Logging","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#log-files","level":3,"title":"Log Files","text":"<p>Cleanup operations are logged to multiple locations:</p> <ol> <li>Application Logs: <code>logs/cleanup.log</code></li> <li>System Logs: <code>/var/log/syslog</code> (when using cron)</li> <li>Journal: <code>journalctl</code> (when using systemd)</li> <li>Container Logs: <code>docker logs eemt-web</code></li> </ol>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#log-format","level":3,"title":"Log Format","text":"<pre><code>2024-01-20 02:00:01 INFO: Starting job cleanup process\n2024-01-20 02:00:02 INFO: Found 45 jobs to check\n2024-01-20 02:00:03 INFO: Cleaning job job-20240113-123456 (completed, 7.5 days old)\n2024-01-20 02:00:05 INFO: Deleted uploads/job-20240113-123456_dem.tif (250 MB)\n2024-01-20 02:00:08 INFO: Deleted results/job-20240113-123456/ (15 GB)\n2024-01-20 02:00:09 INFO: Cleanup summary: 3 successful, 2 failed, 45.5 GB freed\n2024-01-20 02:00:09 INFO: Cleanup completed successfully\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#monitoring-metrics","level":3,"title":"Monitoring Metrics","text":"<p>Track cleanup effectiveness with these metrics:</p> <pre><code># Example monitoring script\nimport sqlite3\nfrom datetime import datetime, timedelta\n\ndef get_cleanup_metrics(db_path=\"jobs.db\"):\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Jobs cleaned in last 24 hours\n    yesterday = datetime.now() - timedelta(hours=24)\n    cursor.execute(\"\"\"\n        SELECT COUNT(*), SUM(data_size_mb) \n        FROM jobs \n        WHERE data_cleaned_at &gt; ?\n    \"\"\", (yesterday.isoformat(),))\n\n    count, size_mb = cursor.fetchone()\n\n    # Disk space usage\n    cursor.execute(\"\"\"\n        SELECT \n            status,\n            COUNT(*) as count,\n            SUM(data_size_mb) as total_mb\n        FROM jobs\n        WHERE data_cleaned_at IS NULL\n        GROUP BY status\n    \"\"\")\n\n    usage_by_status = cursor.fetchall()\n\n    print(f\"Cleanup Metrics (Last 24h):\")\n    print(f\"  Jobs cleaned: {count or 0}\")\n    print(f\"  Space freed: {(size_mb or 0) / 1024:.1f} GB\")\n    print(f\"\\nCurrent Usage:\")\n    for status, count, mb in usage_by_status:\n        print(f\"  {status}: {count} jobs, {mb/1024:.1f} GB\")\n\n    conn.close()\n\n# Run metrics check\nget_cleanup_metrics()\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#common-issues","level":3,"title":"Common Issues","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#1-cleanup-not-running-automatically","level":4,"title":"1. Cleanup Not Running Automatically","text":"<p>Check cron job: <pre><code># List cron jobs\ncrontab -l\n\n# Check cron service\nsystemctl status cron\n\n# View cron logs\ngrep CRON /var/log/syslog\n</code></pre></p> <p>Check systemd timer: <pre><code># Timer status\nsystemctl --user status eemt-cleanup.timer\n\n# List all timers\nsystemctl --user list-timers\n\n# View service logs\njournalctl --user -u eemt-cleanup.service -n 50\n</code></pre></p>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#2-permission-denied-errors","level":4,"title":"2. Permission Denied Errors","text":"<pre><code># Check file permissions\nls -la uploads/ results/\n\n# Fix ownership\nsudo chown -R $(whoami):$(whoami) uploads/ results/\n\n# Fix permissions\nchmod -R 755 uploads/ results/\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#3-database-locked-error","level":4,"title":"3. Database Locked Error","text":"<pre><code># Check for running processes\nfuser jobs.db\n\n# Kill blocking process if needed\nkill -9 &lt;PID&gt;\n\n# Verify database integrity\nsqlite3 jobs.db \"PRAGMA integrity_check;\"\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#4-disk-space-not-being-freed","level":4,"title":"4. Disk Space Not Being Freed","text":"<pre><code># Check if files are actually deleted\ndu -sh uploads/ results/\n\n# Check for open file handles\nlsof | grep results\n\n# Force filesystem sync\nsync\n\n# Check available space\ndf -h .\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#5-cleanup-taking-too-long","level":4,"title":"5. Cleanup Taking Too Long","text":"<p>Optimize cleanup performance:</p> <pre><code># Use parallel deletion\npython scripts/cleanup_jobs.py --parallel --workers 4\n\n# Process in smaller batches\npython scripts/cleanup_jobs.py --batch-size 50\n\n# Skip large directories first\npython scripts/cleanup_jobs.py --skip-large --size-threshold 10G\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#debug-mode","level":3,"title":"Debug Mode","text":"<p>Enable detailed debugging output:</p> <pre><code># Set debug environment variable\nexport EEMT_CLEANUP_LOG_LEVEL=DEBUG\n\n# Run with maximum verbosity\npython scripts/cleanup_jobs.py --verbose --debug\n\n# Debug output includes:\n# - SQL queries executed\n# - File operations attempted\n# - Time taken for each operation\n# - Memory usage statistics\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#recovery-procedures","level":3,"title":"Recovery Procedures","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#restore-accidentally-deleted-data","level":4,"title":"Restore Accidentally Deleted Data","text":"<p>If important data was deleted by cleanup:</p> <ol> <li> <p>Check backups (if configured):    <pre><code># Restore from backup\nrsync -av /backup/eemt/results/job-id/ ./results/job-id/\n</code></pre></p> </li> <li> <p>Recover from database:    <pre><code>-- Job metadata is preserved\nSELECT * FROM jobs WHERE job_id = 'job-20240120-123456';\n</code></pre></p> </li> <li> <p>Re-run job if necessary:    <pre><code># Use preserved configuration\npython scripts/rerun_job.py --job-id job-20240120-123456\n</code></pre></p> </li> </ol>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#reset-cleanup-state","level":4,"title":"Reset Cleanup State","text":"<p>Clear cleanup history and start fresh:</p> <pre><code>-- Reset cleanup timestamps\nUPDATE jobs SET data_cleaned_at = NULL WHERE data_cleaned_at IS NOT NULL;\n\n-- Clear cleanup log entries\nDELETE FROM cleanup_log;\n\n-- Vacuum database\nVACUUM;\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#best-practices","level":2,"title":"Best Practices","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#1-regular-monitoring","level":3,"title":"1. Regular Monitoring","text":"<p>Set up monitoring alerts:</p> <pre><code># Daily check script\n#!/bin/bash\nSPACE_USED=$(du -sb results/ | cut -f1)\nMAX_SPACE=107374182400  # 100 GB\n\nif [ $SPACE_USED -gt $MAX_SPACE ]; then\n    echo \"Warning: Results directory using $(($SPACE_USED / 1073741824)) GB\" | \\\n        mail -s \"EEMT Cleanup Alert\" admin@example.com\nfi\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#2-backup-important-results","level":3,"title":"2. Backup Important Results","text":"<p>Before cleanup, backup critical data:</p> <pre><code># Backup successful jobs before cleanup\nrsync -av --include=\"*completed*\" results/ /backup/eemt/results/\n\n# Archive old successful jobs\ntar -czf archived_jobs_$(date +%Y%m).tar.gz \\\n    $(find results/ -name \"*completed*\" -mtime +7)\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#3-gradual-cleanup","level":3,"title":"3. Gradual Cleanup","text":"<p>Implement staged cleanup for safety:</p> <pre><code># Two-stage cleanup process\ndef staged_cleanup():\n    # Stage 1: Mark for deletion\n    mark_jobs_for_cleanup(age_days=7)\n\n    # Stage 2: Delete marked jobs (24h later)\n    delete_marked_jobs(marked_before=24)\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#4-audit-trail","level":3,"title":"4. Audit Trail","text":"<p>Maintain cleanup audit logs:</p> <pre><code>-- Create audit table\nCREATE TABLE cleanup_audit (\n    id INTEGER PRIMARY KEY,\n    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n    job_id TEXT,\n    action TEXT,\n    space_freed_mb INTEGER,\n    user TEXT\n);\n\n-- Log cleanup actions\nINSERT INTO cleanup_audit (job_id, action, space_freed_mb, user)\nVALUES ('job-123', 'deleted_results', 15360, 'auto_cleanup');\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#5-capacity-planning","level":3,"title":"5. Capacity Planning","text":"<p>Monitor growth trends:</p> <pre><code>import matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\ndef plot_storage_trends():\n    # Get historical data\n    dates, usage = get_storage_history()\n\n    # Plot trend\n    plt.figure(figsize=(10, 6))\n    plt.plot(dates, usage)\n    plt.xlabel('Date')\n    plt.ylabel('Storage (GB)')\n    plt.title('EEMT Storage Usage Trend')\n    plt.grid(True)\n    plt.savefig('storage_trend.png')\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#integration-examples","level":2,"title":"Integration Examples","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#slack-notifications","level":3,"title":"Slack Notifications","text":"<p>Send cleanup summaries to Slack:</p> <pre><code>import requests\nimport json\n\ndef send_slack_notification(webhook_url, message):\n    payload = {\n        \"text\": f\"EEMT Cleanup Report\",\n        \"attachments\": [{\n            \"color\": \"good\",\n            \"fields\": [\n                {\"title\": \"Jobs Cleaned\", \"value\": message['jobs_cleaned'], \"short\": True},\n                {\"title\": \"Space Freed\", \"value\": message['space_freed'], \"short\": True},\n                {\"title\": \"Status\", \"value\": \"Success\", \"short\": True}\n            ]\n        }]\n    }\n    requests.post(webhook_url, json=payload)\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#email-reports","level":3,"title":"Email Reports","text":"<p>Send daily cleanup reports:</p> <pre><code>import smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\ndef send_cleanup_report(recipient, stats):\n    msg = MIMEMultipart()\n    msg['Subject'] = 'EEMT Daily Cleanup Report'\n    msg['From'] = 'eemt@example.com'\n    msg['To'] = recipient\n\n    body = f\"\"\"\n    Daily Cleanup Report\n    ====================\n\n    Jobs Cleaned: {stats['jobs_cleaned']}\n    Space Freed: {stats['space_freed']}\n\n    Current Storage Usage: {stats['current_usage']}\n    Available Space: {stats['available_space']}\n\n    Next cleanup scheduled for: {stats['next_run']}\n    \"\"\"\n\n    msg.attach(MIMEText(body, 'plain'))\n\n    with smtplib.SMTP('localhost') as server:\n        server.send_message(msg)\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#prometheus-metrics","level":3,"title":"Prometheus Metrics","text":"<p>Export cleanup metrics for monitoring:</p> <pre><code>from prometheus_client import Counter, Gauge, Histogram\n\n# Define metrics\ncleanup_runs = Counter('eemt_cleanup_runs_total', 'Total cleanup runs')\njobs_cleaned = Counter('eemt_jobs_cleaned_total', 'Total jobs cleaned', ['status'])\nspace_freed = Counter('eemt_space_freed_bytes_total', 'Total space freed')\ncleanup_duration = Histogram('eemt_cleanup_duration_seconds', 'Cleanup duration')\ncurrent_usage = Gauge('eemt_storage_usage_bytes', 'Current storage usage')\n\n# Update metrics during cleanup\n@cleanup_duration.time()\ndef run_cleanup():\n    cleanup_runs.inc()\n    # ... cleanup logic ...\n    jobs_cleaned.labels(status='completed').inc(3)\n    space_freed.inc(45 * 1024 * 1024 * 1024)  # 45 GB\n    current_usage.set(get_current_usage())\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#security-considerations","level":2,"title":"Security Considerations","text":"","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#access-control","level":3,"title":"Access Control","text":"<p>Restrict cleanup operations:</p> <pre><code>from functools import wraps\nfrom flask import request, abort\n\ndef require_admin(f):\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        auth = request.headers.get('Authorization')\n        if not verify_admin_token(auth):\n            abort(403)\n        return f(*args, **kwargs)\n    return decorated_function\n\n@app.route('/api/cleanup', methods=['POST'])\n@require_admin\ndef api_cleanup():\n    # Only admins can trigger cleanup\n    return run_cleanup()\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#audit-logging","level":3,"title":"Audit Logging","text":"<p>Track all cleanup operations:</p> <pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass AuditLogger:\n    def __init__(self, log_file='cleanup_audit.log'):\n        self.logger = logging.getLogger('cleanup_audit')\n        handler = logging.FileHandler(log_file)\n        formatter = logging.Formatter(\n            '%(asctime)s - %(levelname)s - %(message)s'\n        )\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n        self.logger.setLevel(logging.INFO)\n\n    def log_cleanup(self, user, action, details):\n        audit_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'user': user,\n            'action': action,\n            'details': details,\n            'ip_address': request.remote_addr\n        }\n        self.logger.info(json.dumps(audit_entry))\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#safe-deletion","level":3,"title":"Safe Deletion","text":"<p>Implement safeguards against accidental deletion:</p> <pre><code>def safe_delete(path, job_id):\n    \"\"\"Safely delete files with verification\"\"\"\n\n    # Verify path is within allowed directories\n    allowed_dirs = ['/app/uploads', '/app/results']\n    if not any(str(path).startswith(d) for d in allowed_dirs):\n        raise ValueError(f\"Attempted to delete outside allowed directories: {path}\")\n\n    # Verify job ownership\n    if not verify_job_ownership(job_id, path):\n        raise ValueError(f\"Path does not belong to job {job_id}\")\n\n    # Create backup before deletion (optional)\n    if ENABLE_CLEANUP_BACKUP:\n        backup_path = f\"/backup/{job_id}/{path.name}\"\n        shutil.copy2(path, backup_path)\n\n    # Perform deletion\n    if path.is_dir():\n        shutil.rmtree(path)\n    else:\n        path.unlink()\n\n    return True\n</code></pre>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"web-interface/job-cleanup/#summary","level":2,"title":"Summary","text":"<p>The EEMT job cleanup system provides flexible, automated management of job data to maintain optimal disk usage while preserving important metadata and configurations. Key features include:</p> <ul> <li>Configurable retention policies for different job states</li> <li>Multiple scheduling options (cron, systemd, manual)</li> <li>Docker and Kubernetes integration for containerized deployments</li> <li>Comprehensive logging and monitoring capabilities</li> <li>Safety features including dry-run mode and audit trails</li> <li>REST API for programmatic control</li> </ul> <p>The system ensures that your EEMT deployment remains performant and manageable even with high job volumes, while maintaining data integrity and providing recovery options when needed.</p>","path":["Web Interface","Job Data Cleanup System"],"tags":[]},{"location":"workflows/","level":1,"title":"EEMT Calculation Workflows","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#overview","level":2,"title":"Overview","text":"<p>This guide provides complete workflows for calculating EEMT using the three methodological approaches identified in Rasmussen et al. (2014):</p> <ol> <li>EEMT_TRAD: Traditional approach using climate averages</li> <li>EEMT_TOPO: Topographic controls on energy and water balance  </li> <li>EEMT_TOPO-VEG: Full vegetation and topographic integration</li> </ol>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#complete-eemt-calculation-framework","level":2,"title":"Complete EEMT Calculation Framework","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#mathematical-foundation","level":3,"title":"Mathematical Foundation","text":"<p>Based on Rasmussen et al. (2014), EEMT is calculated as:</p> <pre><code>EEMT = E_BIO + E_PPT [MJ m‚Åª¬≤ yr‚Åª¬π]\n</code></pre>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#biological-energy-component-e_bio","level":4,"title":"Biological Energy Component (E_BIO)","text":"<p><pre><code>E_BIO = NPP √ó h_BIO [W m‚Åª¬≤]\n</code></pre> Where: - NPP = Net Primary Production [kg m‚Åª¬≤ s‚Åª¬π] - h_BIO = Specific biomass enthalpy (22 √ó 10‚Å∂ J kg‚Åª¬π)</p>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#precipitation-energy-component-e_ppt","level":4,"title":"Precipitation Energy Component (E_PPT)","text":"<p><pre><code>E_PPT = F √ó c_w √ó ŒîT [W m‚Åª¬≤]\n</code></pre> Where: - F = Effective precipitation flux [kg m‚Åª¬≤ s‚Åª¬π] - c_w = Specific heat of water (4.18 √ó 10¬≥ J kg‚Åª¬π K‚Åª¬π) - ŒîT = T_ambient - 273.15K [K]</p>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#workflow-1-traditional-eemt-eemt_trad","level":2,"title":"Workflow 1: Traditional EEMT (EEMT_TRAD)","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#overview_1","level":3,"title":"Overview","text":"<p>Simple climate-based approach suitable for regional comparisons.</p>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#required-data","level":3,"title":"Required Data","text":"<ul> <li>Monthly temperature (min/max)</li> <li>Monthly precipitation  </li> <li>Digital elevation model (for area calculation)</li> </ul>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#implementation","level":3,"title":"Implementation","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nTraditional EEMT Calculation\nBased on Rasmussen et al. (2005, 2014)\n\"\"\"\n\nimport numpy as np\nimport rasterio\nimport pandas as pd\nfrom datetime import datetime\n\ndef calculate_pet_hamon(temp_mean, temp_max, temp_min, daylight_hours):\n    \"\"\"\n    Calculate potential evapotranspiration using Hamon's equation\n\n    Parameters:\n    temp_mean: mean monthly temperature [¬∞C]\n    temp_max, temp_min: daily temperature extremes [¬∞C]\n    daylight_hours: day length [hours]\n\n    Returns:\n    PET in mm/month\n    \"\"\"\n\n    # Saturated vapor pressure (Tetens equation)\n    es = 0.6108 * np.exp(17.27 * temp_mean / (temp_mean + 237.3))  # kPa\n\n    # Hamon PET equation\n    pet_daily = 0.55 * (daylight_hours / 12) * (es / (temp_mean + 273.15)) * 25.4\n\n    # Convert to monthly (approximate)\n    days_in_month = 30.4  # Average\n    pet_monthly = pet_daily * days_in_month\n\n    return pet_monthly\n\ndef calculate_npp_lieth(temperature, precipitation, pet):\n    \"\"\"\n    Calculate NPP using Lieth (1975) temperature-based approach\n\n    Parameters:\n    temperature: mean monthly temperature [¬∞C]\n    precipitation: monthly precipitation [mm]\n    pet: potential evapotranspiration [mm]\n\n    Returns:\n    NPP in kg/m¬≤/yr\n    \"\"\"\n\n    # Only calculate NPP for months with water surplus\n    npp_monthly = np.zeros_like(temperature)\n\n    for i, (temp, precip, evap) in enumerate(zip(temperature, precipitation, pet)):\n        if precip &gt; evap and temp &gt; 0:  # Growing conditions\n            # Lieth equation: NPP = 3000[1 - exp(1.315 - 0.119T)]^-1\n            npp_monthly[i] = 3000 * (1 - np.exp(1.315 - 0.119 * temp))**(-1)\n            npp_monthly[i] *= (precip - evap) / precip  # Scale by water availability\n        else:\n            npp_monthly[i] = 0\n\n    # Convert g/m¬≤/month to kg/m¬≤/yr  \n    npp_annual = np.sum(npp_monthly) / 1000  # g to kg conversion\n\n    return npp_annual\n\ndef calculate_eemt_traditional(temperature_data, precipitation_data, daylight_data):\n    \"\"\"\n    Calculate traditional EEMT for each pixel\n\n    Parameters:\n    temperature_data: dict with 'tmin', 'tmax', 'tmean' monthly arrays\n    precipitation_data: monthly precipitation array [mm]\n    daylight_data: monthly daylight hours array [hours]\n\n    Returns:\n    EEMT array in MJ/m¬≤/yr\n    \"\"\"\n\n    # Get array dimensions\n    shape = temperature_data['tmean'].shape\n    eemt_result = np.zeros(shape)\n\n    # Process each pixel\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n\n            # Extract pixel time series\n            temp_mean = temperature_data['tmean'][i, j, :]\n            temp_max = temperature_data['tmax'][i, j, :]\n            temp_min = temperature_data['tmin'][i, j, :]\n            precip = precipitation_data[i, j, :]\n            daylight = daylight_data[i, j, :]\n\n            # Skip if any data is missing\n            if np.any(np.isnan([temp_mean, precip])):\n                eemt_result[i, j] = np.nan\n                continue\n\n            # Calculate PET\n            pet = calculate_pet_hamon(temp_mean, temp_max, temp_min, daylight)\n\n            # Calculate effective precipitation (F)\n            effective_precip = np.maximum(0, precip - pet)  # mm/month\n\n            # Convert to mass flux [kg/m¬≤/s]\n            seconds_per_month = 30.4 * 24 * 3600\n            F = (effective_precip / 1000) / seconds_per_month  # kg/m¬≤/s\n\n            # Calculate E_PPT [W/m¬≤]\n            c_w = 4180  # J/kg/K\n            delta_T = np.maximum(0, temp_mean - 0)  # ¬∞C above freezing\n            E_PPT = F * c_w * delta_T\n\n            # Calculate NPP\n            npp_annual = calculate_npp_lieth(temp_mean, precip, pet)  # kg/m¬≤/yr\n            npp_flux = npp_annual / (365 * 24 * 3600)  # kg/m¬≤/s\n\n            # Calculate E_BIO [W/m¬≤]\n            h_BIO = 22e6  # J/kg\n            E_BIO = npp_flux * h_BIO\n\n            # Calculate EEMT [W/m¬≤]\n            eemt_flux = np.mean(E_BIO + E_PPT)  # Average over months\n\n            # Convert to MJ/m¬≤/yr\n            eemt_result[i, j] = eemt_flux * 365 * 24 * 3600 / 1e6\n\n    return eemt_result\n\n# Example usage\ndef run_traditional_workflow(dem_file, climate_dir, output_file):\n    \"\"\"Complete traditional EEMT workflow\"\"\"\n\n    # Load climate data (assumes NetCDF format)\n    import xarray as xr\n\n    # Load DAYMET data\n    tmin = xr.open_dataset(f\"{climate_dir}/tmin_monthly.nc\")\n    tmax = xr.open_dataset(f\"{climate_dir}/tmax_monthly.nc\") \n    precip = xr.open_dataset(f\"{climate_dir}/prcp_monthly.nc\")\n\n    # Calculate mean temperature\n    tmean = (tmin + tmax) / 2\n\n    # Calculate daylight hours (simplified)\n    # This should use actual solar geometry calculations\n    daylight_hours = np.full_like(tmean, 12.0)  # Placeholder\n\n    # Prepare data arrays\n    temp_data = {\n        'tmean': tmean.values,\n        'tmax': tmax.values, \n        'tmin': tmin.values\n    }\n\n    # Calculate EEMT\n    eemt = calculate_eemt_traditional(temp_data, precip.values, daylight_hours)\n\n    # Save results\n    with rasterio.open(dem_file) as dem_src:\n        profile = dem_src.profile.copy()\n        profile.update(dtype='float32', count=1)\n\n        with rasterio.open(output_file, 'w', **profile) as dst:\n            dst.write(eemt.astype(np.float32), 1)\n\n    print(f\"Traditional EEMT saved to: {output_file}\")\n\n# Run workflow\n# run_traditional_workflow('dem.tif', 'climate_data/', 'eemt_traditional.tif')\n</code></pre>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#workflow-2-topographic-eemt-eemt_topo","level":2,"title":"Workflow 2: Topographic EEMT (EEMT_TOPO)","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#overview_2","level":3,"title":"Overview","text":"<p>Incorporates topographic controls on solar radiation, temperature, and water redistribution.</p>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#enhanced-implementation","level":3,"title":"Enhanced Implementation","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nTopographic EEMT Calculation\nBased on Rasmussen et al. (2014) methodology\n\"\"\"\n\nimport numpy as np\nimport rasterio\nimport subprocess\nimport tempfile\nimport os\nfrom pathlib import Path\n\nclass TopographicEEMT:\n    \"\"\"Calculate EEMT with topographic controls\"\"\"\n\n    def __init__(self, dem_path, climate_dir, output_dir):\n        self.dem_path = Path(dem_path)\n        self.climate_dir = Path(climate_dir)\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def calculate_solar_radiation(self):\n        \"\"\"Calculate topographically-modified solar radiation\"\"\"\n\n        # Run annual solar radiation calculation\n        from grass_solar_calculator import GrassSolarCalculator\n\n        solar_calc = GrassSolarCalculator(\n            str(self.dem_path), \n            str(self.output_dir / 'solar')\n        )\n\n        # Calculate for full year\n        solar_calc.calculate_annual_solar()\n        solar_calc.calculate_monthly_summaries()\n\n        print(\"‚úì Solar radiation calculation completed\")\n\n    def calculate_topographic_temperature(self, base_temp, lapse_rate=-6.5):\n        \"\"\"\n        Calculate topographically-modified temperature\n        Following Eq. 6 from Rasmussen et al. (2014)\n        \"\"\"\n\n        with rasterio.open(self.dem_path) as dem_src:\n            elevation = dem_src.read(1)\n            profile = dem_src.profile\n\n        # Load solar radiation ratio (S_topo / S_flat)\n        solar_ratio_file = self.output_dir / 'solar' / 'solar_ratio_annual.tif'\n\n        if solar_ratio_file.exists():\n            with rasterio.open(solar_ratio_file) as src:\n                solar_ratio = src.read(1)\n        else:\n            print(\"Warning: Solar ratio not found, using elevation only\")\n            solar_ratio = np.ones_like(elevation)\n\n        # Calculate temperature modification\n        # T_i = T_b - T_lapse * (z_i - z_b)/1000 + C * (S_i - 1/S_i) * (1 - LAI_i/LAI_max)\n\n        base_elevation = np.nanmin(elevation)\n        elevation_diff = (elevation - base_elevation) / 1000  # km\n\n        # Temperature lapse rate effect\n        temp_lapse_effect = lapse_rate * elevation_diff\n\n        # Solar radiation effect (simplified - no LAI for TOPO method)\n        solar_effect = 2.0 * (solar_ratio - 1/solar_ratio)  # C=2 constant\n\n        # Modified temperature\n        temp_modified = base_temp - temp_lapse_effect + solar_effect\n\n        return temp_modified, profile\n\n    def calculate_mcwi(self):\n        \"\"\"\n        Calculate Mass Conservative Wetness Index\n        Following Rasmussen et al. (2014) Eq. 9-10\n        \"\"\"\n\n        # Use GRASS to calculate flow accumulation and slope\n        temp_location = tempfile.mkdtemp()\n\n        grass_commands = f\"\"\"\n# Import DEM\nr.in.gdal input={self.dem_path} output=dem\n\n# Calculate flow accumulation using D-infinity\nr.terraflow elevation=dem filled=dem_filled direction=flow_dir \\\\\n            swatershed=watersheds accumulation=flow_accum tci=twi\n\n# Calculate slope in degrees\nr.slope.aspect elevation=dem slope=slope_deg\n\n# Calculate traditional wetness index  \nr.mapcalc \"wetness_index = log(flow_accum / tan(slope_deg * 3.14159/180))\"\n\n# Calculate mass conservative wetness index (MCWI)\n# Normalize by mean wetness index to conserve mass\nr.univar wetness_index\n\"\"\"\n\n        # Execute GRASS commands and calculate MCWI\n        # (Implementation details for MCWI calculation)\n\n        return self.output_dir / 'mcwi.tif'\n\n    def calculate_effective_precipitation(self, precipitation, temperature):\n        \"\"\"\n        Calculate effective precipitation with topographic redistribution\n        Using Penman-Monteith and Budyko curve approach\n        \"\"\"\n\n        # Load MCWI for water redistribution\n        with rasterio.open(self.output_dir / 'mcwi.tif') as src:\n            mcwi = src.read(1)\n\n        # Calculate PET using Penman-Monteith (simplified)\n        # This is a placeholder - full implementation needs wind, humidity, radiation\n        pet = self.calculate_penman_monteith(temperature, precipitation)\n\n        # Calculate AET using Budyko curve\n        aridity_index = pet / precipitation\n        w = 2.63  # Empirical constant\n\n        # Zhang-Budyko equation\n        aet_ratio = (1 + w * aridity_index) / (1 + w * aridity_index + 1/aridity_index)\n        aet = precipitation * aet_ratio\n\n        # Effective precipitation\n        effective_precip = precipitation - aet\n\n        # Redistribute using MCWI\n        effective_precip_redistributed = effective_precip * mcwi\n\n        return effective_precip_redistributed\n\n    def calculate_npp_topographic(self, elevation, aspect, slope):\n        \"\"\"\n        Calculate NPP with topographic controls\n        Following Eq. 11 from Rasmussen et al. (2014)\n        \"\"\"\n\n        # Calculate northness\n        aspect_rad = np.deg2rad(aspect)\n        slope_rad = np.deg2rad(slope)\n        northness = np.cos(aspect_rad) * np.sin(slope_rad)\n\n        # Empirical relationship from Whittaker and Niering (1975)\n        # NPP = 0.39z + 346n - 187 [g/m¬≤/yr]\n        npp = 0.39 * elevation + 346 * northness - 187\n\n        # Set minimum NPP\n        npp = np.maximum(npp, 100)  # g/m¬≤/yr minimum\n\n        # Convert to kg/m¬≤/yr\n        npp_kg = npp / 1000\n\n        return npp_kg\n\n    def run_complete_workflow(self):\n        \"\"\"Execute complete topographic EEMT workflow\"\"\"\n\n        print(\"Starting Topographic EEMT Calculation...\")\n\n        # Step 1: Calculate solar radiation\n        print(\"1. Calculating solar radiation...\")\n        self.calculate_solar_radiation()\n\n        # Step 2: Calculate MCWI\n        print(\"2. Calculating mass conservative wetness index...\")\n        mcwi_file = self.calculate_mcwi()\n\n        # Step 3: Load climate data and DEM\n        print(\"3. Loading input data...\")\n        with rasterio.open(self.dem_path) as src:\n            elevation = src.read(1)\n            profile = src.profile\n\n        # Load climate data (implementation depends on data format)\n        # This is a placeholder for actual climate data loading\n        climate_data = self.load_climate_data()\n\n        # Step 4: Calculate topographically modified temperature\n        print(\"4. Calculating topographic temperature modification...\")\n        temp_modified, _ = self.calculate_topographic_temperature(\n            climate_data['temperature'], \n            lapse_rate=-6.5\n        )\n\n        # Step 5: Calculate effective precipitation with redistribution\n        print(\"5. Calculating effective precipitation...\")\n        effective_precip = self.calculate_effective_precipitation(\n            climate_data['precipitation'],\n            temp_modified\n        )\n\n        # Step 6: Calculate NPP with topographic effects\n        print(\"6. Calculating topographic NPP...\")\n\n        # Load slope and aspect from DEM\n        slope, aspect = self.calculate_slope_aspect()\n        npp = self.calculate_npp_topographic(elevation, aspect, slope)\n\n        # Step 7: Calculate EEMT components\n        print(\"7. Calculating EEMT components...\")\n\n        # E_BIO calculation\n        h_bio = 22e6  # J/kg\n        npp_flux = npp / (365 * 24 * 3600)  # kg/m¬≤/s\n        e_bio = npp_flux * h_bio  # W/m¬≤\n\n        # E_PPT calculation  \n        c_w = 4180  # J/kg/K\n        delta_t = np.maximum(0, temp_modified - 273.15)  # K above freezing\n        precip_flux = effective_precip / (30.4 * 24 * 3600)  # kg/m¬≤/s (monthly avg)\n        e_ppt = precip_flux * c_w * delta_t  # W/m¬≤\n\n        # Calculate EEMT\n        eemt_flux = e_bio + e_ppt  # W/m¬≤\n        eemt_annual = eemt_flux * 365 * 24 * 3600 / 1e6  # MJ/m¬≤/yr\n\n        # Step 8: Save results\n        print(\"8. Saving results...\")\n\n        # Save EEMT\n        with rasterio.open(self.output_dir / 'eemt_topographic.tif', 'w', **profile) as dst:\n            dst.write(eemt_annual.astype(np.float32), 1)\n\n        # Save components\n        with rasterio.open(self.output_dir / 'e_bio_topographic.tif', 'w', **profile) as dst:\n            dst.write((e_bio * 365 * 24 * 3600 / 1e6).astype(np.float32), 1)\n\n        with rasterio.open(self.output_dir / 'e_ppt_topographic.tif', 'w', **profile) as dst:\n            dst.write((e_ppt * 365 * 24 * 3600 / 1e6).astype(np.float32), 1)\n\n        print(f\"‚úì Topographic EEMT calculation completed\")\n        print(f\"Results saved to: {self.output_dir}\")\n\n        return eemt_annual\n\n# Usage example\nif __name__ == '__main__':\n\n    # Initialize calculator\n    calculator = TopographicEEMT(\n        dem_path='data/elevation/dem.tif',\n        climate_dir='data/climate/',\n        output_dir='results/eemt_topographic/'\n    )\n\n    # Run complete workflow\n    eemt_result = calculator.run_complete_workflow()\n\n    # Print summary statistics\n    print(f\"\\nEEMT Summary Statistics:\")\n    print(f\"  Mean: {np.nanmean(eemt_result):.2f} MJ/m¬≤/yr\")\n    print(f\"  Min:  {np.nanmin(eemt_result):.2f} MJ/m¬≤/yr\") \n    print(f\"  Max:  {np.nanmax(eemt_result):.2f} MJ/m¬≤/yr\")\n    print(f\"  Std:  {np.nanstd(eemt_result):.2f} MJ/m¬≤/yr\")\n</code></pre>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#workflow-3-vegetation-eemt-eemt_topo-veg","level":2,"title":"Workflow 3: Vegetation EEMT (EEMT_TOPO-VEG)","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#overview_3","level":3,"title":"Overview","text":"<p>Full implementation including vegetation structure, LAI, and surface resistance effects.</p>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#implementation_1","level":3,"title":"Implementation","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nVegetation-Enhanced EEMT Calculation  \nBased on Rasmussen et al. (2014) EEMT_TOPO-VEG approach\n\"\"\"\n\nimport numpy as np\nimport rasterio\nfrom scipy import ndimage\n\nclass VegetationEEMT(TopographicEEMT):\n    \"\"\"EEMT calculation with full vegetation integration\"\"\"\n\n    def __init__(self, dem_path, climate_dir, output_dir, vegetation_data=None):\n        super().__init__(dem_path, climate_dir, output_dir)\n        self.vegetation_data = vegetation_data\n\n    def calculate_lai_from_ndvi(self, ndvi_file):\n        \"\"\"\n        Calculate Leaf Area Index from NDVI\n        Using Qi et al. (2000) polynomial for semiarid regions\n        \"\"\"\n\n        with rasterio.open(ndvi_file) as src:\n            ndvi = src.read(1)\n            profile = src.profile\n\n        # Qi et al. (2000) polynomial: LAI = ax¬≥ + bx¬≤ + cx + d\n        a, b, c, d = 18.99, -15.24, 6.124, -0.352\n        lai = a * ndvi**3 + b * ndvi**2 + c * ndvi + d\n\n        # Constrain LAI to reasonable range\n        lai = np.clip(lai, 0, 10)\n\n        # Save LAI\n        lai_file = self.output_dir / 'lai.tif'\n        with rasterio.open(lai_file, 'w', **profile) as dst:\n            dst.write(lai.astype(np.float32), 1)\n\n        return lai, lai_file\n\n    def calculate_canopy_height_from_lidar(self, lidar_file):\n        \"\"\"\n        Extract canopy height from LiDAR data\n        \"\"\"\n\n        # This would process LiDAR point clouds or canopy height models\n        # For now, return placeholder based on LAI\n        with rasterio.open(self.output_dir / 'lai.tif') as src:\n            lai = src.read(1)\n            profile = src.profile\n\n        # Estimate canopy height from LAI (simplified relationship)\n        # In practice, use actual LiDAR processing\n        canopy_height = lai * 2.5  # Rough approximation\n\n        # Save canopy height\n        height_file = self.output_dir / 'canopy_height.tif'\n        with rasterio.open(height_file, 'w', **profile) as dst:\n            dst.write(canopy_height.astype(np.float32), 1)\n\n        return canopy_height, height_file\n\n    def calculate_npp_vegetation(self, canopy_height):\n        \"\"\"\n        Calculate NPP from canopy height\n        Following Eq. 12 from Rasmussen et al. (2014)\n        \"\"\"\n\n        # Polynomial relationship: NPP = 196 + 36h - 0.61h¬≤ - 12.09h¬≥\n        h = canopy_height\n        npp = 196 + 36*h - 0.61*h**2 - 12.09*h**3\n\n        # Set minimum NPP\n        npp = np.maximum(npp, 100)  # g/m¬≤/yr\n\n        # Convert to kg/m¬≤/yr\n        npp_kg = npp / 1000\n\n        return npp_kg\n\n    def calculate_surface_resistance(self, lai):\n        \"\"\"\n        Calculate surface resistance from LAI\n        Following Schulze et al. (1994) and Kelliher et al. (1995)\n        \"\"\"\n\n        # Maximum leaf stomatal conductance \n        g_max = 0.008  # m/s\n\n        # Bulk surface conductance from LAI\n        # Polynomial fit to literature data\n        g_surface = g_max * (1 - np.exp(-0.5 * lai))\n\n        # Surface resistance (inverse of conductance)\n        r_surface = 1 / np.maximum(g_surface, 1e-6)  # Avoid division by zero\n\n        # Constrain to reasonable range\n        r_surface = np.clip(r_surface, 38, 1000)  # s/m\n\n        return r_surface\n\n    def calculate_aet_penman_monteith(self, temperature, humidity, wind_speed, \n                                    net_radiation, lai):\n        \"\"\"\n        Calculate actual evapotranspiration using full Penman-Monteith\n        Including surface and aerodynamic resistance\n        \"\"\"\n\n        # Calculate surface resistance\n        r_surface = self.calculate_surface_resistance(lai)\n\n        # Calculate aerodynamic resistance (simplified)\n        # In practice, use canopy height and wind profile\n        canopy_height = lai * 2.0  # Rough estimate\n        r_aero = 208 / np.maximum(wind_speed, 0.1) * np.log(2.0 / (0.1 * canopy_height))\n        r_aero = np.clip(r_aero, 10, 500)  # s/m\n\n        # Psychrometric constant\n        gamma = 0.665  # kPa/¬∞C\n\n        # Slope of saturation vapor pressure curve\n        delta = 4098 * (0.6108 * np.exp(17.27 * temperature / (temperature + 237.3))) / (temperature + 237.3)**2\n\n        # Vapor pressure deficit\n        es = 0.6108 * np.exp(17.27 * temperature / (temperature + 237.3))\n        ea = humidity * es / 100  # Assuming humidity is relative humidity %\n        vpd = es - ea\n\n        # Penman-Monteith equation\n        numerator = delta * net_radiation + gamma * 900 * vpd / (temperature + 273) / r_aero\n        denominator = delta + gamma * (1 + r_surface / r_aero)\n\n        aet = numerator / denominator  # mm/day\n\n        return aet\n\n    def run_vegetation_workflow(self, ndvi_file=None, lidar_file=None):\n        \"\"\"Execute complete vegetation EEMT workflow\"\"\"\n\n        print(\"Starting Vegetation EEMT Calculation...\")\n\n        # Step 1: Calculate solar radiation (inherited)\n        print(\"1. Calculating solar radiation...\")\n        self.calculate_solar_radiation()\n\n        # Step 2: Process vegetation data\n        print(\"2. Processing vegetation data...\")\n\n        if ndvi_file:\n            lai, lai_file = self.calculate_lai_from_ndvi(ndvi_file)\n        else:\n            print(\"Warning: No NDVI data provided, using default LAI\")\n            lai = np.ones((100, 100)) * 2.0  # Placeholder\n\n        if lidar_file:\n            canopy_height, height_file = self.calculate_canopy_height_from_lidar(lidar_file)\n        else:\n            canopy_height, height_file = self.calculate_canopy_height_from_lidar(None)\n\n        # Step 3: Calculate vegetation-modified NPP\n        print(\"3. Calculating vegetation NPP...\")\n        npp = self.calculate_npp_vegetation(canopy_height)\n\n        # Step 4: Calculate AET with vegetation effects\n        print(\"4. Calculating AET with vegetation controls...\")\n\n        # Load climate data\n        climate_data = self.load_climate_data()\n\n        # Calculate AET using Penman-Monteith with vegetation resistance\n        aet = self.calculate_aet_penman_monteith(\n            climate_data['temperature'],\n            climate_data['humidity'], \n            climate_data['wind_speed'],\n            climate_data['net_radiation'],\n            lai\n        )\n\n        # Step 5: Calculate effective precipitation\n        effective_precip = climate_data['precipitation'] - aet\n\n        # Step 6: Apply topographic redistribution\n        with rasterio.open(self.output_dir / 'mcwi.tif') as src:\n            mcwi = src.read(1)\n\n        effective_precip_redistributed = effective_precip * mcwi\n\n        # Step 7: Calculate EEMT\n        print(\"5. Calculating final EEMT...\")\n\n        # Load DEM for output profile\n        with rasterio.open(self.dem_path) as src:\n            profile = src.profile\n\n        # Energy calculations\n        h_bio = 22e6  # J/kg\n        c_w = 4180   # J/kg/K\n\n        # Convert fluxes to W/m¬≤\n        npp_flux = npp / (365 * 24 * 3600)  # kg/m¬≤/s\n        precip_flux = effective_precip_redistributed / (30.4 * 24 * 3600)  # kg/m¬≤/s\n\n        # Energy components\n        e_bio = npp_flux * h_bio\n        e_ppt = precip_flux * c_w * np.maximum(0, climate_data['temperature'] - 273.15)\n\n        # Total EEMT\n        eemt_flux = e_bio + e_ppt\n        eemt_annual = eemt_flux * 365 * 24 * 3600 / 1e6  # MJ/m¬≤/yr\n\n        # Step 8: Save results\n        print(\"6. Saving results...\")\n\n        outputs = {\n            'eemt_vegetation.tif': eemt_annual,\n            'e_bio_vegetation.tif': e_bio * 365 * 24 * 3600 / 1e6,\n            'e_ppt_vegetation.tif': e_ppt * 365 * 24 * 3600 / 1e6,\n            'npp_vegetation.tif': npp,\n            'lai.tif': lai,\n            'canopy_height.tif': canopy_height\n        }\n\n        for filename, data in outputs.items():\n            output_path = self.output_dir / filename\n            with rasterio.open(output_path, 'w', **profile) as dst:\n                dst.write(data.astype(np.float32), 1)\n\n        print(f\"‚úì Vegetation EEMT calculation completed\")\n        print(f\"Results saved to: {self.output_dir}\")\n\n        return eemt_annual\n\n# Command line interface\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser(description='Vegetation EEMT Calculator')\n    parser.add_argument('dem', help='Input DEM file')\n    parser.add_argument('--climate-dir', required=True, help='Climate data directory')\n    parser.add_argument('--output-dir', required=True, help='Output directory')\n    parser.add_argument('--ndvi', help='NDVI raster file for LAI calculation')\n    parser.add_argument('--lidar', help='LiDAR file for canopy height')\n\n    args = parser.parse_args()\n\n    # Run vegetation EEMT calculation\n    calculator = VegetationEEMT(args.dem, args.climate_dir, args.output_dir)\n    result = calculator.run_vegetation_workflow(args.ndvi, args.lidar)\n\n    print(f\"Vegetation EEMT range: {np.nanmin(result):.1f} - {np.nanmax(result):.1f} MJ/m¬≤/yr\")\n</code></pre>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#automated-workflow-integration","level":2,"title":"Automated Workflow Integration","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#complete-eemt-pipeline","level":3,"title":"Complete EEMT Pipeline","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete EEMT calculation pipeline integrating all three approaches\nEnhanced version of eemt/run-workflow\n\"\"\"\n\nimport argparse\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport rasterio\n\ndef run_complete_eemt_pipeline(dem_file, output_dir, climate_dir, \n                             start_year=2015, end_year=2020,\n                             vegetation_data=None, validation_data=None):\n    \"\"\"\n    Run complete EEMT pipeline with all three calculation methods\n\n    Parameters:\n    dem_file: Path to elevation data\n    output_dir: Output directory for results\n    climate_dir: Directory containing climate data\n    start_year, end_year: Analysis time period\n    vegetation_data: Dict with 'ndvi' and 'lidar' file paths\n    validation_data: Dict with validation datasets (soil depth, biomass, etc.)\n    \"\"\"\n\n    print(\"=== EEMT Complete Pipeline ===\")\n\n    output_dir = Path(output_dir) \n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    results = {}\n\n    # Method 1: Traditional EEMT\n    print(\"\\n1. Calculating Traditional EEMT...\")\n\n    try:\n        eemt_trad = run_traditional_workflow(\n            dem_file, \n            climate_dir, \n            output_dir / 'eemt_traditional.tif'\n        )\n        results['traditional'] = eemt_trad\n        print(\"‚úì Traditional EEMT completed\")\n    except Exception as e:\n        print(f\"‚úó Traditional EEMT failed: {e}\")\n\n    # Method 2: Topographic EEMT  \n    print(\"\\n2. Calculating Topographic EEMT...\")\n\n    try:\n        topo_calculator = TopographicEEMT(dem_file, climate_dir, output_dir / 'topographic')\n        eemt_topo = topo_calculator.run_complete_workflow()\n        results['topographic'] = eemt_topo\n        print(\"‚úì Topographic EEMT completed\")\n    except Exception as e:\n        print(f\"‚úó Topographic EEMT failed: {e}\")\n\n    # Method 3: Vegetation EEMT\n    print(\"\\n3. Calculating Vegetation EEMT...\")\n\n    try:\n        veg_calculator = VegetationEEMT(\n            dem_file, climate_dir, output_dir / 'vegetation',\n            vegetation_data\n        )\n\n        ndvi_file = vegetation_data.get('ndvi') if vegetation_data else None\n        lidar_file = vegetation_data.get('lidar') if vegetation_data else None\n\n        eemt_veg = veg_calculator.run_vegetation_workflow(ndvi_file, lidar_file)\n        results['vegetation'] = eemt_veg\n        print(\"‚úì Vegetation EEMT completed\")\n    except Exception as e:\n        print(f\"‚úó Vegetation EEMT failed: {e}\")\n\n    # Comparison Analysis\n    print(\"\\n4. Generating Comparison Analysis...\")\n\n    if len(results) &gt; 1:\n        generate_comparison_analysis(results, output_dir / 'comparison')\n\n    # Validation (if data provided)\n    if validation_data:\n        print(\"\\n5. Running Validation Analysis...\")\n        run_validation_analysis(results, validation_data, output_dir / 'validation')\n\n    print(f\"\\n=== Pipeline Complete ===\")\n    print(f\"Results saved to: {output_dir}\")\n\n    return results\n\ndef generate_comparison_analysis(results, output_dir):\n    \"\"\"Generate comparison plots and statistics between EEMT methods\"\"\"\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load first result to get spatial structure\n    first_key = list(results.keys())[0]\n\n    if isinstance(results[first_key], str):\n        # Results are file paths\n        comparison_data = {}\n        for method, filepath in results.items():\n            with rasterio.open(filepath) as src:\n                comparison_data[method] = src.read(1)\n                profile = src.profile\n    else:\n        # Results are arrays\n        comparison_data = results\n\n    # Calculate difference maps\n    if 'traditional' in comparison_data and 'topographic' in comparison_data:\n        diff_topo_trad = comparison_data['topographic'] - comparison_data['traditional']\n\n        with rasterio.open(output_dir / 'difference_topo_minus_trad.tif', 'w', **profile) as dst:\n            dst.write(diff_topo_trad.astype(np.float32), 1)\n\n    if 'vegetation' in comparison_data and 'topographic' in comparison_data:\n        diff_veg_topo = comparison_data['vegetation'] - comparison_data['topographic'] \n\n        with rasterio.open(output_dir / 'difference_veg_minus_topo.tif', 'w', **profile) as dst:\n            dst.write(diff_veg_topo.astype(np.float32), 1)\n\n    # Summary statistics\n    stats_file = output_dir / 'comparison_statistics.txt'\n    with open(stats_file, 'w') as f:\n        f.write(\"EEMT Method Comparison Statistics\\\\n\")\n        f.write(\"=\" * 40 + \"\\\\n\\\\n\")\n\n        for method, data in comparison_data.items():\n            f.write(f\"{method.upper()} EEMT:\\\\n\")\n            f.write(f\"  Mean: {np.nanmean(data):.2f} MJ/m¬≤/yr\\\\n\")\n            f.write(f\"  Std:  {np.nanstd(data):.2f} MJ/m¬≤/yr\\\\n\")\n            f.write(f\"  Min:  {np.nanmin(data):.2f} MJ/m¬≤/yr\\\\n\")\n            f.write(f\"  Max:  {np.nanmax(data):.2f} MJ/m¬≤/yr\\\\n\\\\n\")\n\n    print(f\"‚úì Comparison analysis saved to: {output_dir}\")\n\ndef main():\n    \"\"\"Main command line interface\"\"\"\n\n    parser = argparse.ArgumentParser(description='Complete EEMT Pipeline')\n    parser.add_argument('dem', help='Input DEM file path')\n    parser.add_argument('--output', '-o', required=True, help='Output directory')\n    parser.add_argument('--climate', '-c', required=True, help='Climate data directory')\n    parser.add_argument('--start-year', type=int, default=2015, help='Start year')\n    parser.add_argument('--end-year', type=int, default=2020, help='End year')\n    parser.add_argument('--ndvi', help='NDVI file for vegetation analysis')\n    parser.add_argument('--lidar', help='LiDAR file for canopy height')\n    parser.add_argument('--validation-soil', help='Soil depth data for validation')\n    parser.add_argument('--validation-biomass', help='Biomass data for validation')\n\n    args = parser.parse_args()\n\n    # Prepare vegetation data\n    vegetation_data = {}\n    if args.ndvi:\n        vegetation_data['ndvi'] = args.ndvi\n    if args.lidar:\n        vegetation_data['lidar'] = args.lidar\n\n    # Prepare validation data  \n    validation_data = {}\n    if args.validation_soil:\n        validation_data['soil_depth'] = args.validation_soil\n    if args.validation_biomass:\n        validation_data['biomass'] = args.validation_biomass\n\n    # Run complete pipeline\n    results = run_complete_eemt_pipeline(\n        args.dem,\n        args.output,\n        args.climate,\n        args.start_year,\n        args.end_year,\n        vegetation_data if vegetation_data else None,\n        validation_data if validation_data else None\n    )\n\n    # Success summary\n    success_count = len(results)\n    print(f\"\\\\nPipeline completed with {success_count}/3 methods successful\")\n\n    return success_count &gt; 0\n\nif __name__ == '__main__':\n    success = main()\n    sys.exit(0 if success else 1)\n</code></pre>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#integration-with-public-data-sources","level":2,"title":"Integration with Public Data Sources","text":"","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/#automated-data-acquisition-pipeline","level":3,"title":"Automated Data Acquisition Pipeline","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nAutomated data acquisition for EEMT calculations\nIntegrates with public data sources\n\"\"\"\n\nfrom data_sources.elevation import download_3dep, download_opentopo\nfrom data_sources.climate import download_daymet_spatial, download_prism\nfrom data_sources.satellite import download_landsat_ndvi\n\ndef setup_eemt_project(study_area, years, project_dir):\n    \"\"\"\n    Automated setup of complete EEMT project with public data\n\n    Parameters:\n    study_area: [west, south, east, north] bounding box\n    years: [start_year, end_year] \n    project_dir: output directory\n    \"\"\"\n\n    project_dir = Path(project_dir)\n    project_dir.mkdir(parents=True, exist_ok=True)\n\n    print(f\"Setting up EEMT project for {study_area}\")\n    print(f\"Time period: {years[0]}-{years[1]}\")\n    print(f\"Project directory: {project_dir}\")\n\n    # 1. Download elevation data\n    print(\"\\\\n1. Downloading elevation data...\")\n    dem_file = download_3dep(study_area, resolution='10m')\n    # Fallback to global data if US data unavailable  \n    if not dem_file:\n        dem_file = download_opentopo(study_area, 'SRTMGL1')\n\n    # 2. Download climate data\n    print(\"\\\\n2. Downloading climate data...\")\n    climate_files = download_daymet_spatial(\n        study_area, range(years[0], years[1]+1), \n        ['tmin', 'tmax', 'prcp', 'vp']\n    )\n\n    # 3. Download vegetation data\n    print(\"\\\\n3. Downloading vegetation data...\")\n    ndvi_file = download_landsat_ndvi(study_area, years[0])\n\n    # 4. Set up analysis directories\n    print(\"\\\\n4. Setting up analysis structure...\")\n\n    analysis_config = {\n        'dem_file': dem_file,\n        'climate_dir': project_dir / 'climate',\n        'vegetation_data': {'ndvi': ndvi_file},\n        'output_dir': project_dir / 'results'\n    }\n\n    # Save configuration\n    import json\n    with open(project_dir / 'eemt_config.json', 'w') as f:\n        json.dump(analysis_config, f, indent=2, default=str)\n\n    print(\"\\\\n‚úì Project setup completed!\")\n    print(f\"Configuration saved to: {project_dir}/eemt_config.json\")\n    print(f\"Run analysis with: python run_complete_eemt.py {project_dir}/eemt_config.json\")\n\n    return analysis_config\n\n# Example usage\nif __name__ == '__main__':\n\n    # Arizona Sky Islands study area\n    bbox = [-111.0, 32.0, -110.0, 32.5]\n    years = [2015, 2020]\n\n    config = setup_eemt_project(bbox, years, 'arizona_eemt_project')\n</code></pre> <p>This comprehensive workflow documentation provides the foundation for modern EEMT calculations using public datasets and open-source tools, with significant improvements in parallel processing and computational efficiency over the original 2016 implementation.</p>","path":["Workflows","Calculation Methods"],"tags":[]},{"location":"workflows/data-preparation/","level":1,"title":"Data Preparation Guide","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#overview","level":2,"title":"Overview","text":"<p>Proper data preparation is crucial for accurate EEMT calculations. This guide covers all aspects of preparing Digital Elevation Models (DEMs), acquiring climate data, and processing vegetation datasets for EEMT analysis.</p>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#table-of-contents","level":2,"title":"Table of Contents","text":"<ol> <li>DEM Preparation</li> <li>Climate Data Acquisition</li> <li>Vegetation Data Processing</li> <li>Quality Control</li> <li>Advanced Techniques</li> </ol>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#dem-preparation","level":2,"title":"DEM Preparation","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#data-sources","level":3,"title":"Data Sources","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#free-global-dems","level":4,"title":"Free Global DEMs","text":"Dataset Resolution Coverage Best For Access SRTM 30m 60¬∞N to 56¬∞S Global studies USGS EarthExplorer ASTER GDEM 30m 83¬∞N to 83¬∞S Complete global coverage NASA Earthdata ALOS PALSAR 12.5m Global Higher resolution needs ASF DAAC FABDEM 30m Global Vegetation-corrected University of Bristol","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#regional-high-resolution-dems","level":4,"title":"Regional High-Resolution DEMs","text":"Region Dataset Resolution Access USA 3DEP 1m, 3m, 10m USGS National Map Europe EU-DEM 25m Copernicus Canada CDEM 20m Open Canada Australia DEM-H 5m Geoscience Australia","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#step-1-download-dem-data","level":3,"title":"Step 1: Download DEM Data","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#using-gdal-to-access-cloud-optimized-geotiffs","level":4,"title":"Using GDAL to Access Cloud-Optimized GeoTIFFs","text":"<pre><code># Access USGS 3DEP data directly (no download needed)\ngdal_translate \\\n  /vsicurl/https://prd-tnm.s3.amazonaws.com/StagedProducts/Elevation/1m/Projects/CA_SanDiego_2016/TIFF/USGS_1M_CA_SanDiego_2016.tif \\\n  -projwin -117.0 32.5 -116.5 32.0 \\\n  study_area_dem.tif\n\n# Access SRTM data via AWS\ngdal_translate \\\n  /vsizip//vsicurl/https://cloud.sdsc.edu/v1/AUTH_opentopography/Raster/SRTM_GL1/SRTM_GL1_srtm/North_America/N32W117.hgt.zip/N32W117.hgt \\\n  srtm_tile.tif\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#using-python-for-automated-download","level":4,"title":"Using Python for Automated Download","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nAutomated DEM download for study area\n\"\"\"\n\nimport requests\nimport rasterio\nfrom rasterio.merge import merge\nfrom pathlib import Path\nimport numpy as np\n\ndef download_3dep_dem(bbox, resolution='10m', output_file='dem.tif'):\n    \"\"\"\n    Download USGS 3DEP DEM for bounding box\n\n    Parameters:\n    bbox: [west, south, east, north] in decimal degrees\n    resolution: '1m', '3m', '10m', or '30m'\n    output_file: Output filename\n    \"\"\"\n\n    # USGS 3DEP WCS endpoint\n    wcs_url = \"https://elevation.nationalmap.gov/arcgis/services/3DEPElevation/ImageServer/WCSServer\"\n\n    # Build WCS request\n    params = {\n        'service': 'WCS',\n        'version': '2.0.1',\n        'request': 'GetCoverage',\n        'coverageId': f'DEP3Elevation:{resolution}',\n        'format': 'image/tiff',\n        'subset': f'x({bbox[0]},{bbox[2]})',\n        'subset': f'y({bbox[1]},{bbox[3]})',\n        'subsettingCRS': 'EPSG:4326'\n    }\n\n    # Download DEM\n    response = requests.get(wcs_url, params=params, stream=True)\n\n    if response.status_code == 200:\n        with open(output_file, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        print(f\"DEM downloaded: {output_file}\")\n        return output_file\n    else:\n        print(f\"Error downloading DEM: {response.status_code}\")\n        return None\n\ndef download_srtm_tiles(bbox, output_file='srtm_merged.tif'):\n    \"\"\"\n    Download and merge SRTM tiles for study area\n    \"\"\"\n\n    # Calculate required tiles\n    west, south, east, north = bbox\n\n    # SRTM tiles are 1x1 degree\n    lat_tiles = range(int(south), int(north) + 1)\n    lon_tiles = range(int(west), int(east) + 1)\n\n    tile_files = []\n\n    for lat in lat_tiles:\n        for lon in lon_tiles:\n            # Determine tile name\n            ns = 'N' if lat &gt;= 0 else 'S'\n            ew = 'E' if lon &gt;= 0 else 'W'\n            tile_name = f\"{ns}{abs(lat):02d}{ew}{abs(lon):03d}\"\n\n            # Download from OpenTopography\n            url = f\"https://cloud.sdsc.edu/v1/AUTH_opentopography/Raster/SRTM_GL1/SRTM_GL1_srtm/North_America/{tile_name}.hgt\"\n\n            tile_file = f\"{tile_name}.tif\"\n\n            # Convert HGT to GeoTIFF\n            gdal_command = f\"gdal_translate /vsicurl/{url} {tile_file}\"\n            os.system(gdal_command)\n\n            if Path(tile_file).exists():\n                tile_files.append(tile_file)\n\n    # Merge tiles\n    if tile_files:\n        datasets = [rasterio.open(f) for f in tile_files]\n        merged, transform = merge(datasets)\n\n        # Save merged DEM\n        profile = datasets[0].profile.copy()\n        profile.update({\n            'height': merged.shape[1],\n            'width': merged.shape[2],\n            'transform': transform\n        })\n\n        with rasterio.open(output_file, 'w', **profile) as dst:\n            dst.write(merged[0], 1)\n\n        # Cleanup\n        for ds in datasets:\n            ds.close()\n        for f in tile_files:\n            Path(f).unlink()\n\n        print(f\"Merged SRTM DEM: {output_file}\")\n        return output_file\n\n    return None\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#step-2-preprocess-dem","level":3,"title":"Step 2: Preprocess DEM","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#projection-and-resampling","level":4,"title":"Projection and Resampling","text":"<pre><code># Check DEM projection\ngdalinfo input_dem.tif | grep -A 3 \"Coordinate System\"\n\n# Reproject to appropriate UTM zone\n# Find UTM zone for center of study area\nutm_zone=$(gdal_query.py -lon -117.0 -lat 32.5 -utm)\n\n# Reproject DEM\ngdalwarp \\\n  -t_srs \"+proj=utm +zone=${utm_zone} +datum=WGS84\" \\\n  -r bilinear \\\n  -tr 10 10 \\\n  input_dem.tif \\\n  dem_utm.tif\n\n# Alternative: Use local projected system (e.g., State Plane)\ngdalwarp -t_srs EPSG:2230 input_dem.tif dem_stateplane.tif\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#resolution-optimization","level":4,"title":"Resolution Optimization","text":"<pre><code>def optimize_dem_resolution(dem_file, target_cells=1000000):\n    \"\"\"\n    Resample DEM to optimal resolution for processing\n\n    Target ~1 million cells for efficient processing\n    \"\"\"\n\n    with rasterio.open(dem_file) as src:\n        current_cells = src.width * src.height\n\n        if current_cells &gt; target_cells:\n            # Calculate resampling factor\n            scale_factor = np.sqrt(target_cells / current_cells)\n\n            # New resolution\n            new_res = src.res[0] / scale_factor\n\n            print(f\"Current cells: {current_cells:,}\")\n            print(f\"Resampling from {src.res[0]}m to {new_res:.1f}m\")\n\n            # Resample using GDAL\n            output_file = dem_file.replace('.tif', '_resampled.tif')\n\n            os.system(f\"\"\"\n                gdalwarp \\\n                  -tr {new_res} {new_res} \\\n                  -r bilinear \\\n                  {dem_file} \\\n                  {output_file}\n            \"\"\")\n\n            return output_file\n\n    return dem_file\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#step-3-fill-sinks-and-pits","level":3,"title":"Step 3: Fill Sinks and Pits","text":"<p>Hydrological conditioning ensures proper flow routing:</p> <pre><code>def fill_sinks(dem_file, output_file='dem_filled.tif'):\n    \"\"\"\n    Fill sinks in DEM using GRASS GIS\n    \"\"\"\n\n    import grass.script as gs\n\n    # Import DEM to GRASS\n    gs.run_command('r.in.gdal', input=dem_file, output='dem_raw')\n\n    # Fill sinks using r.terraflow\n    gs.run_command('r.terraflow',\n                   elevation='dem_raw',\n                   filled='dem_filled',\n                   direction='flow_dir',\n                   swatershed='watersheds',\n                   accumulation='flow_acc',\n                   tci='twi')\n\n    # Alternative: Use r.fill.dir for smaller DEMs\n    gs.run_command('r.fill.dir',\n                   input='dem_raw',\n                   output='dem_filled_alt',\n                   direction='flow_dir_alt')\n\n    # Export filled DEM\n    gs.run_command('r.out.gdal',\n                   input='dem_filled',\n                   output=output_file,\n                   format='GTiff',\n                   createopt='COMPRESS=LZW')\n\n    return output_file\n\n# Alternative using RichDEM (Python)\nimport richdem as rd\n\ndef fill_sinks_richdem(dem_file, output_file='dem_filled.tif'):\n    \"\"\"\n    Fill sinks using RichDEM library\n    \"\"\"\n\n    # Load DEM\n    dem = rd.LoadGDAL(dem_file)\n\n    # Fill depressions\n    filled = rd.FillDepressions(dem, epsilon=False, in_place=False)\n\n    # Save result\n    rd.SaveGDAL(output_file, filled)\n\n    return output_file\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#step-4-create-analysis-mask","level":3,"title":"Step 4: Create Analysis Mask","text":"<p>Define your exact study area:</p> <pre><code>def create_analysis_mask(dem_file, boundary_file, output_file='dem_masked.tif'):\n    \"\"\"\n    Clip DEM to study area boundary\n\n    Parameters:\n    dem_file: Input DEM\n    boundary_file: Shapefile or GeoJSON with study area boundary\n    output_file: Masked DEM output\n    \"\"\"\n\n    import geopandas as gpd\n    from rasterio.mask import mask\n\n    # Load boundary\n    boundary = gpd.read_file(boundary_file)\n\n    # Open DEM\n    with rasterio.open(dem_file) as src:\n        # Reproject boundary if needed\n        if boundary.crs != src.crs:\n            boundary = boundary.to_crs(src.crs)\n\n        # Mask DEM\n        out_image, out_transform = mask(src, \n                                        boundary.geometry,\n                                        crop=True,\n                                        nodata=-9999)\n\n        # Update metadata\n        out_meta = src.meta.copy()\n        out_meta.update({\n            'height': out_image.shape[1],\n            'width': out_image.shape[2],\n            'transform': out_transform,\n            'nodata': -9999\n        })\n\n        # Save masked DEM\n        with rasterio.open(output_file, 'w', **out_meta) as dst:\n            dst.write(out_image[0], 1)\n\n    return output_file\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#climate-data-acquisition","level":2,"title":"Climate Data Acquisition","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#daymet-data-download","level":3,"title":"DAYMET Data Download","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#automated-daymet-download","level":4,"title":"Automated DAYMET Download","text":"<pre><code>def download_daymet_for_dem(dem_file, years, variables=['tmin', 'tmax', 'prcp', 'vp']):\n    \"\"\"\n    Download DAYMET data matching DEM extent\n    \"\"\"\n\n    import xarray as xr\n    from pyproj import Transformer\n\n    # Get DEM bounds\n    with rasterio.open(dem_file) as src:\n        bounds = src.bounds\n\n        # Transform to lat/lon if needed\n        if src.crs != 'EPSG:4326':\n            transformer = Transformer.from_crs(src.crs, 'EPSG:4326', always_xy=True)\n            west, south = transformer.transform(bounds.left, bounds.bottom)\n            east, north = transformer.transform(bounds.right, bounds.top)\n        else:\n            west, south, east, north = bounds\n\n    # DAYMET API endpoint\n    api_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n\n    climate_data = {}\n\n    for year in years:\n        for var in variables:\n            print(f\"Downloading {var} for {year}...\")\n\n            # Build request\n            params = {\n                'lat': (south + north) / 2,\n                'lon': (west + east) / 2,\n                'vars': var,\n                'start': f'{year}-01-01',\n                'end': f'{year}-12-31',\n                'format': 'netcdf'\n            }\n\n            # For spatial data, use Daymet web service\n            spatial_url = f\"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1840/daymet_v4_daily_na_{var}_{year}.nc\"\n\n            params_spatial = {\n                'var': var,\n                'north': north,\n                'south': south,\n                'east': east,\n                'west': west,\n                'time_start': f'{year}-01-01T12:00:00Z',\n                'time_end': f'{year}-12-31T12:00:00Z',\n                'accept': 'netcdf'\n            }\n\n            response = requests.get(spatial_url, params=params_spatial)\n\n            if response.status_code == 200:\n                output_file = f'daymet_{var}_{year}.nc'\n                with open(output_file, 'wb') as f:\n                    f.write(response.content)\n\n                climate_data[f'{var}_{year}'] = output_file\n\n    return climate_data\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#process-daymet-for-eemt","level":4,"title":"Process DAYMET for EEMT","text":"<pre><code>def process_daymet_for_eemt(daymet_files, dem_file, output_dir='climate_processed'):\n    \"\"\"\n    Process DAYMET data to match DEM resolution and projection\n    \"\"\"\n\n    Path(output_dir).mkdir(exist_ok=True)\n\n    # Load DEM for reference\n    with rasterio.open(dem_file) as dem_src:\n        dem_profile = dem_src.profile\n        dem_bounds = dem_src.bounds\n        dem_res = dem_src.res\n\n    processed_files = {}\n\n    for var_year, nc_file in daymet_files.items():\n        print(f\"Processing {var_year}...\")\n\n        # Open NetCDF\n        ds = xr.open_dataset(nc_file)\n\n        # Get variable name\n        var = var_year.split('_')[0]\n\n        # Process each day/month\n        for time_idx in range(len(ds.time)):\n\n            # Extract data for this time\n            data = ds[var].isel(time=time_idx)\n\n            # Convert to GeoTIFF\n            temp_file = f'temp_{var}_{time_idx}.tif'\n\n            # Write to temporary GeoTIFF\n            # (Implementation depends on DAYMET projection)\n\n            # Reproject to match DEM\n            output_file = Path(output_dir) / f'{var}_{time_idx:03d}.tif'\n\n            gdal_command = f\"\"\"\n                gdalwarp \\\n                  -t_srs '{dem_profile['crs']}' \\\n                  -te {dem_bounds.left} {dem_bounds.bottom} {dem_bounds.right} {dem_bounds.top} \\\n                  -tr {dem_res[0]} {dem_res[1]} \\\n                  -r bilinear \\\n                  {temp_file} \\\n                  {output_file}\n            \"\"\"\n\n            os.system(gdal_command)\n\n            processed_files[f'{var}_{time_idx}'] = output_file\n\n    return processed_files\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#vegetation-data-processing","level":2,"title":"Vegetation Data Processing","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#ndvi-from-satellite-imagery","level":3,"title":"NDVI from Satellite Imagery","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#landsat-ndvi-calculation","level":4,"title":"Landsat NDVI Calculation","text":"<pre><code>def calculate_ndvi_landsat(scene_dir, output_file='ndvi.tif'):\n    \"\"\"\n    Calculate NDVI from Landsat 8/9 imagery\n    \"\"\"\n\n    # Find band files\n    red_file = list(Path(scene_dir).glob('*_B4.TIF'))[0]  # Band 4 - Red\n    nir_file = list(Path(scene_dir).glob('*_B5.TIF'))[0]  # Band 5 - NIR\n\n    # Load bands\n    with rasterio.open(red_file) as red_src:\n        red = red_src.read(1).astype(float)\n        profile = red_src.profile\n\n    with rasterio.open(nir_file) as nir_src:\n        nir = nir_src.read(1).astype(float)\n\n    # Calculate NDVI\n    # Avoid division by zero\n    denominator = nir + red\n    denominator[denominator == 0] = 1\n\n    ndvi = (nir - red) / denominator\n\n    # Constrain to valid range\n    ndvi = np.clip(ndvi, -1, 1)\n\n    # Save NDVI\n    profile.update(dtype=rasterio.float32, count=1)\n\n    with rasterio.open(output_file, 'w', **profile) as dst:\n        dst.write(ndvi.astype(np.float32), 1)\n\n    return output_file\n\ndef download_and_process_sentinel2_ndvi(bbox, date_range, output_file='ndvi_s2.tif'):\n    \"\"\"\n    Download and process Sentinel-2 data for NDVI\n    Using sentinelsat library\n    \"\"\"\n\n    from sentinelsat import SentinelAPI\n    from datetime import datetime\n\n    # Connect to Copernicus Hub\n    api = SentinelAPI('username', 'password', 'https://scihub.copernicus.eu/dhus')\n\n    # Search for products\n    footprint = f\"POLYGON(({bbox[0]} {bbox[1]}, {bbox[2]} {bbox[1]}, {bbox[2]} {bbox[3]}, {bbox[0]} {bbox[3]}, {bbox[0]} {bbox[1]}))\"\n\n    products = api.query(footprint,\n                         date=date_range,\n                         platformname='Sentinel-2',\n                         cloudcoverpercentage=(0, 20),\n                         producttype='S2MSI2A')  # Level-2A (atmospherically corrected)\n\n    # Download best product\n    if products:\n        product_id = list(products.keys())[0]\n        api.download(product_id)\n\n        # Process downloaded data\n        # Extract and calculate NDVI\n        # Band 4 (Red) and Band 8 (NIR) for Sentinel-2\n\n    return output_file\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#lai-estimation","level":3,"title":"LAI Estimation","text":"<pre><code>def estimate_lai_from_ndvi(ndvi_file, output_file='lai.tif', method='exponential'):\n    \"\"\"\n    Estimate Leaf Area Index from NDVI\n    \"\"\"\n\n    with rasterio.open(ndvi_file) as src:\n        ndvi = src.read(1)\n        profile = src.profile\n\n    if method == 'exponential':\n        # Exponential relationship (Boegh et al., 2002)\n        lai = -2.0 * np.log(1 - ndvi)\n\n    elif method == 'polynomial':\n        # Polynomial for semiarid (Qi et al., 2000)\n        lai = 18.99 * ndvi**3 - 15.24 * ndvi**2 + 6.124 * ndvi - 0.352\n\n    elif method == 'linear':\n        # Simple linear (for grassland/crops)\n        lai = 6.0 * ndvi - 0.5\n\n    # Constrain to valid range\n    lai = np.clip(lai, 0, 10)\n\n    # Handle water/bare soil\n    lai[ndvi &lt; 0.1] = 0\n\n    # Save LAI\n    with rasterio.open(output_file, 'w', **profile) as dst:\n        dst.write(lai.astype(np.float32), 1)\n\n    return output_file\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#quality-control","level":2,"title":"Quality Control","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#dem-quality-checks","level":3,"title":"DEM Quality Checks","text":"<pre><code>def validate_dem(dem_file):\n    \"\"\"\n    Comprehensive DEM quality control\n    \"\"\"\n\n    issues = []\n    warnings = []\n\n    with rasterio.open(dem_file) as src:\n        dem = src.read(1)\n\n        # Check 1: NoData values\n        nodata_count = np.sum(dem == src.nodata) if src.nodata else 0\n        nodata_percent = (nodata_count / dem.size) * 100\n\n        if nodata_percent &gt; 10:\n            warnings.append(f\"High NoData percentage: {nodata_percent:.1f}%\")\n\n        # Check 2: Elevation range\n        valid_data = dem[dem != src.nodata] if src.nodata else dem\n\n        if len(valid_data) &gt; 0:\n            min_elev = np.min(valid_data)\n            max_elev = np.max(valid_data)\n\n            if min_elev &lt; -500:\n                issues.append(f\"Unrealistic minimum elevation: {min_elev}m\")\n            if max_elev &gt; 9000:\n                issues.append(f\"Unrealistic maximum elevation: {max_elev}m\")\n\n            # Check 3: Flat areas\n            unique_values = np.unique(valid_data)\n            if len(unique_values) &lt; 10:\n                issues.append(\"DEM appears to be heavily quantized or flat\")\n\n        # Check 4: Resolution\n        res_x, res_y = src.res\n        if abs(res_x - res_y) &gt; 0.001:\n            warnings.append(f\"Non-square pixels: {res_x} x {res_y}\")\n\n        # Check 5: Projection\n        if not src.crs:\n            issues.append(\"No coordinate system defined\")\n        elif src.crs.to_epsg() == 4326:\n            warnings.append(\"Geographic coordinates - consider reprojecting\")\n\n        # Check 6: File size\n        size_mb = dem.nbytes / 1024 / 1024\n        if size_mb &gt; 500:\n            warnings.append(f\"Large file size: {size_mb:.1f} MB - consider resampling\")\n\n    # Report results\n    print(\"=== DEM Validation Report ===\")\n\n    if not issues and not warnings:\n        print(\"‚úÖ DEM passed all checks\")\n\n    if issues:\n        print(\"\\n‚ùå CRITICAL ISSUES:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n\n    if warnings:\n        print(\"\\n‚ö†Ô∏è  WARNINGS:\")\n        for warning in warnings:\n            print(f\"  - {warning}\")\n\n    return len(issues) == 0\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#climate-data-validation","level":3,"title":"Climate Data Validation","text":"<pre><code>def validate_climate_data(climate_files, dem_file):\n    \"\"\"\n    Validate climate data against DEM\n    \"\"\"\n\n    issues = []\n\n    # Load DEM bounds\n    with rasterio.open(dem_file) as dem_src:\n        dem_bounds = dem_src.bounds\n        dem_crs = dem_src.crs\n\n    for var, file_path in climate_files.items():\n        print(f\"Checking {var}...\")\n\n        if file_path.endswith('.nc'):\n            # NetCDF file\n            ds = xr.open_dataset(file_path)\n\n            # Check spatial coverage\n            # (Implementation depends on projection)\n\n        elif file_path.endswith('.tif'):\n            # GeoTIFF file\n            with rasterio.open(file_path) as src:\n                # Check alignment with DEM\n                if src.crs != dem_crs:\n                    issues.append(f\"{var}: CRS mismatch with DEM\")\n\n                # Check overlap\n                if not rasterio.coords.disjoint_bounds(src.bounds, dem_bounds):\n                    issues.append(f\"{var}: No spatial overlap with DEM\")\n\n                # Check values\n                data = src.read(1)\n\n                if 'temp' in var or 'tmin' in var or 'tmax' in var:\n                    if np.any(data &lt; -100) or np.any(data &gt; 60):\n                        issues.append(f\"{var}: Temperature values out of range\")\n\n                elif 'prcp' in var or 'precip' in var:\n                    if np.any(data &lt; 0) or np.any(data &gt; 500):\n                        issues.append(f\"{var}: Precipitation values out of range\")\n\n    return issues\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#advanced-techniques","level":2,"title":"Advanced Techniques","text":"","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#multi-scale-analysis","level":3,"title":"Multi-Scale Analysis","text":"<pre><code>def prepare_multiscale_dems(base_dem, scales=[10, 30, 90]):\n    \"\"\"\n    Create DEMs at multiple resolutions for scale-dependent analysis\n    \"\"\"\n\n    output_dems = {}\n\n    for scale in scales:\n        output_file = f'dem_{scale}m.tif'\n\n        # Resample using appropriate method\n        if scale &lt; 30:\n            method = 'bilinear'  # Smooth for fine scales\n        else:\n            method = 'average'   # Aggregate for coarse scales\n\n        gdal_command = f\"\"\"\n            gdalwarp \\\n              -tr {scale} {scale} \\\n              -r {method} \\\n              {base_dem} \\\n              {output_file}\n        \"\"\"\n\n        os.system(gdal_command)\n        output_dems[scale] = output_file\n\n    return output_dems\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#temporal-data-organization","level":3,"title":"Temporal Data Organization","text":"<pre><code>def organize_temporal_data(data_dir, output_structure='hierarchical'):\n    \"\"\"\n    Organize climate data for efficient temporal processing\n    \"\"\"\n\n    from datetime import datetime\n    import shutil\n\n    data_dir = Path(data_dir)\n\n    if output_structure == 'hierarchical':\n        # Organize as: year/month/day/variable.tif\n\n        for file_path in data_dir.glob('*.tif'):\n            # Parse filename (assumes: variable_YYYYMMDD.tif)\n            parts = file_path.stem.split('_')\n\n            if len(parts) &gt;= 2:\n                var_name = parts[0]\n                date_str = parts[1]\n\n                # Parse date\n                date = datetime.strptime(date_str, '%Y%m%d')\n\n                # Create directory structure\n                year_dir = data_dir / str(date.year)\n                month_dir = year_dir / f'{date.month:02d}'\n                day_dir = month_dir / f'{date.day:02d}'\n\n                day_dir.mkdir(parents=True, exist_ok=True)\n\n                # Move file\n                new_path = day_dir / f'{var_name}.tif'\n                shutil.move(file_path, new_path)\n\n    elif output_structure == 'netcdf':\n        # Combine into NetCDF with time dimension\n\n        import xarray as xr\n\n        # Group by variable\n        variables = {}\n\n        for file_path in data_dir.glob('*.tif'):\n            var_name = file_path.stem.split('_')[0]\n\n            if var_name not in variables:\n                variables[var_name] = []\n\n            variables[var_name].append(file_path)\n\n        # Create NetCDF for each variable\n        for var_name, file_list in variables.items():\n            # Sort by date\n            file_list.sort()\n\n            # Load all files\n            arrays = []\n            times = []\n\n            for file_path in file_list:\n                with rasterio.open(file_path) as src:\n                    arrays.append(src.read(1))\n\n                    # Extract date from filename\n                    date_str = file_path.stem.split('_')[1]\n                    times.append(pd.to_datetime(date_str))\n\n            # Create xarray dataset\n            da = xr.DataArray(\n                np.stack(arrays),\n                dims=['time', 'y', 'x'],\n                coords={'time': times}\n            )\n\n            # Save to NetCDF\n            da.to_netcdf(data_dir / f'{var_name}_timeseries.nc')\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#parallel-processing-setup","level":3,"title":"Parallel Processing Setup","text":"<pre><code>def prepare_for_parallel_processing(dem_file, tile_size=5000, overlap=100):\n    \"\"\"\n    Tile DEM for parallel processing\n    \"\"\"\n\n    output_dir = Path('tiles')\n    output_dir.mkdir(exist_ok=True)\n\n    with rasterio.open(dem_file) as src:\n        # Calculate number of tiles\n        n_tiles_x = int(np.ceil(src.width / tile_size))\n        n_tiles_y = int(np.ceil(src.height / tile_size))\n\n        tiles = []\n\n        for i in range(n_tiles_y):\n            for j in range(n_tiles_x):\n                # Calculate window with overlap\n                col_off = j * tile_size - overlap if j &gt; 0 else 0\n                row_off = i * tile_size - overlap if i &gt; 0 else 0\n\n                width = min(tile_size + overlap, src.width - col_off)\n                height = min(tile_size + overlap, src.height - row_off)\n\n                # Read tile\n                window = rasterio.windows.Window(col_off, row_off, width, height)\n                tile_data = src.read(1, window=window)\n\n                # Get transform for tile\n                tile_transform = rasterio.windows.transform(window, src.transform)\n\n                # Save tile\n                tile_file = output_dir / f'tile_{i:02d}_{j:02d}.tif'\n\n                profile = src.profile.copy()\n                profile.update({\n                    'width': width,\n                    'height': height,\n                    'transform': tile_transform\n                })\n\n                with rasterio.open(tile_file, 'w', **profile) as dst:\n                    dst.write(tile_data, 1)\n\n                tiles.append({\n                    'file': tile_file,\n                    'row': i,\n                    'col': j,\n                    'window': window\n                })\n\n    # Save tile metadata\n    import json\n    with open(output_dir / 'tiles.json', 'w') as f:\n        json.dump([{\n            'file': str(t['file']),\n            'row': t['row'],\n            'col': t['col'],\n            'window': {\n                'col_off': t['window'].col_off,\n                'row_off': t['window'].row_off,\n                'width': t['window'].width,\n                'height': t['window'].height\n            }\n        } for t in tiles], f, indent=2)\n\n    return tiles\n</code></pre>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#data-preparation-checklist","level":2,"title":"Data Preparation Checklist","text":"<p>Before running EEMT calculations, ensure:</p>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#dem-preparation_1","level":3,"title":"‚úÖ DEM Preparation","text":"<ul> <li> Downloaded appropriate resolution DEM</li> <li> Reprojected to suitable coordinate system</li> <li> Filled sinks and pits</li> <li> Clipped to study area</li> <li> Validated elevation ranges</li> <li> Created buffer zone if needed</li> </ul>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#climate-data","level":3,"title":"‚úÖ Climate Data","text":"<ul> <li> Downloaded all required variables (tmin, tmax, prcp, vp)</li> <li> Matched spatial extent with DEM</li> <li> Aligned projection and resolution</li> <li> Validated data ranges</li> <li> Organized temporal structure</li> </ul>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#vegetation-data-if-using-eemt_veg","level":3,"title":"‚úÖ Vegetation Data (if using EEMT_VEG)","text":"<ul> <li> Acquired cloud-free imagery</li> <li> Calculated NDVI</li> <li> Estimated LAI</li> <li> Processed canopy height (if available)</li> <li> Matched resolution with DEM</li> </ul>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#quality-control_1","level":3,"title":"‚úÖ Quality Control","text":"<ul> <li> No significant NoData gaps</li> <li> Consistent projections across datasets</li> <li> Reasonable value ranges</li> <li> Adequate spatial coverage</li> <li> Appropriate temporal coverage</li> </ul>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/data-preparation/#summary","level":2,"title":"Summary","text":"<p>Proper data preparation is essential for accurate EEMT calculations. Key points:</p> <ol> <li>Start with quality DEM data - Resolution and accuracy matter</li> <li>Ensure spatial alignment - All data must match DEM extent and projection</li> <li>Validate thoroughly - Check for issues before processing</li> <li>Consider scale - Choose appropriate resolution for your analysis</li> <li>Document processing - Keep track of all preparation steps</li> </ol> <p>With properly prepared data, you're ready to run EEMT calculations following the Quick Start Guide or advanced Calculation Methods.</p> <p>For additional help with data preparation, see the API Reference or contact the development team.</p>","path":["Workflows","Data Preparation Guide"],"tags":[]},{"location":"workflows/quick-start/","level":1,"title":"Quick Start Guide","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#overview","level":2,"title":"Overview","text":"<p>This guide will help you run your first EEMT calculation using the web interface in under 10 minutes. We'll use a sample DEM file and default parameters to demonstrate the basic workflow.</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#prerequisites","level":2,"title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Docker Desktop installed and running (Download Docker)</li> <li>4GB of available RAM (8GB recommended)</li> <li>10GB of free disk space for data and results</li> <li>Internet connection for downloading climate data</li> </ul>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-1-deploy-eemt-with-docker-compose","level":2,"title":"Step 1: Deploy EEMT with Docker Compose","text":"<p>The fastest way to get started is using Docker Compose:</p> <pre><code># Clone the repository\ngit clone https://github.com/cyverse-gis/eemt.git\ncd eemt\n\n# Start the EEMT web interface\ndocker-compose up\n</code></pre> <p>This command will: 1. Build the EEMT container with all dependencies 2. Start the web interface on port 5000 3. Enable job monitoring on the dashboard</p> <p>Wait for the message: <code>INFO: Application startup complete</code></p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-2-access-the-web-interface","level":2,"title":"Step 2: Access the Web Interface","text":"<p>Open your web browser and navigate to:</p> <pre><code>http://127.0.0.1:5000\n</code></pre> <p>You should see the EEMT Web Interface homepage with a job submission form.</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-3-prepare-your-first-dem","level":2,"title":"Step 3: Prepare Your First DEM","text":"<p>For this quick start, we'll use the included sample DEM:</p> <pre><code># The sample DEM is located at:\n# sol/examples/mcn_10m.tif\n\n# This is a 10m resolution DEM of a small watershed\n# in southeastern Arizona (Marshall Gulch, Santa Catalina Mountains)\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#dem-requirements","level":3,"title":"DEM Requirements","text":"<p>Your DEM must meet these criteria: - Format: GeoTIFF (.tif or .tiff) - Projection: Any valid coordinate system (will be auto-detected) - Resolution: 1m to 1000m (10-30m recommended for regional analysis) - Size: Under 100MB for quick processing</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-4-submit-your-first-job","level":2,"title":"Step 4: Submit Your First Job","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#using-the-web-interface","level":3,"title":"Using the Web Interface","text":"<ol> <li>Select Workflow Type: Choose \"Solar Radiation Only\" for a quick test</li> <li>Upload DEM: Click \"Choose File\" and select <code>mcn_10m.tif</code></li> <li>Set Parameters:</li> <li>Step: 15 (minutes between calculations)</li> <li>Threads: 4 (parallel processes)</li> <li>Linke Turbidity: 3.0 (clear atmosphere)</li> <li>Albedo: 0.2 (typical soil/vegetation)</li> <li>Submit: Click \"Submit Job\"</li> </ol>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#understanding-the-parameters","level":3,"title":"Understanding the Parameters","text":"Parameter Quick Start Value What It Does Step 15 minutes Time interval for solar calculations. Lower = more accurate but slower Threads 4 Number of parallel processes. Match your CPU cores Linke Turbidity 3.0 Atmospheric clarity. 1-2 = very clear, 3-4 = average, 5-8 = hazy Albedo 0.2 Surface reflectance. 0.1 = dark soil, 0.2 = vegetation, 0.8 = snow","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-5-monitor-job-progress","level":2,"title":"Step 5: Monitor Job Progress","text":"<p>After submission, you'll be redirected to the monitoring page:</p> <pre><code>http://127.0.0.1:5000/monitor\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#what-youll-see","level":3,"title":"What You'll See","text":"<pre><code>Job ID: SOL_20240115_143022\nStatus: Running\nProgress: Processing day 42 of 365...\nEstimated Time Remaining: 8 minutes\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#processing-stages","level":3,"title":"Processing Stages","text":"<ol> <li>Initialization (30 seconds)</li> <li>Setting up GRASS GIS environment</li> <li>Importing DEM</li> <li> <p>Calculating horizons</p> </li> <li> <p>Daily Calculations (5-15 minutes)</p> </li> <li>365 solar radiation maps</li> <li>One calculation per day of year</li> <li> <p>Progress updated in real-time</p> </li> <li> <p>Aggregation (1 minute)</p> </li> <li>Monthly summaries</li> <li>Annual totals</li> <li>Statistics generation</li> </ol>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-6-view-and-download-results","level":2,"title":"Step 6: View and Download Results","text":"<p>When complete, the monitoring page will show:</p> <pre><code>Status: Completed\nProcessing Time: 12 minutes 34 seconds\nResults: Available for download\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#output-files","level":3,"title":"Output Files","text":"<p>Click \"Download Results\" to get a ZIP file containing:</p> <pre><code>results_SOL_20240115_143022.zip\n‚îú‚îÄ‚îÄ global/\n‚îÇ   ‚îú‚îÄ‚îÄ daily/           # 365 daily solar radiation maps\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ total_sun_day_001.tif\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ total_sun_day_002.tif\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îú‚îÄ‚îÄ monthly/         # 12 monthly summaries\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ total_sun_01_sum.tif\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ total_sun_02_sum.tif\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îî‚îÄ‚îÄ annual/          # Annual total\n‚îÇ       ‚îî‚îÄ‚îÄ total_sun_annual.tif\n‚îú‚îÄ‚îÄ metadata/\n‚îÇ   ‚îú‚îÄ‚îÄ parameters.json  # Input parameters\n‚îÇ   ‚îî‚îÄ‚îÄ statistics.csv   # Summary statistics\n‚îî‚îÄ‚îÄ logs/\n    ‚îú‚îÄ‚îÄ workflow.log     # Processing log\n    ‚îî‚îÄ‚îÄ errors.log       # Any errors (should be empty)\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#step-7-visualize-results","level":2,"title":"Step 7: Visualize Results","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#quick-visualization-with-qgis","level":3,"title":"Quick Visualization with QGIS","text":"<ol> <li>Open QGIS</li> <li>Drag <code>total_sun_annual.tif</code> into the map window</li> <li>Right-click layer ‚Üí Properties ‚Üí Symbology</li> <li>Choose \"Singleband pseudocolor\" with \"Spectral\" color ramp</li> <li>Click Apply</li> </ol>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#understanding-the-output","level":3,"title":"Understanding the Output","text":"<p>Solar radiation values are in Wh/m¬≤ (watt-hours per square meter):</p> Annual Total Environment Type &lt; 1,000,000 Deep valleys, north-facing cliffs 1,000,000 - 1,500,000 Shaded slopes, forest 1,500,000 - 2,000,000 Open terrain, grassland &gt; 2,000,000 South-facing slopes, ridgetops","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#next-steps","level":2,"title":"Next Steps","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#run-a-full-eemt-analysis","level":3,"title":"Run a Full EEMT Analysis","text":"<p>Now try the complete EEMT workflow with climate data:</p> <pre><code># From the web interface, select \"Full EEMT\"\n# This will:\n# 1. Calculate solar radiation\n# 2. Download DAYMET climate data\n# 3. Compute NPP and effective precipitation\n# 4. Generate EEMT maps\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#customize-parameters","level":3,"title":"Customize Parameters","text":"<p>Experiment with different settings:</p> <pre><code># High-resolution analysis (slower, more accurate)\nstep = 5        # 5-minute intervals\nthreads = 8     # Use more cores\n\n# Different environments\nlinke = 1.5     # Very clear mountain air\nalbedo = 0.8    # Snow-covered terrain\n\n# Arid region\nlinke = 4.0     # Dusty atmosphere\nalbedo = 0.35   # Desert soil\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#process-your-own-study-area","level":3,"title":"Process Your Own Study Area","text":"<ol> <li> <p>Prepare your DEM:    <pre><code># Reproject if needed\ngdalwarp -t_srs EPSG:4326 your_dem.tif dem_wgs84.tif\n\n# Clip to study area\ngdalwarp -te xmin ymin xmax ymax dem_wgs84.tif study_area.tif\n</code></pre></p> </li> <li> <p>Submit through web interface with parameters appropriate for your region</p> </li> <li> <p>Monitor and download results when complete</p> </li> </ol>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#common-issues-and-solutions","level":2,"title":"Common Issues and Solutions","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#issue-docker-not-starting","level":3,"title":"Issue: Docker not starting","text":"<pre><code># Check Docker status\ndocker --version\ndocker ps\n\n# Restart Docker Desktop\n# On Mac/Windows: Use the Docker Desktop app\n# On Linux:\nsudo systemctl restart docker\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#issue-port-5000-already-in-use","level":3,"title":"Issue: Port 5000 already in use","text":"<pre><code># Use a different port\ndocker-compose run -p 5001:5000 eemt-web\n# Then access at http://127.0.0.1:5001\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#issue-slow-processing","level":3,"title":"Issue: Slow processing","text":"<p>Solutions: - Increase step size (15 ‚Üí 30 minutes) - Reduce DEM resolution - Allocate more threads - Ensure adequate RAM available</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#issue-climate-data-download-fails","level":3,"title":"Issue: Climate data download fails","text":"<p>Solutions: - Check internet connection - Verify study area is within DAYMET coverage (North America) - Try again (ORNL server may be temporarily unavailable)</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#tips-for-best-results","level":2,"title":"Tips for Best Results","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#dem-preparation","level":3,"title":"DEM Preparation","text":"<p>‚úÖ DO: - Use projected coordinate systems (e.g., UTM) - Include buffer area around study region - Fill sinks/pits in DEM before processing - Use consistent resolution (10-30m recommended)</p> <p>‚ùå DON'T: - Use geographic coordinates for large areas - Include ocean or large water bodies - Mix different resolution DEMs - Use DEMs with many NoData gaps</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#parameter-selection","level":3,"title":"Parameter Selection","text":"<p>For Different Regions:</p> Region Type Step Linke Albedo Notes Mountains 10 2.0 0.15 High resolution for complex terrain Desert 15 4.0 0.35 Account for dust and bright soil Forest 15 3.0 0.15 Dark canopy, moderate atmosphere Agricultural 15 3.5 0.20 Seasonal variation important Arctic 30 1.5 0.80 Very clear air, snow cover","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#performance-optimization","level":3,"title":"Performance Optimization","text":"<pre><code># For large areas (&gt;10,000 km¬≤)\n# Split into tiles and process separately\n\n# Tile your DEM\ngdal_retile.py -ps 5000 5000 -overlap 100 large_dem.tif -targetDir tiles/\n\n# Process each tile\nfor tile in tiles/*.tif:\n    submit_eemt_job(tile)\n\n# Merge results\ngdal_merge.py -o final_eemt.tif results/*/eemt.tif\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#example-complete-5-minute-workflow","level":2,"title":"Example: Complete 5-Minute Workflow","text":"<p>Here's a complete workflow you can run in 5 minutes with the sample data:</p> <pre><code># 1. Start EEMT (if not already running)\ncd eemt\ndocker-compose up -d\n\n# 2. Wait for startup (30 seconds)\nsleep 30\n\n# 3. Submit job via curl (alternative to web interface)\ncurl -X POST http://127.0.0.1:5000/api/submit-job \\\n  -F \"workflow_type=sol\" \\\n  -F \"dem_file=@sol/examples/mcn_10m.tif\" \\\n  -F \"step=30\" \\\n  -F \"num_threads=4\" \\\n  -F \"linke_value=3.0\" \\\n  -F \"albedo_value=0.2\"\n\n# 4. Monitor progress\n# Job will complete in ~3 minutes with step=30\n\n# 5. Results will be available at:\n# http://127.0.0.1:5000/results/[JOB_ID]/\n</code></pre>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#getting-help","level":2,"title":"Getting Help","text":"","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#resources","level":3,"title":"Resources","text":"<ul> <li>Documentation: http://127.0.0.1:8000 (when running with <code>--profile docs</code>)</li> <li>GitHub Issues: https://github.com/cyverse-gis/eemt/issues</li> <li>Algorithm Details: See Solar Radiation Algorithms</li> <li>API Reference: See Web Interface API</li> </ul>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#support-channels","level":3,"title":"Support Channels","text":"<ul> <li>Scientific Questions: Review EEMT Publications</li> <li>Technical Issues: Check Development Guide</li> <li>Bug Reports: Use GitHub issue tracker</li> </ul>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#summary","level":2,"title":"Summary","text":"<p>You've successfully: - ‚úÖ Deployed EEMT v2.0.0 with improved reliability - ‚úÖ Submitted your first job through the enhanced interface - ‚úÖ Monitored job progress with accurate tracking - ‚úÖ Downloaded and understood results - ‚úÖ Learned to use the new system resource detection</p>","path":["Workflows","Quick Start Guide"],"tags":[]},{"location":"workflows/quick-start/#whats-different-in-v200","level":3,"title":"What's Different in v2.0.0?","text":"<ul> <li>No more submission failures - Robust error handling</li> <li>Accurate resource display - Know your system capabilities</li> <li>Reliable progress tracking - No more hanging at 25%</li> <li>Better user experience - Clear feedback at every step</li> </ul> <p>Next: Try the Data Preparation Guide to work with your own study area, or explore Full EEMT Calculations for complete energy balance modeling.</p> <p>Estimated time to complete this guide: Under 5 minutes with v2.0.0 improvements!</p>","path":["Workflows","Quick Start Guide"],"tags":[]}]}